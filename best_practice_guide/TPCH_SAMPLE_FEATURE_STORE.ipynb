{
 "metadata": {
  "kernelspec": {
   "display_name": "py_snowpark_df_ml_fs_1_7_0_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "lastEditStatus": {
   "notebookId": "hc6yw4jjvib4tpjzvt3z",
   "authorId": "33288905817",
   "authorName": "SIMON",
   "authorEmail": "simon.field@snowflake.com",
   "sessionId": "c693ed30-6591-4016-8e6d-60d0f526f8d6",
   "lastEditTime": 1748896007293
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ac5656-9c96-4bc3-b751-6dfadc83d078",
   "metadata": {
    "name": "PACKAGE_INSTALLATION",
    "collapsed": false
   },
   "source": "### Package Installation"
  },
  {
   "cell_type": "code",
   "id": "ca81e7d5-44c9-4eb8-b4b0-1da4e952d356",
   "metadata": {
    "language": "python",
    "name": "pip_install_tabulate"
   },
   "outputs": [],
   "source": "!pip install tabulate",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c07236e-ab52-4de2-9b7d-0c7020ce8781",
   "metadata": {
    "language": "python",
    "name": "pip_install_numpy_specific_ver",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "!pip install numpy==1.23.5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29083ad2-1f44-4571-b023-d666cf0bd4b8",
   "metadata": {
    "language": "python",
    "name": "cell141",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "!pip install sqlglot",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "693791c1-5081-4559-8cb7-09fc32588694",
   "metadata": {
    "collapsed": false,
    "name": "INTRODUCTION",
    "resultHeight": 343
   },
   "source": "# SAMPLE FEATURE STORE\n\nThis Notebook walks through an example of creating a Snowflake Feature Store over a multiple table relational data-model using Sample data available in all Snowflake accounts. As such this Notebook should run as is in any Snowflake account assuming the user has the required privileges to access the Sample data schema, and create the underlying database objects used by Feature Store (Schema, Tags, Dynamic Tables, Tables, Views & Datasets).  You can read more about the required privileges, and the sample script provided to grant them in the Feaure Store documentation [here](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/rbac).  \n\n\nWe will work through creating the key components of a feature-store:\n- creating mutiple Entities representing the different things we collect data about\n- deriving different types of features using dataframes within feature-views\n- retrieving datasets for training combining (joining) feature-views from multiple entities and key considerations when doing so\n\n\n## Entities\nIt is common for organisations to model and store features for differINent business entities (units of analysis) within their Feature Store so they can build ML models and make predictions on the behavior of those things.  These entities have relationships with each other and have a business entity hierarchy.\n\nFor example, a retail organisation may need to store features representing a product-hierarchy.  At the lowest level we might have PART, rolling up to PART_SUB_CATEGORY, PART_CATEGORY, DEPARTMENT, and finally COMPANY.   There may be a geographic/location hierarchy which might roll up from PART, through SHELF, UNIT, AISLE, STORE, REGION and COMPANY.  We may also want to create features at the intersection of some of the levels in these hierarchies. For example, the features representing sales, and stocking levels at STORE-DEPARTMENT level.  Within the Snowflake Feature Store we represent these business-entities using the Entity class and its functions.  \n\n### Parameters, Packages & Snowpark Session\nFirstly, we will define some parameters used within the Notebook, import the required packages and create a session for Snowpark.  Feel free to adjust the parameters to suit your need.\n\n#### Notebook Parameters\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fb9691-a820-46d8-a510-31dd57e6d768",
   "metadata": {
    "name": "database_schema_names",
    "language": "python",
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Source Data Database and Schema\nsrc_database = 'SNOWFLAKE_SAMPLE_DATA'\nsrc_schema = 'TPCH_SF1' # <-- Modify this if you want to test with one of the larger data scale-factors. e.g. TPCH_SF1, TPCH_SF10, TPCH_SF100, TPCH_SF1000\n\n# Database to use to create Schemas\nsess_db = 'SIMON' # The database within which we will create our Feature Store (schema), and data-source schema.\n\norg = 'TPCHSF1'                       # Name for working Schema and used to derive Feature Store Name\nfs_name = f\"{org}_FEATURE_STORE\"      # Feature Store Name.  This will create a Schema to contain our Feature Store database objects\nmr_name = f\"{org}_MODEL_REGISTRY\"     # Model-Registry Name.\nnum_spine_rows = 10                   # Maximum number of rows to use when sampling source data for Spine Entity Keys."
  },
  {
   "cell_type": "markdown",
   "id": "a238e9fd-a6d7-46e7-81ab-bb5630f33d2d",
   "metadata": {
    "name": "packages_md",
    "collapsed": false
   },
   "source": "#### Packages"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd938f7-2401-4ce1-bf11-0a1a8bdb77bc",
   "metadata": {
    "name": "package_imports",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": "### Packages\n## Python packages\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\nfrom datetime import date, datetime\nimport time \nfrom decimal import Decimal\nimport pkg_resources\npkg_resources.require(\"numpy==1.23.5\")\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_colwidth', 500)\nimport streamlit as st\nimport tabulate\nfrom pathlib import Path\nimport json\n\n## SNOWFLAKE\n\n## Snowpark\n#import snowflake.snowpark as S\n#from snowflake.snowpark import Session\n#from snowflake.snowpark import Analytics\nfrom snowflake.snowpark.version import VERSION\nfrom snowflake.snowpark import functions as F, types as T\nfrom snowflake.snowpark.types import StringType\n# Snowpark functions representing some SQL functions we need\ntryparsejson = F.builtin('TRY_PARSE_JSON')\ntimestampadd = F.builtin('TIMESTAMPADD')\n\n# Snowflake Feature Store\nfrom snowflake.ml.feature_store import FeatureStore, CreationMode\nfrom snowflake.ml.feature_store import Entity, FeatureView\nfrom snowflake.ml.utils import connection_params\n\n# Snowflake ML preprocessing\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder\n\n# Model Registry\nfrom snowflake.ml.registry import registry\nfrom snowflake.ml.model import custom_model\nfrom typing import Optional\nfrom snowflake.ml.model.model_signature import FeatureSpec, DataType, ModelSignature"
  },
  {
   "cell_type": "markdown",
   "id": "a3bd3ad3-89a0-4972-9997-98aaace9a6f1",
   "metadata": {
    "name": "session_md",
    "resultHeight": 46,
    "collapsed": false
   },
   "source": "#### Session\nSetup Snowflake connection and database and return execution enviroment state.\n"
  },
  {
   "cell_type": "code",
   "id": "fbef2b6b-bdbb-4608-96d7-c89de34a6516",
   "metadata": {
    "language": "python",
    "name": "create_session",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# CREATE SESSION\n# ## Using Snowflake Notebook\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0835037b-925a-440c-bcbb-f330f71207a7",
   "metadata": {
    "name": "describe_session",
    "language": "python",
    "resultHeight": 329,
    "codeCollapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SnowflakeLoginOptions() is in private preview since 0.2.0. Do not use it in production. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in non-Snowflake Notebook\n",
      "================================================================================\n",
      "\n",
      "Connection Established with the following parameters:\n",
      "Account                      : AK32940\n",
      "User                         : SIMON\n",
      "Role                         : SYSADMIN\n",
      "Database                     : SIMON\n",
      "Schema                       : SCRATCH\n",
      "Warehouse Name               : SIMON_XS\n",
      "Warehouse Type               : STANDARD\n",
      "Warehouse State              : STARTED\n",
      "Warehouse Size               : X-SMALL\n",
      "Warehouse Available Resource : 100\n",
      "SPCS Compute Name            : NA\n",
      "Notebook Client Execution    : USER_CLIENT\n",
      "Snowflake version            : 8.44.2\n",
      "Snowpark for Python version  : 1.25.0 \n",
      "\n"
     ]
    }
   ],
   "source": "session.sql_simplifier_enabled = True\n\n# Capture and Print the Current Environment Details\nsnowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\nsnowpark_version = VERSION\nsession_role = session.get_current_role().replace('\"', \"\")\nsession_database = session.get_current_database().replace('\"', \"\")\nsession_schema = session.get_current_schema().replace('\"', \"\")\nsession_vw = session.get_current_warehouse().replace('\"', \"\")\n\nvw_status = session.sql(f\"\"\"show warehouses like '{session_vw}' \"\"\").collect()[0]\nvw_type = vw_status['type']\nvw_state = vw_status['state']\nvw_size = vw_status['size'].upper()\nvw_available = vw_status['available']\nprint('================================================================================')\nprint('\\nConnection Established with the following parameters:')\nprint(f'Account                      : {session.sql(\"select current_account()\").collect()[0][0]}')\nprint(f'User                         : {snowflake_environment[0][0]}')\nprint(f'Role                         : {session_role}')\nprint(f'Database                     : {session_database}')\nprint(f'Schema                       : {session_schema}')\nprint(f'Warehouse Name               : {session_vw}')\nprint(f'Warehouse Type               : {vw_type}')\nprint(f'Warehouse State              : {vw_state}')\nprint(f'Warehouse Size               : {vw_size}')\nprint(f'Warehouse Available Resource :{vw_available}')\nprint(f'Snowflake version            : {snowflake_environment[0][1]}')\nprint(f'Snowpark for Python version  : {snowpark_version[0]}.{snowpark_version[1]}.{snowpark_version[2]} \\n')"
  },
  {
   "cell_type": "code",
   "id": "c20e6783-7a1a-43c6-8b3d-933ed5a5406e",
   "metadata": {
    "language": "python",
    "name": "create_working_schema_for_datasets",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Name of the schema where we will persist our generated training datasets\nsession.sql(f''' Create schema if not exists {sess_db}.{org}''').collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc0f3820-05ff-442e-a77d-d5ee5c59852e",
   "metadata": {
    "collapsed": false,
    "name": "TPCH_EXAMPLE_DATA",
    "resultHeight": 1210
   },
   "source": "To make a start, we need some easily accessible data to work with that contains recognisable business entities and hierarchy. We will make use of the Transaction Processing Councils Adhoc (TPCH) dataset that is made available as a data-share in the `SNOWFLAKE_SAMPLE_DATA` database in all accounts. You can read more about the TPCH Data [here](https://docs.snowflake.com/en/user-guide/sample-data-tpch) and see the ER Diagram that represents that data below."
  },
  {
   "cell_type": "code",
   "id": "10945b83-8f82-4e0b-9686-13d73645c96b",
   "metadata": {
    "language": "python",
    "name": "TPCH_data_model_image",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.image(\"https://docs.snowflake.com/en/_images/sample-data-tpch-schema.png\") # ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d8c98fe-bd8c-4b8e-8886-9ffd7683add2",
   "metadata": {
    "name": "PRIMARY_KEY_CONSTRAINTS",
    "collapsed": false
   },
   "source": "\nFor this example, we will assume that all of the tables contain raw-features that will form the foundation of our Feature Store.  The Primary Key for each table will be used as the Entity key of our Entities, and the other columns will form the Features.  Data-shared Snowflake tables do not have Primary Key constraints visible on them within Snowflake metadata. The following contraints show the Primary Key for each table.\n\n```sql\nALTER TABLE PART\n  ADD CONSTRAINT part_kpey\n     PRIMARY KEY (P_PARTKEY);\n\nALTER TABLE SUPPLIER\n  ADD CONSTRAINT supplier_pkey\n     PRIMARY KEY (S_SUPPKEY);\n\nALTER TABLE PARTSUPP\n  ADD CONSTRAINT partsupp_pkey\n     PRIMARY KEY (PS_PARTKEY, PS_SUPPKEY);\n\nALTER TABLE CUSTOMER\n  ADD CONSTRAINT customer_pkey\n     PRIMARY KEY (C_CUSTKEY);\n\nALTER TABLE ORDERS\n  ADD CONSTRAINT orders_pkey\n     PRIMARY KEY (O_ORDERKEY);\n\nALTER TABLE LINEITEM\n  ADD CONSTRAINT lineitem_pkey\n     PRIMARY KEY (L_ORDERKEY, L_LINENUMBER);\n\nALTER TABLE NATION\n  ADD CONSTRAINT nation_pkey\n     PRIMARY KEY (N_NATIONKEY);\n\nALTER TABLE REGION\n  ADD CONSTRAINT region_pkey\n     PRIMARY KEY (R_REGIONKEY);\n```       \n\nTwo of the tables contain multi-column (compound) Primary Keys; `LINEITEM` and `PARTSUPP`.\n\nData models often have a mix of time-varying and non-time-varying tables. This TPCH data-model is simpler and not time-varying, in that there are no timestamps (e.g. LAST_MODIFIED_TS) to record multiple values for the features through time for a specific Entity. Each table records only the current/as-is values for an Entity so there is record of changes (updates) to the data per primary-key.  Therefore, we do not need to include a Timestamp column when creating the FeatureViews for this data. \n\nWe will create a dataframe for each table that we will use as a proxy to represent the base level FeatureViews in our Feature Store.  In each dataframe we will rename any of the primary or secondary key columns, removing the table-name acronym prefix present in the source tables, in order to make the key column names consistent across all tables. e.g. \n\n`P_PARTKEY & PS_PARTKEY --> PARTKEY`"
  },
  {
   "cell_type": "markdown",
   "id": "84387f55-2473-4af6-8e03-6c95d06bd3fb",
   "metadata": {
    "name": "BASE_DATAFRAMES",
    "collapsed": false
   },
   "source": "### Create base level dataframes for FeatureViews"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1bec8537-1989-44bc-9673-6ed1e951a166",
   "metadata": {
    "name": "source_data_dataframes",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# CUSTOMER\ncustomer_table = 'CUSTOMER'\ncustomer_sdf = session.table(f'{src_database}.{src_schema}.{customer_table}') \\\n                        .with_columns(['CUSTKEY','NATIONKEY'],[F.col('C_CUSTKEY'), F.col('C_NATIONKEY')]) \\\n                        .drop('C_CUSTKEY', 'C_NATIONKEY')\ncustomer_nosk_sdf = customer_sdf.drop('NATIONKEY')\n\n\n# LINEITEM\nlineitem_table = 'LINEITEM'\nlineitem_sdf = session.table(f'{src_database}.{src_schema}.{lineitem_table}') \\\n                        .with_columns(['ORDERKEY','LINENUMBER', 'SUPPKEY', 'PARTKEY'],[F.col('L_ORDERKEY'), F.col('L_LINENUMBER'),  F.col('L_SUPPKEY'), F.col('L_PARTKEY')]) \\\n                        .drop('L_ORDERKEY', 'L_LINENUMBER', 'L_PARTKEY', 'L_SUPPKEY')\nlineitem_nosk_sdf = lineitem_sdf.drop('SUPPKEY','PARTKEY')\n\n\n# NATION\nnation_table = 'NATION'\nnation_sdf = session.table(f'{src_database}.{src_schema}.{nation_table}') \\\n                        .with_columns(['NATIONKEY','REGIONKEY'],[F.col('N_NATIONKEY'),F.col('N_REGIONKEY')]) \\\n                        .drop('N_NATIONKEY', 'N_REGIONKEY')\nnation_nosk_sdf = nation_sdf.drop('REGIONKEY')\n\n\n# ORDER\norders_table = 'ORDERS'\norders_sdf = session.table(f'{src_database}.{src_schema}.{orders_table}') \\\n                .with_columns(['ORDERKEY','CUSTKEY'],[F.col('O_ORDERKEY'),F.col('O_CUSTKEY')]) \\\n                .drop('O_ORDERKEY', 'O_CUSTKEY')\norders_nosk_sdf = orders_sdf.drop('CUSTKEY')\n\n\n# PART\npart_table = 'PART'\npart_sdf = session.table(f'{src_database}.{src_schema}.{part_table}') \\\n                        .with_columns(['PARTKEY'],[F.col('P_PARTKEY')]) \\\n                        .drop('P_PARTKEY')\n\n\n# PART SUPPLIER\npartsupp_table = 'PARTSUPP'\npartsupp_sdf = session.table(f'{src_database}.{src_schema}.{partsupp_table}') \\\n                        .with_columns(['SUPPKEY','PARTKEY'],[F.col('PS_SUPPKEY'), F.col('PS_PARTKEY')]) \\\n                        .drop('PS_SUPPKEY', 'PS_PARTKEY')\n\n\n# REGION\nregion_table = 'REGION'\nregion_sdf = session.table(f'{src_database}.{src_schema}.{region_table}') \\\n                        .with_columns(['REGIONKEY'],[F.col('R_REGIONKEY')]) \\\n                        .drop('R_REGIONKEY')\n\n\n# SUPPLIER\nsupplier_table = 'SUPPLIER'\nsupplier_sdf = session.table(f'{src_database}.{src_schema}.{supplier_table}') \\\n                        .with_columns(['SUPPKEY','NATIONKEY'],[F.col('S_SUPPKEY'),F.col('S_NATIONKEY')]) \\\n                        .drop('S_SUPPKEY', 'S_NATIONKEY')\nsupplier_nosk_sdf = supplier_sdf.drop('S_SUPPKEY', 'S_NATIONKEY','NATIONKEY')"
  },
  {
   "cell_type": "markdown",
   "id": "dafaa21b-6b01-43bb-bb91-d69418ea57d1",
   "metadata": {
    "collapsed": false,
    "name": "FEATURE_STORE",
    "resultHeight": 102
   },
   "source": "## Feature Store\n\nWe now need to create a Feature Store to develop in.  FeatureStore creates and/or returns an instance of the FeatureStore class in python.  Within Snowflake this creates a Schema and some database tags that are used to identify it as a FeatureStore."
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3cbf76f1-5c69-47d1-be55-9bf03d4e95fc",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "create_feature_store",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# session.sql(f''' Drop schema if exists {org}_FEATURE_STORE''').collect()\n\nfs =  FeatureStore(\n        session=session,\n        database=sess_db,\n        name=fs_name,\n        default_warehouse=\"SIMON_XS\",\n        creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "598b2eaa-77f5-4fb0-9ce8-66edef45846f",
   "metadata": {
    "collapsed": false,
    "name": "ENTITIES",
    "resultHeight": 102
   },
   "source": "## Entities\n\nWe now need create Entities for the various units-of-analysis and their identifying keys that are represented in our Source Tables and ER model.\n\nYou can think of Entities as the 'glue' that enables features stored across multiple FeatureViews in the Feature Store to be combined to return data for training/inference etc."
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "39e0bd28-570f-4fcc-a636-3e0a18037ade",
   "metadata": {
    "name": "create_entities",
    "language": "python",
    "resultHeight": 58,
    "codeCollapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/py_snowpark_df_ml_fs_1_7_0_v1/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity CUSTOMER already exists. Skip registration.\n",
      "  return f(self, *args, **kargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/py_snowpark_df_ml_fs_1_7_0_v1/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity LINEITEM already exists. Skip registration.\n",
      "  return f(self, *args, **kargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/py_snowpark_df_ml_fs_1_7_0_v1/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity NATION already exists. Skip registration.\n",
      "  return f(self, *args, **kargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/py_snowpark_df_ml_fs_1_7_0_v1/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity ORDERS already exists. Skip registration.\n",
      "  return f(self, *args, **kargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/py_snowpark_df_ml_fs_1_7_0_v1/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity PART already exists. Skip registration.\n",
      "  return f(self, *args, **kargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/py_snowpark_df_ml_fs_1_7_0_v1/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity PART_SUPPLIER already exists. Skip registration.\n",
      "  return f(self, *args, **kargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/py_snowpark_df_ml_fs_1_7_0_v1/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity REGION already exists. Skip registration.\n",
      "  return f(self, *args, **kargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/py_snowpark_df_ml_fs_1_7_0_v1/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity SUPPLIER already exists. Skip registration.\n",
      "  return f(self, *args, **kargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Entity(name=SUPPLIER, join_keys=['SUPPKEY'], owner=None, desc=Supplier entity)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# Customer Entity Definition \ncustomer_entity = Entity(\n    name=\"CUSTOMER\",\n    join_keys=[\"CUSTKEY\"],\n    desc=\"Customer entity\"\n    )\nfs.register_entity(customer_entity)\n\n\n# Lineitem Entity Definition \nlineitem_entity = Entity(\n    name=\"LINEITEM\",\n    join_keys=['ORDERKEY','LINENUMBER'],\n    desc=\"Lineitem entity\"\n    )\n#    join_keys=['ORDERKEY','PARTKEY', 'SUPPKEY','LINENUMBER'],\nfs.register_entity(lineitem_entity)\n\n\n# Nation Entity Definition \nnation_entity = Entity(\n    name=\"NATION\",\n    join_keys=[\"NATIONKEY\"],\n    desc=\"Nation entity\"\n    )\nfs.register_entity(nation_entity)\n\n# Order Entity Definition \norder_entity = Entity(\n    name=\"ORDERS\",\n    join_keys=[\"ORDERKEY\"],\n    desc=\"Order entity\"\n    )\nfs.register_entity(order_entity)\n\n\n# Part Entity Definition \npart_entity = Entity(\n    name=\"PART\",\n    join_keys=[\"PARTKEY\"],\n    desc=\"Part entity\"\n    )\nfs.register_entity(part_entity)\n\n\n# Part Supplier Entity Definition \npart_supplier_entity = Entity(\n    name=\"PART_SUPPLIER\",\n    join_keys=[\"PARTKEY\", \"SUPPKEY\"],\n    desc=\"Part Supplier entity\"\n    )\nfs.register_entity(part_supplier_entity)\n\n\n# Region Entity Definition \nregion_entity = Entity(\n    name=\"REGION\",\n    join_keys=[\"REGIONKEY\"],\n    desc=\"Region entity\"\n    )\nfs.register_entity(region_entity)\n\n\n# Supplier Entity Definition \nsupplier_entity = Entity(\n    name=\"SUPPLIER\",\n    join_keys=[\"SUPPKEY\"],\n    desc=\"Supplier entity\"\n    )\nfs.register_entity(supplier_entity)"
  },
  {
   "cell_type": "markdown",
   "id": "11578e45-2245-457c-bf5b-3eb00a48a2c2",
   "metadata": {
    "name": "list_entities_md",
    "collapsed": false
   },
   "source": "We can check out the Entities that we have just created."
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "53a725ba-2546-48ea-97d1-5d8e74470144",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "list_entity",
    "resultHeight": 284
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "|\"NAME\"         |\"JOIN_KEYS\"              |\"DESC\"                |\"OWNER\"   |\n",
      "-----------------------------------------------------------------------------\n",
      "|CUSTOMER       |[\"CUSTKEY\"]              |Customer entity       |SYSADMIN  |\n",
      "|LINEITEM       |[\"ORDERKEY,LINENUMBER\"]  |Lineitem entity       |SYSADMIN  |\n",
      "|NATION         |[\"NATIONKEY\"]            |Nation entity         |SYSADMIN  |\n",
      "|ORDERS         |[\"ORDERKEY\"]             |Order entity          |SYSADMIN  |\n",
      "|PART           |[\"PARTKEY\"]              |Part entity           |SYSADMIN  |\n",
      "|PART_SUPPLIER  |[\"PARTKEY,SUPPKEY\"]      |Part Supplier entity  |SYSADMIN  |\n",
      "|REGION         |[\"REGIONKEY\"]            |Region entity         |SYSADMIN  |\n",
      "|SUPPLIER       |[\"SUPPKEY\"]              |Supplier entity       |SYSADMIN  |\n",
      "-----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.list_entities().sort('NAME').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336bbba4-2c65-4d85-97fc-5d43138f1370",
   "metadata": {
    "collapsed": false,
    "name": "FEATUREVIEWS",
    "resultHeight": 128
   },
   "source": "## FeatureViews\n\nFeatureViews contain a collection of Features that we want to organise, derive and store together.  When a FeatureView is registered it creates a Snowflake tabular object, either a Dynamic Table or View,  within the Feature Store schema with some tags used to identify metadata related to it.\n\nWe will create a FeatureView for each data-source base dataframe and entity. In each FeatureView definition we add all the applicable `entities`, and for compound keys, also include the primary `entities` that they are made of.  For example for the Part-Suppler FeatureView, we add the Part and Supplier entities."
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e77eb92d-af81-450b-9cb0-e703d07034f5",
   "metadata": {
    "language": "python",
    "name": "create_featureviews",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# CUSTOMER FEATUREVIEW\ncustomer_features = customer_sdf.drop(\"CUSTKEY\").columns\n\n# Create Customer FeatureView in Feature Store\ncustomer_fv = FeatureView(\n    name = f\"FV_CUSTOMER\",\n    entities = [customer_entity],\n    feature_df = customer_nosk_sdf,\n    desc = f\"Customer feature view\"\n)\n# Register the Customer FeatureView in the schema, and add the python featureview to our list\ncustomer_fv_v1 = fs.register_feature_view(\n    feature_view = customer_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)\n\n\n# LINEITEM FEATUREVIEW\nlineitem_features = lineitem_sdf.drop(\"ORDERKEY\", \"PARTKEY\", 'SUPPKEY', 'LINENUMBER').columns\n\n# Create Lineitem FeatureView in Feature Store\nlineitem_fv = FeatureView(\n    name = f\"FV_LINEITEM\",\n    entities = [order_entity, lineitem_entity],\n    feature_df = lineitem_nosk_sdf,\n    desc = f\"Lineitem feature view\"\n)\n# Register the Lineitem FeatureView in the schema, and add the python featureview to our list\nlineitem_fv_v1 = fs.register_feature_view(\n    feature_view = lineitem_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)\n\n\n# NATION FEATUREVIEW\nnation_features = nation_sdf.drop('NATIONKEY').columns\n\n# Create Nation FeatureView in Feature Store\nnation_fv = FeatureView(\n    name = f\"FV_NATION\",\n    entities = [nation_entity],\n    feature_df = nation_nosk_sdf,\n    desc = f\"Nation feature view\"\n)\n# Register the Nation FeatureView in the schema, and add the python featureview to our list\nnation_fv_v1 = fs.register_feature_view(\n    feature_view = nation_fv,   # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)\n\n\n# ORDERS FEATUREVIEW\norders_features = orders_sdf.drop(\"ORDERKEY\", \"CUSTKEY\").columns\n\n# Create Order FeatureView in Feature Store\norders_fv = FeatureView(\n    name = f\"FV_ORDERS\",\n    entities = [order_entity],\n    feature_df = orders_nosk_sdf,\n    desc = f\"Order feature view\"\n)\n# Register the Order FeatureView in the schema, and add the python featureview to our list\norders_fv_v1 = fs.register_feature_view(\n    feature_view = orders_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)\n\n\n# PART FEATUREVIEW \npart_features = part_sdf.drop(\"PARTKEY\").columns\n\n# Create Part FeatureView in Feature Store\npart_fv = FeatureView(\n    name = f\"FV_PART\",\n    entities = [part_entity],\n    feature_df = part_sdf,\n    desc = f\"Part feature view\"\n)\n# Register the Part FeatureView in the schema, and add the python featureview to our list\npart_fv_v1 = fs.register_feature_view(\n    feature_view = part_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)\n\n\n# PART SUPPLIER FEATUREVIEW\npart_supp_features = partsupp_sdf.drop(\"PARTKEY\", \"SUPPKEY\").columns\n\n# Create Part Supplier FeatureView in Feature Store\npart_supplier_fv = FeatureView(\n    name = f\"FV_PART_SUPPLIER\",\n    entities = [part_entity, supplier_entity],\n    feature_df = partsupp_sdf,\n    desc = f\"Part Supplier feature view\"\n)\n# Register the Part Supplier FeatureView in the schema, and add the python featureview to our list\npart_supplier_fv_v1 = fs.register_feature_view(\n    feature_view = part_supplier_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)\n\n\n# REGION FEATUREVIEW\nregion_features = region_sdf.drop('REGIONKEY').columns\n\n# Create Region FeatureView in Feature Store\nregion_fv = FeatureView(\n    name = f\"FV_REGION\",\n    entities = [region_entity],\n    feature_df = region_sdf,\n    desc = f\"Region feature view\"\n)\n# Register the Region FeatureView in the schema, and add the python featureview to our list\nregion_fv_v1 = fs.register_feature_view(\n    feature_view = region_fv,   # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)\n\n\n# SUPPLIER FEATUREVIEW\nsupplier_features = supplier_sdf.drop(\"SUPPKEY\", \"NATIONKEY\").columns\n\n# Create Supplier FeatureView in Feature Store\nsupplier_fv = FeatureView(\n    name = f\"FV_SUPPLIER\",\n    entities = [supplier_entity],\n    feature_df = supplier_nosk_sdf,\n    desc = f\"Supplier feature view\"\n)\n# Register the Supplier FeatureView in the schema, and add the python featureview to our list\nsupplier_fv_v1 = fs.register_feature_view(\n    feature_view = supplier_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)"
  },
  {
   "cell_type": "markdown",
   "id": "448de280-69d7-4e80-b532-bde314ec2dd6",
   "metadata": {
    "name": "list_featureviews_md",
    "collapsed": false
   },
   "source": "We can list the base level FeatureViews we have just created to see the metadata related to them."
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3ad09c86-b489-4d64-a822-01caddca95d6",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "list_feature_view",
    "resultHeight": 693
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"NAME\"            |\"VERSION\"  |\"DATABASE_NAME\"  |\"SCHEMA_NAME\"              |\"CREATED_ON\"                |\"OWNER\"   |\"DESC\"                      |\"ENTITIES\"    |\"REFRESH_FREQ\"  |\"REFRESH_MODE\"  |\"SCHEDULING_STATE\"  |\"WAREHOUSE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|FV_CUSTOMER       |V1         |SIMON            |FS_ENTITIES_FEATURE_STORE  |2024-11-26 06:45:16.579000  |SYSADMIN  |Customer feature view       |[             |NULL            |NULL            |NULL                |NULL         |\n",
      "|                  |           |                 |                           |                            |          |                            |  \"CUSTOMER\"  |                |                |                    |             |\n",
      "|                  |           |                 |                           |                            |          |                            |]             |                |                |                    |             |\n",
      "|FV_LINEITEM       |V1         |SIMON            |FS_ENTITIES_FEATURE_STORE  |2024-11-26 06:45:20.735000  |SYSADMIN  |Lineitem feature view       |[             |NULL            |NULL            |NULL                |NULL         |\n",
      "|                  |           |                 |                           |                            |          |                            |  \"ORDERS\",   |                |                |                    |             |\n",
      "|                  |           |                 |                           |                            |          |                            |  \"LINEITEM\"  |                |                |                    |             |\n",
      "|                  |           |                 |                           |                            |          |                            |]             |                |                |                    |             |\n",
      "|FV_NATION         |V1         |SIMON            |FS_ENTITIES_FEATURE_STORE  |2024-11-26 06:45:24.434000  |SYSADMIN  |Nation feature view         |[             |NULL            |NULL            |NULL                |NULL         |\n",
      "|                  |           |                 |                           |                            |          |                            |  \"NATION\"    |                |                |                    |             |\n",
      "|                  |           |                 |                           |                            |          |                            |]             |                |                |                    |             |\n",
      "|FV_ORDERS         |V1         |SIMON            |FS_ENTITIES_FEATURE_STORE  |2024-11-26 06:45:27.854000  |SYSADMIN  |Order feature view          |[             |NULL            |NULL            |NULL                |NULL         |\n",
      "|                  |           |                 |                           |                            |          |                            |  \"ORDERS\"    |                |                |                    |             |\n",
      "|                  |           |                 |                           |                            |          |                            |]             |                |                |                    |             |\n",
      "|FV_PART           |V1         |SIMON            |FS_ENTITIES_FEATURE_STORE  |2024-11-26 06:45:31.800000  |SYSADMIN  |Part feature view           |[             |NULL            |NULL            |NULL                |NULL         |\n",
      "|                  |           |                 |                           |                            |          |                            |  \"PART\"      |                |                |                    |             |\n",
      "|                  |           |                 |                           |                            |          |                            |]             |                |                |                    |             |\n",
      "|FV_PART_SUPPLIER  |V1         |SIMON            |FS_ENTITIES_FEATURE_STORE  |2024-11-26 06:45:36.245000  |SYSADMIN  |Part Supplier feature view  |[             |NULL            |NULL            |NULL                |NULL         |\n",
      "|                  |           |                 |                           |                            |          |                            |  \"PART\",     |                |                |                    |             |\n",
      "|                  |           |                 |                           |                            |          |                            |  \"SUPPLIER\"  |                |                |                    |             |\n",
      "|                  |           |                 |                           |                            |          |                            |]             |                |                |                    |             |\n",
      "|FV_REGION         |V1         |SIMON            |FS_ENTITIES_FEATURE_STORE  |2024-11-26 06:45:40.122000  |SYSADMIN  |Region feature view         |[             |NULL            |NULL            |NULL                |NULL         |\n",
      "|                  |           |                 |                           |                            |          |                            |  \"REGION\"    |                |                |                    |             |\n",
      "|                  |           |                 |                           |                            |          |                            |]             |                |                |                    |             |\n",
      "|FV_SUPPLIER       |V1         |SIMON            |FS_ENTITIES_FEATURE_STORE  |2024-11-26 06:45:43.956000  |SYSADMIN  |Supplier feature view       |[             |NULL            |NULL            |NULL                |NULL         |\n",
      "|                  |           |                 |                           |                            |          |                            |  \"SUPPLIER\"  |                |                |                    |             |\n",
      "|                  |           |                 |                           |                            |          |                            |]             |                |                |                    |             |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.list_feature_views().sort('NAME').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e2471-7d28-4196-9ca0-dc3b0c7bcd33",
   "metadata": {
    "name": "RETRIEVING_DATA_FROM_FEATURESTORE",
    "resultHeight": 169,
    "collapsed": false
   },
   "source": "## Retrieve Data from the Feature Store\n\nNow we have setup the Feature Store objects we need representing our Entities and source tables using FeatureViews, we are ready to use it to retrieve data for training or inference. \n\nTo do this we need to create a spine dataframe containing the key-columns and their values we want returned in our training data set. The spine could be created with SQL or Snowpark Dataframe.  We create a simple dataframe to retrieve the required columns from the source table and sample rows from the result to provide a sample of keys.  At this step for model-training, you will also typically want to join the entity keys to a table containing the label/target column that we will use for training our model. We can also add any other data into the Spine that we want to appear in our returned Dataset.\n\nWe step through a number of different examples for retrieving a training Dataset from single through multiple entities and FeatureViews"
  },
  {
   "cell_type": "markdown",
   "id": "ed1f629e-06db-4c66-8314-0e2b52234a70",
   "metadata": {
    "name": "singlekey_retrieval_md",
    "resultHeight": 130,
    "collapsed": false
   },
   "source": "#### Single Key Column Entity - Customer Training Data \n\nFirst create a Spine dataframe.\n\n> **_NOTE:_**  We save the Spine dataframe as a Temporary Table so we can get reproducible results when we use the same Spine.\n\nOur spine contains a single Entity entity-key (CUSTKEY)."
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a0fc7f5f-b35a-4f14-b69b-d47065fcb2a2",
   "metadata": {
    "name": "customer_spine",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 345
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|\"CUSTKEY\"  |\n",
      "-------------\n",
      "|40357      |\n",
      "|48403      |\n",
      "|58610      |\n",
      "|64255      |\n",
      "|66672      |\n",
      "|82805      |\n",
      "|88259      |\n",
      "|92587      |\n",
      "|112440     |\n",
      "|138444     |\n",
      "-------------\n",
      "\n"
     ]
    }
   ],
   "source": "customer_spine_tbl = [sess_db,org,'CUSTOMER_SPINE']\n\n(customer_sdf.select('CUSTKEY')  \n    .distinct()  \n    .sample(n=num_spine_rows)  \n    .write.saveAsTable(customer_spine_tbl,mode = 'overwrite', table_type = 'temp')\n)\n\ncustomer_spine_df = session.table(customer_spine_tbl)\n    \ncustomer_spine_df.sort('CUSTKEY').show()"
  },
  {
   "cell_type": "markdown",
   "id": "d20f138b-ad67-48d0-84cf-b076b851eaa0",
   "metadata": {
    "name": "customer_generate_training_set_md",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": "Now we have a Spine we can generate a training set for a single FeatureView (customer_fv_v1), which returns a Dataframe object."
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac08ebb9-c6c2-4862-b0b9-761147eee31f",
   "metadata": {
    "name": "customer_generate_training_set",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 345
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTKEY\"  |\"C_NAME\"            |\"C_ADDRESS\"                              |\"C_PHONE\"        |\"C_ACCTBAL\"  |\"C_MKTSEGMENT\"  |\"C_COMMENT\"                                         |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|40357      |Customer#000040357  |vAsvErZPb1i                              |14-707-231-4746  |3429.18      |HOUSEHOLD       |nto beans. slyly bold excuses wake slyly sly        |\n",
      "|48403      |Customer#000048403  |LqPenCvrCPrz9AuB,MKIqGLMuHPlBt           |12-894-249-5575  |-466.51      |FURNITURE       |ges sleep furiously express, ironic theodolites...  |\n",
      "|58610      |Customer#000058610  |ZliIen 3HmZWYsbGCbd ckuB1jr5             |29-787-573-8778  |4542.85      |FURNITURE       |eas. regularly even tithes snooze furiously pac...  |\n",
      "|64255      |Customer#000064255  |VPf7iV9bdIvPjHu PV86fz,egamXih,TqA5gIBT  |26-980-754-5706  |543.28       |FURNITURE       |s furiously bold theodolites. ironic ideas hagg...  |\n",
      "|66672      |Customer#000066672  |zcOYvCUyfux                              |18-537-761-5743  |7190.80      |FURNITURE       |bold, pending grouches. final deposits wake acr...  |\n",
      "|82805      |Customer#000082805  |7czpajk,CWC0G 2L3GqHN6zxgMf6aqf,kJ       |33-391-335-1604  |8378.05      |MACHINERY       |ges. carefully bold deposits are. blithely quiet    |\n",
      "|88259      |Customer#000088259  |QWkzyArDeGXBxrfWNRm5EGPfQX5LjekBzV       |25-343-607-1077  |1597.19      |BUILDING        | about the fluffily regular asymptotes. even do...  |\n",
      "|92587      |Customer#000092587  |OoLWMfs,v3JCbd6uFEBKm7d7f7Z15ItUs84S     |22-275-571-6299  |8537.20      |HOUSEHOLD       |fully unusual packages thrash silently q            |\n",
      "|112440     |Customer#000112440  |V9as4RwGJZk2Z kOQdvTGkFGd                |34-947-611-8291  |251.30       |MACHINERY       |kages. blithely sly patterns detect carefully. ...  |\n",
      "|138444     |Customer#000138444  |c,iimKieTuHLoMZjLptrD2pm6OsX9ySnIxxIFhi  |32-163-825-1714  |7475.00      |AUTOMOBILE      |ntain permanently about the slyly express depos...  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_training_df = fs.generate_training_set(\n",
    "    customer_spine_df,\n",
    "    features = [customer_fv_v1]\n",
    ")\n",
    "\n",
    "customer_training_df.sort('CUSTKEY').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ab6b32-40a8-4956-b381-31484e6bfa9f",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "We can see the CUSTKEY we supplied in the Spine dataframe, and the Features from the FeatureView are returned."
  },
  {
   "cell_type": "markdown",
   "id": "63a247ff-cedd-4a0e-a89a-39fa16d0a9d1",
   "metadata": {
    "name": "multikey_column_md",
    "collapsed": false
   },
   "source": "#### Multi Key Column Entity - Lineitem Training Data \n\nAs before we create a Spine dataframe, this time including the two key columns that we need for the Lineitem FeatureView and its corresponding Entity. This is because the unique indentifier for Lineitems, is a compound-key consisting of both the ORDERKEY, and the LINENUMBER\n\n> **_NOTE:_** We also need to provide the secondary keys that were explicitly added to the lineitem FeatureView. "
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "325e87aa-a52a-4121-b596-b1d72d58e835",
   "metadata": {
    "name": "multikey_column_spine",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 345
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "|\"ORDERKEY\"  |\"LINENUMBER\"  |\n",
      "-----------------------------\n",
      "|173280      |4             |\n",
      "|564964      |5             |\n",
      "|718275      |2             |\n",
      "|1026627     |5             |\n",
      "|1757382     |5             |\n",
      "|1789542     |1             |\n",
      "|3042596     |1             |\n",
      "|4413249     |7             |\n",
      "|4955171     |3             |\n",
      "|5831846     |2             |\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": "lineitem_spine_tbl = [sess_db,org,'LINEITEM_SPINE']\n\n( lineitem_sdf.select('ORDERKEY','LINENUMBER') \n    .distinct() \n    .sample(n=num_spine_rows) \n    .write.saveAsTable(lineitem_spine_tbl,mode = 'overwrite', table_type = 'temp')\n)\n\nlineitem_spine_df = session.table(lineitem_spine_tbl)\n    \nlineitem_spine_df.sort('ORDERKEY', 'LINENUMBER').show()"
  },
  {
   "cell_type": "markdown",
   "id": "7695f504-f551-48f6-9671-91ac7c2b78d2",
   "metadata": {
    "name": "multikey_column_retrieval_md",
    "resultHeight": 42,
    "collapsed": false
   },
   "source": [
    "Now we can retreive the training dataframe from the single FeatureView `lineitem_fv_v1`, using the compound key-columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a398de92-4548-4e42-9959-3e4c4a5bc8ec",
   "metadata": {
    "name": "multikey_column_retrieval",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 351
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDERKEY\"  |\"LINENUMBER\"  |\"L_QUANTITY\"  |\"L_EXTENDEDPRICE\"  |\"L_DISCOUNT\"  |\"L_TAX\"  |\"L_RETURNFLAG\"  |\"L_LINESTATUS\"  |\"L_SHIPDATE\"  |\"L_COMMITDATE\"  |\"L_RECEIPTDATE\"  |\"L_SHIPINSTRUCT\"   |\"L_SHIPMODE\"  |\"L_COMMENT\"                                 |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|173280      |4             |35.00         |68351.85           |0.07          |0.06     |R               |F               |1994-01-29    |1994-03-12      |1994-02-23       |TAKE BACK RETURN   |SHIP          | sly foxes. acc                             |\n",
      "|564964      |5             |18.00         |32668.02           |0.01          |0.04     |A               |F               |1994-07-06    |1994-07-27      |1994-07-18       |TAKE BACK RETURN   |MAIL          |ly pending pinto beans u                    |\n",
      "|718275      |2             |28.00         |42180.60           |0.05          |0.08     |N               |O               |1998-09-28    |1998-09-24      |1998-10-08       |TAKE BACK RETURN   |TRUCK         |instructions sleep beyond the quickly fina  |\n",
      "|1026627     |5             |18.00         |17136.54           |0.07          |0.02     |R               |F               |1993-01-11    |1993-03-21      |1993-01-15       |TAKE BACK RETURN   |SHIP          |eans: furiously ev                          |\n",
      "|1757382     |5             |22.00         |29577.02           |0.01          |0.01     |N               |O               |1998-04-16    |1998-04-01      |1998-04-23       |DELIVER IN PERSON  |AIR           |special sentiments.                         |\n",
      "|1789542     |1             |7.00          |8506.89            |0.00          |0.02     |N               |O               |1996-06-27    |1996-07-02      |1996-06-28       |NONE               |SHIP          |dencies print along the unusual accounts.   |\n",
      "|3042596     |1             |5.00          |8188.40            |0.00          |0.02     |A               |F               |1992-10-25    |1992-09-09      |1992-11-03       |TAKE BACK RETURN   |FOB           |e carefully about                           |\n",
      "|4413249     |7             |11.00         |14050.19           |0.10          |0.03     |N               |O               |1996-06-03    |1996-07-06      |1996-06-10       |TAKE BACK RETURN   |SHIP          | final package                              |\n",
      "|4955171     |3             |44.00         |80604.48           |0.05          |0.02     |A               |F               |1993-11-05    |1993-10-30      |1993-11-10       |DELIVER IN PERSON  |AIR           |ular foxes.                                 |\n",
      "|5831846     |2             |33.00         |45423.51           |0.06          |0.07     |R               |F               |1992-10-24    |1992-11-08      |1992-11-23       |NONE               |TRUCK         |ely ironic accounts boost quickly accordi   |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lineitem_training = fs.generate_training_set(\n",
    "    lineitem_spine_df,\n",
    "    features = [lineitem_fv_v1]\n",
    ")\n",
    "\n",
    "lineitem_training.sort('ORDERKEY', 'LINENUMBER').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b7202-3a7f-4923-8ffa-44346b56fdcd",
   "metadata": {
    "name": "cell58",
    "collapsed": false
   },
   "source": "In the result we can see the entity-key columns that we explicitly added to our Spine and FeatureView definition and the 'features' all prefixed with `L_`."
  },
  {
   "cell_type": "markdown",
   "id": "87345729-2de4-4a2e-8738-13e4bc106491",
   "metadata": {
    "name": "multiple_entities_md",
    "collapsed": false
   },
   "source": "####  Multiple Entities - Using Primary and Secondary Keys derived from a single Source table\n\nLet's try adding additional FeatureViews into the `generate_training_set` function to return features that match on the additional secondary keys provided in the lineitem table. \n\nThis will enable us to return features from the `part`, `supplier`, and `partsupp` FeatureViews, using the Secondary Keys supplied in the `lineitem` table, using the Entity key-columns of those.\n\nAs before we need create a Spine to explicitly provide the keys for the additional entities; `part`, `supplier`"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1acc4a9d-b77d-4d82-b5d2-93d72ad27927",
   "metadata": {
    "name": "multiple_entities_spine",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 345
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "|\"ORDERKEY\"  |\"LINENUMBER\"  |\"PARTKEY\"  |\"SUPPKEY\"  |\n",
      "-----------------------------------------------------\n",
      "|173090      |6             |118494     |8495       |\n",
      "|2360514     |6             |147980     |5523       |\n",
      "|2365956     |3             |133959     |6473       |\n",
      "|2644133     |3             |63027      |8040       |\n",
      "|3242246     |4             |94382      |9401       |\n",
      "|3502467     |4             |116181     |6182       |\n",
      "|4045989     |3             |36656      |6657       |\n",
      "|4700679     |1             |198776     |3815       |\n",
      "|5288550     |2             |60542      |5555       |\n",
      "|5371170     |4             |165853     |886        |\n",
      "-----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lineitem_orders_part_supplier_spine_tbl = [sess_db, org,'LINEITEM_ORDERS_PART_SUPPLIER_SPINE']\n",
    "\n",
    "lineitem_sdf.select('ORDERKEY','LINENUMBER','PARTKEY', 'SUPPKEY') \\\n",
    "    .distinct() \\\n",
    "    .sample(n=num_spine_rows) \\\n",
    "    .write.saveAsTable(lineitem_orders_part_supplier_spine_tbl, mode = 'overwrite', table_type = 'temp')\n",
    "\n",
    "lineitem_orders_part_supplier_spine_df = session.table(lineitem_orders_part_supplier_spine_tbl)\n",
    "\n",
    "lineitem_orders_part_supplier_spine_df.sort('ORDERKEY', 'LINENUMBER','PARTKEY', 'SUPPKEY').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f59e92-2c2e-4df5-962e-fdcce7a79ab6",
   "metadata": {
    "name": "multikey_single_featureview_md",
    "resultHeight": 92,
    "collapsed": false
   },
   "source": [
    "If we just use the Lineitem featureview, as before, we can see that our result now includes the additional keys that we provided in the Spine.  The Spine is left-joined to each FeatureView and the keys supplied in the Spine are included and the result.  For each FeatureView we want to retrive features from, we only need to provide the specific entity key-columns for that FeatureView within the Spine.  In effect, the Spine contains the entity-hierarchy information within, based on the relations between those entity key-columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c348222d-3b6e-4e24-9b66-d8ce83fa0bec",
   "metadata": {
    "name": "multikey_single_featureview",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 351
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDERKEY\"  |\"LINENUMBER\"  |\"PARTKEY\"  |\"SUPPKEY\"  |\"L_QUANTITY\"  |\"L_EXTENDEDPRICE\"  |\"L_DISCOUNT\"  |\"L_TAX\"  |\"L_RETURNFLAG\"  |\"L_LINESTATUS\"  |\"L_SHIPDATE\"  |\"L_COMMITDATE\"  |\"L_RECEIPTDATE\"  |\"L_SHIPINSTRUCT\"   |\"L_SHIPMODE\"  |\"L_COMMENT\"                                 |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|46822       |7             |112822     |2823       |12.00         |22017.84           |0.02          |0.07     |N               |O               |1997-04-16    |1997-05-28      |1997-04-27       |NONE               |MAIL          |ffily even deposits doubt fluffily even     |\n",
      "|268805      |3             |162211     |2212       |23.00         |29283.83           |0.00          |0.08     |N               |O               |1996-03-15    |1996-01-17      |1996-04-05       |COLLECT COD        |MAIL          |etly final                                  |\n",
      "|2320519     |4             |137160     |2187       |49.00         |58660.84           |0.04          |0.03     |N               |O               |1997-05-24    |1997-03-30      |1997-06-19       |NONE               |FOB           |uickly slyly                                |\n",
      "|2468323     |4             |190312     |313        |19.00         |26643.89           |0.00          |0.00     |N               |O               |1996-12-05    |1996-10-28      |1997-01-02       |NONE               |RAIL          | haggle blithely regular theodolites. pend  |\n",
      "|2618787     |5             |171829     |6864       |36.00         |68429.52           |0.08          |0.07     |R               |F               |1993-03-24    |1993-02-15      |1993-04-12       |COLLECT COD        |TRUCK         |old, pendi                                  |\n",
      "|2637346     |3             |179783     |4818       |1.00          |1862.78            |0.02          |0.07     |N               |O               |1996-05-14    |1996-06-12      |1996-05-18       |DELIVER IN PERSON  |AIR           |requests cajole fluffily. slyly daring h    |\n",
      "|3140966     |1             |33595      |3596       |48.00         |73372.32           |0.06          |0.08     |N               |O               |1996-11-13    |1997-01-15      |1996-12-04       |DELIVER IN PERSON  |SHIP          |ependencies. carefully iro                  |\n",
      "|3712422     |1             |52276      |9792       |24.00         |29478.48           |0.05          |0.04     |A               |F               |1994-01-24    |1994-01-06      |1994-01-29       |DELIVER IN PERSON  |RAIL          |ronic deposits. regular patte               |\n",
      "|4884610     |3             |89433      |1942       |2.00          |2844.86            |0.05          |0.04     |A               |F               |1992-07-16    |1992-08-01      |1992-08-09       |DELIVER IN PERSON  |MAIL          |asymptotes. slyly even deposit              |\n",
      "|5852803     |1             |81111      |1112       |28.00         |30579.08           |0.07          |0.06     |R               |F               |1993-06-24    |1993-06-10      |1993-06-28       |NONE               |TRUCK         |quests. final accounts ha                   |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "line_item_training = fs.generate_training_set(\n",
    "    lineitem_orders_part_supplier_spine_df,\n",
    "    features = [lineitem_fv_v1]\n",
    ")\n",
    "\n",
    "line_item_training.sort('ORDERKEY', 'LINENUMBER','PARTKEY', 'SUPPKEY').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d8509-f614-42ad-83e2-d5accd66b3a0",
   "metadata": {
    "name": "multikey_multi_featureview_md",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": [
    "If we now include the additional FeatureViews we want the features from, that are at the higher entity-hierarchy levels from lineitem the required joins will be created using the Spine key-columns to each of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "742625f7-d020-4ff9-b51f-e6b7669ef3a6",
   "metadata": {
    "name": "multikey_multi_featureview",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 351
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDERKEY\"  |\"LINENUMBER\"  |\"PARTKEY\"  |\"SUPPKEY\"  |\"L_QUANTITY\"  |\"L_EXTENDEDPRICE\"  |\"L_DISCOUNT\"  |\"L_TAX\"  |\"L_RETURNFLAG\"  |\"L_LINESTATUS\"  |\"L_SHIPDATE\"  |\"L_COMMITDATE\"  |\"L_RECEIPTDATE\"  |\"L_SHIPINSTRUCT\"   |\"L_SHIPMODE\"  |\"L_COMMENT\"                              |\"O_ORDERSTATUS\"  |\"O_TOTALPRICE\"  |\"O_ORDERDATE\"  |\"O_ORDERPRIORITY\"  |\"O_CLERK\"        |\"O_SHIPPRIORITY\"  |\"O_COMMENT\"                                         |\"P_NAME\"                                |\"P_MFGR\"        |\"P_BRAND\"  |\"P_TYPE\"                   |\"P_SIZE\"  |\"P_CONTAINER\"  |\"P_RETAILPRICE\"  |\"P_COMMENT\"             |\"S_NAME\"            |\"S_ADDRESS\"                          |\"S_PHONE\"        |\"S_ACCTBAL\"  |\"S_COMMENT\"                                         |\"PS_AVAILQTY\"  |\"PS_SUPPLYCOST\"  |\"PS_COMMENT\"                                        |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|173090      |6             |118494     |8495       |17.00         |25712.33           |0.04          |0.08     |A               |F               |1993-12-04    |1993-11-01      |1993-12-26       |COLLECT COD        |FOB           |usly. even deposits sleep                |F                |95997.01        |1993-08-11     |5-LOW              |Clerk#000000378  |0                 |al deposits haggle furiously furiously pending ...  |dark indian peru plum lavender          |Manufacturer#3  |Brand#32   |STANDARD PLATED NICKEL     |16        |WRAP BAG       |1512.49          |eas try to              |Supplier#000008495  |RGAKqbyph7l2r8FEDY                   |31-229-537-8994  |3759.81      |ickly alongside of the platelets. regular, iron...  |8715           |299.45           |hely final deposits: ironic, dogged requests ca...  |\n",
      "|2360514     |6             |147980     |5523       |13.00         |26363.74           |0.04          |0.02     |N               |O               |1996-01-09    |1996-02-06      |1996-01-18       |COLLECT COD        |TRUCK         |heodolites sleep. bold, fina             |O                |258210.85       |1995-11-10     |4-NOT SPECIFIED    |Clerk#000000010  |0                 |st; furiously regular accounts use quickly p        |saddle chiffon black wheat seashell     |Manufacturer#3  |Brand#31   |LARGE ANODIZED TIN         |11        |JUMBO CASE     |2027.98          |packages. reques        |Supplier#000005523  |loM,UvBG0qY86VDOroyLneGZkMx69nKO9    |33-179-120-1158  |2398.22      | to the final accounts haggle q                     |8035           |25.29            |uriously slyly bold instructions. special plate...  |\n",
      "|2365956     |3             |133959     |6473       |35.00         |69753.25           |0.07          |0.01     |N               |O               |1998-01-11    |1998-02-27      |1998-02-02       |NONE               |SHIP          |. doggedly ruthless plat                 |O                |230070.90       |1997-12-17     |1-URGENT           |Clerk#000000439  |0                 |ly even deposits cajole along the slyly             |dark ghost antique burnished gainsboro  |Manufacturer#1  |Brand#11   |LARGE BRUSHED TIN          |32        |JUMBO CAN      |1992.95          |yly across t            |Supplier#000006473  |u,9PUBiVGKHnDoP,b                    |32-889-606-4292  |8053.25      |packages. furiously regular asymptotes engage furi  |1331           |295.34           |ing foxes. fluffily sly platelets affix. carefu...  |\n",
      "|2644133     |3             |63027      |8040       |11.00         |10890.22           |0.05          |0.08     |N               |O               |1998-07-18    |1998-07-03      |1998-08-11       |COLLECT COD        |MAIL          |endencies are fluffily around            |O                |243247.22       |1998-05-28     |3-MEDIUM           |Clerk#000000414  |0                 |ly final requests use fi                            |chiffon metallic firebrick linen grey   |Manufacturer#3  |Brand#35   |SMALL PLATED BRASS         |38        |SM BAG         |990.02           | carefully fluffily ex  |Supplier#000008040  |TJUSGy72qAbW6ynoKp                   |21-850-239-4091  |3107.77      | packages. carefully special theodolites lose ac    |7062           |518.18           |kages. slyly regular requests sleep furiously e...  |\n",
      "|3242246     |4             |94382      |9401       |10.00         |13763.80           |0.02          |0.08     |N               |O               |1996-02-05    |1996-02-04      |1996-03-04       |DELIVER IN PERSON  |MAIL          |escapades. quickly bold pla              |O                |47167.04        |1995-12-08     |2-HIGH             |Clerk#000000493  |0                 |lithely by the furio                                |spring antique cornsilk green medium    |Manufacturer#3  |Brand#31   |STANDARD BURNISHED NICKEL  |37        |MED CASE       |1376.38          | blithely final dugout  |Supplier#000009401  |VfE1W9ZfSHf673xmaOXSB3vv,PGBkAZIVV   |18-117-294-2978  |21.01        | ideas use among the slyly final accounts. fluf...  |3958           |935.50           |es at the slyly ironic packages. quickly ruthle...  |\n",
      "|3502467     |4             |116181     |6182       |48.00         |57464.64           |0.07          |0.05     |N               |O               |1998-03-01    |1998-04-08      |1998-03-06       |DELIVER IN PERSON  |MAIL          |carefully carefully regular dep          |O                |203004.98       |1998-01-18     |3-MEDIUM           |Clerk#000000635  |0                 |ages are about the even asymptotes. final, even...  |slate misty metallic violet blush       |Manufacturer#3  |Brand#31   |LARGE BRUSHED STEEL        |20        |LG DRUM        |1197.18          |yly. even               |Supplier#000006182  |b j0rPTZhI,QRpdBbph1uCMGZiMMNpBv3HA  |27-130-469-8203  |7099.14      |ual requests. accounts boost. express dependenc...  |6466           |175.24           |c deposits cajole finally? slyly unusual deposi...  |\n",
      "|4045989     |3             |36656      |6657       |29.00         |46186.85           |0.06          |0.01     |N               |O               |1995-08-20    |1995-06-04      |1995-09-18       |DELIVER IN PERSON  |SHIP          |uickly furiously regular foxes           |P                |285700.38       |1995-04-26     |5-LOW              |Clerk#000000888  |0                 |y? ironic pinto beans dazzle slyly against the ...  |hot forest burnished steel powder       |Manufacturer#1  |Brand#14   |LARGE ANODIZED BRASS       |29        |WRAP CASE      |1592.65          |leep furiousl           |Supplier#000006657  |9a0mbaiz3,SVNBYkUX5nZrG              |18-420-339-5474  |9865.98      |ideas cajole across the slyly unusual packages....  |2352           |585.45           |r theodolites above the pending, final packages...  |\n",
      "|4700679     |1             |198776     |3815       |21.00         |39370.17           |0.00          |0.05     |R               |F               |1993-06-14    |1993-05-07      |1993-06-19       |DELIVER IN PERSON  |SHIP          | fluffily regula                         |F                |41338.67        |1993-02-16     |5-LOW              |Clerk#000000997  |0                 |platelets sleep. deposits across the ironic asy...  |rose mint lime slate white              |Manufacturer#5  |Brand#54   |MEDIUM PLATED BRASS        |23        |WRAP PKG       |1874.77          | even                   |Supplier#000003815  |X6Hv7E2gTir0 XMliDjn1NxTzDCiK        |12-795-283-3574  |767.18       |the quickly final platelets. final, regular req...  |1220           |545.41           |ove the ironic patterns? ironic, ruthless asymp...  |\n",
      "|5288550     |2             |60542      |5555       |46.00         |69116.84           |0.07          |0.07     |N               |O               |1998-07-21    |1998-08-06      |1998-08-02       |TAKE BACK RETURN   |RAIL          |g carefull                               |O                |165600.80       |1998-06-10     |3-MEDIUM           |Clerk#000000560  |0                 |deposits. fluffily even notornis b                  |hot burnished cream dim thistle         |Manufacturer#1  |Brand#11   |ECONOMY PLATED NICKEL      |13        |WRAP JAR       |1502.54          |encies. slyly eve       |Supplier#000005555  |OVDFyVFCvGZSMF                       |25-372-584-7324  |5781.63      |s sleep carefully ironic acc                        |4982           |598.87           |ajole. furiously regular pinto beans haggle qui...  |\n",
      "|5371170     |4             |165853     |886        |48.00         |92104.80           |0.05          |0.02     |N               |O               |1995-09-25    |1995-12-05      |1995-10-02       |DELIVER IN PERSON  |REG AIR       | across the fluffy pinto beans. silentl  |O                |163753.69       |1995-09-12     |2-HIGH             |Clerk#000000461  |0                 |l requests. slyly regular                           |smoke goldenrod ivory lawn tan          |Manufacturer#4  |Brand#43   |ECONOMY BRUSHED NICKEL     |11        |LG BAG         |1918.85          |en pains                |Supplier#000000886  |R52IgT6b0yBuU r8,dNRZVWRY            |11-329-720-1904  |-158.08      |ts during the blithely silent packages c            |7333           |260.54           |ly bold requests. express theodolites integrate...  |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lineitem_orders_part_supplier_training = fs.generate_training_set(\n",
    "    lineitem_orders_part_supplier_spine_df,\n",
    "    features = [lineitem_fv_v1, orders_fv_v1, part_fv_v1, supplier_fv_v1, part_supplier_fv_v1]\n",
    ")\n",
    "\n",
    "lineitem_orders_part_supplier_training.sort('ORDERKEY', 'LINENUMBER','PARTKEY', 'SUPPKEY').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352596b6-5eee-43ee-9593-6677dff943da",
   "metadata": {
    "name": "cell59",
    "collapsed": false
   },
   "source": "We can see in the result that the tablename prefix for each of the columns denotes the source table those features have been derived from.\n- `L_` = lineitem\n- `O_` = orders\n- `P_` = part\n- `S_` = supplier\n- `PS_` = part-supplier"
  },
  {
   "cell_type": "markdown",
   "id": "f63bfcd0-0a26-43f9-b8c8-cf1d69cbc820",
   "metadata": {
    "name": "multiple_entities_multiple_source_tables_md",
    "collapsed": false
   },
   "source": "####  Multiple Entities - Using Primary and Secondary Keys derived from a multiple Source tables\n\nLet's further extend our Spine by joining to the `orders`, `part` and `supplier` tables and filtering out the non-key columns.  i.e. any columns without an `_` in the column-name.\n\nThis will add the customer (`CUSTKEY`) from `orders`, the nation (`NATIONKEY`) from `customer` and the region (`REGIONKEY`) from `nation`."
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f092d673-356c-4eca-b8b7-cd4d7202626c",
   "metadata": {
    "name": "multiple_entities_multiple_source_tables_spine",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 345
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "|\"CUSTKEY\"  |\"LINENUMBER\"  |\"NATIONKEY\"  |\"ORDERKEY\"  |\"PARTKEY\"  |\"REGIONKEY\"  |\"SUPPKEY\"  |\n",
      "---------------------------------------------------------------------------------------------\n",
      "|15610      |4             |1            |2635969     |171803     |1            |9355       |\n",
      "|27628      |4             |12           |2610436     |72078      |2            |7093       |\n",
      "|32033      |1             |4            |5555648     |51463      |4            |3969       |\n",
      "|62255      |1             |0            |3290948     |184948     |0            |4949       |\n",
      "|63613      |4             |20           |2016578     |52590      |4            |7601       |\n",
      "|78599      |1             |0            |4834368     |35063      |0            |2573       |\n",
      "|80230      |4             |7            |2766945     |189850     |3            |4887       |\n",
      "|81821      |1             |2            |824896      |62093      |1            |4600       |\n",
      "|88987      |6             |17           |1257380     |72313      |1            |9835       |\n",
      "|91975      |1             |13           |4115072     |32334      |4            |4838       |\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lineitem_orders_part_supplier_spine_tbl = [sess_db, org,'LINEITEM_ORDERS_PART_SUPPLIER_CUSTOMER_NATION_REGION_SPINE']\n",
    "\n",
    "lineitem_order_part_supplier_customer_nation_region_spine_allcols_df = lineitem_sdf.select('ORDERKEY','LINENUMBER','PARTKEY', 'SUPPKEY') \\\n",
    "      .join(orders_sdf, on = 'ORDERKEY') \\\n",
    "      .join(customer_sdf, on = 'CUSTKEY') \\\n",
    "      .join(nation_sdf, on = 'NATIONKEY') \\\n",
    "\n",
    "# Get key columns from result\n",
    "keycols = [col for col in lineitem_order_part_supplier_customer_nation_region_spine_allcols_df.columns if '_' not in col]\n",
    "keycols.sort()\n",
    "\n",
    "lineitem_order_part_supplier_customer_nation_region_spine_allcols_df.select(keycols) \\\n",
    "      .distinct() \\\n",
    "      .sample(n=num_spine_rows) \\\n",
    "      .write.saveAsTable(lineitem_orders_part_supplier_spine_tbl, mode = 'overwrite', table_type = 'temp')\n",
    "\n",
    "lineitem_order_part_supplier_customer_nation_region_spine_df  = session.table(lineitem_orders_part_supplier_spine_tbl)\n",
    "\n",
    "lineitem_order_part_supplier_customer_nation_region_spine_df.sort(keycols).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6037b5-4371-43fe-bf26-6deb3b3dd89c",
   "metadata": {
    "name": "multiple_entities_multiple_source_tables_generate_training_md",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": "This Spine includes all the key-column information across all FeatureViews so we should now be able to retrieve features from all the FeatureViews we have created, joining them together, and returning a Lineitem level set of features.  The higher entity-hierarchy level features have been duplicated appropriately down to Lineitem level."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf3468-6d18-4bb6-b8be-4fb00259be64",
   "metadata": {
    "name": "multiple_entities_multiple_source_tables_generate_training",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 351
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTKEY\"  |\"LINENUMBER\"  |\"NATIONKEY\"  |\"ORDERKEY\"  |\"PARTKEY\"  |\"REGIONKEY\"  |\"SUPPKEY\"  |\"L_QUANTITY\"  |\"L_EXTENDEDPRICE\"  |\"L_DISCOUNT\"  |\"L_TAX\"  |\"L_RETURNFLAG\"  |\"L_LINESTATUS\"  |\"L_SHIPDATE\"  |\"L_COMMITDATE\"  |\"L_RECEIPTDATE\"  |\"L_SHIPINSTRUCT\"   |\"L_SHIPMODE\"  |\"L_COMMENT\"                           |\"N_NAME\"      |\"N_COMMENT\"                                         |\"P_NAME\"                                     |\"P_MFGR\"        |\"P_BRAND\"  |\"P_TYPE\"                 |\"P_SIZE\"  |\"P_CONTAINER\"  |\"P_RETAILPRICE\"  |\"P_COMMENT\"            |\"S_NAME\"            |\"S_ADDRESS\"                             |\"S_PHONE\"        |\"S_ACCTBAL\"  |\"S_COMMENT\"                                         |\"PS_AVAILQTY\"  |\"PS_SUPPLYCOST\"  |\"PS_COMMENT\"                                        |\"O_ORDERSTATUS\"  |\"O_TOTALPRICE\"  |\"O_ORDERDATE\"  |\"O_ORDERPRIORITY\"  |\"O_CLERK\"        |\"O_SHIPPRIORITY\"  |\"O_COMMENT\"                                         |\"R_NAME\"     |\"R_COMMENT\"                                         |\"C_NAME\"            |\"C_ADDRESS\"                               |\"C_PHONE\"        |\"C_ACCTBAL\"  |\"C_MKTSEGMENT\"  |\"C_COMMENT\"                                         |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|15610      |4             |1            |2635969     |171803     |1            |9355       |3.00          |5624.40            |0.04          |0.06     |N               |O               |1998-11-10    |1998-09-03      |1998-11-11       |DELIVER IN PERSON  |FOB           |rs sleep fluffily after the           |ARGENTINA     |al foxes promise slyly according to the regular...  |chartreuse lemon lace turquoise almond       |Manufacturer#3  |Brand#35   |PROMO BRUSHED COPPER     |2         |LG JAR         |1874.80          |ptotes                 |Supplier#000009355  |ATw43gXAnm3QIHdlX1S                     |27-371-891-4773  |9335.77      |gside of the ironic gifts. regular pinto beans ...  |7928           |414.78           | carefully ironic pinto beans wake furiously ir...  |O                |242646.98       |1998-07-20     |4-NOT SPECIFIED    |Clerk#000000295  |0                 |r deposits. slyly careful asymptotes use about the  |AMERICA      |hs use ironic, even requests. s                     |Customer#000015610  |6dQefjWYQ,hVupQkIk4S 95qg                 |11-197-664-4021  |8867.49      |BUILDING        |riously carefully ironic requests. fluffily reg...  |\n",
      "|27628      |4             |12           |2610436     |72078      |2            |7093       |13.00         |13650.91           |0.05          |0.00     |A               |F               |1994-12-26    |1994-11-09      |1995-01-05       |NONE               |SHIP          |about the furiously regular re        |JAPAN         |ously. final, express gifts cajole a                |cyan beige puff saddle cornflower            |Manufacturer#4  |Brand#43   |LARGE PLATED TIN         |38        |SM PKG         |1050.07          |rious theodo           |Supplier#000007093  |ZkZJTAjNMSo5MhOy                        |16-131-789-8723  |538.90       | decoys. accounts haggle blithely. quick, regul...  |5971           |340.92           |quests boost fluffily-- evenly final platelets ...  |F                |241090.56       |1994-09-07     |1-URGENT           |Clerk#000000232  |0                 |ctions above the slyly final pearls u               |ASIA         |ges. thinly even pinto beans ca                     |Customer#000027628  |aJGF4oa , WEtMLryEydEQaa61D8 oI           |22-353-170-6450  |5147.46      |HOUSEHOLD       | ruthless pinto beans cajole quickly. carefully...  |\n",
      "|32033      |1             |4            |5555648     |51463      |4            |3969       |16.00         |22631.36           |0.01          |0.01     |N               |O               |1998-08-09    |1998-07-14      |1998-09-07       |NONE               |RAIL          | requests are thin                    |EGYPT         |y above the carefully unusual theodolites. fina...  |chiffon black navajo rosy mint               |Manufacturer#3  |Brand#33   |ECONOMY BURNISHED TIN    |2         |JUMBO CAN      |1414.46          |ncies. quickly eve     |Supplier#000003969  |jTZvvV2MU7hqLZiI6ZqufbfGX1              |26-329-999-9943  |4986.19      |nstructions. quickly ironic pac                     |4982           |777.06           |olphins affix furiously! quickly even packages ...  |O                |129457.52       |1998-05-05     |1-URGENT           |Clerk#000000545  |0                 |eodolites. blithely spec                            |MIDDLE EAST  |uickly special accounts cajole carefully blithe...  |Customer#000032033  |S2,4wJP48Y9mLFjzV4Z0JTBXIIfR              |14-678-961-4340  |3469.83      |MACHINERY       | carefully regular accounts. express packages u...  |\n",
      "|62255      |1             |0            |3290948     |184948     |0            |4949       |49.00         |99614.06           |0.00          |0.03     |R               |F               |1993-12-31    |1994-02-03      |1994-01-28       |DELIVER IN PERSON  |TRUCK         |ely according                         |ALGERIA       | haggle. carefully final deposits detect slyly ...  |blush mint lace ghost steel                  |Manufacturer#2  |Brand#22   |ECONOMY ANODIZED COPPER  |16        |MED PACK       |2032.94          |es. fluffily regular   |Supplier#000004949  |Rcqi8k3vuqVDrHWebhehhBl0VMFMCJV2j f     |17-991-530-7017  |7774.62      |nding deposits. final, ironic requests wake. fu...  |5398           |203.57           |lly regular pinto beans detect after the slyly ...  |F                |313955.99       |1993-12-30     |3-MEDIUM           |Clerk#000000111  |0                 |iously ironic theodolites. blithely regular dep     |AFRICA       |lar deposits. blithely final packages cajole. r...  |Customer#000062255  |,ySh2Mf2iyUfUbJctWBQ0AI723 di             |10-911-452-2997  |4766.49      |HOUSEHOLD       |sly deposits are carefully pending depo             |\n",
      "|63613      |4             |20           |2016578     |52590      |4            |7601       |34.00         |52448.06           |0.02          |0.06     |A               |F               |1994-02-14    |1994-04-11      |1994-02-17       |NONE               |REG AIR       |. blithely ironic requests na         |SAUDI ARABIA  |ts. silent requests haggle. closely express pac...  |wheat goldenrod red indian turquoise         |Manufacturer#3  |Brand#31   |ECONOMY ANODIZED COPPER  |50        |WRAP BOX       |1542.59          |ole blithely ironi     |Supplier#000007601  |RTiDfo6aTuo1d IwsxnZak3gjjNwms9fQWr     |30-600-354-6597  |3841.92      |unusual asymptotes. always express ideas boost ...  |4394           |936.44           |ide of the daring, ironic dependencies. fluffil...  |F                |206910.05       |1994-02-01     |4-NOT SPECIFIED    |Clerk#000000299  |0                 |ironic, final packages.                             |MIDDLE EAST  |uickly special accounts cajole carefully blithe...  |Customer#000063613  |Pl7,aXtKw4e                               |30-248-728-6475  |7431.79      |FURNITURE       |l accounts could have to use blithely among the si  |\n",
      "|78599      |1             |0            |4834368     |35063      |0            |2573       |18.00         |17965.08           |0.04          |0.05     |R               |F               |1994-01-06    |1994-01-29      |1994-01-14       |COLLECT COD        |REG AIR       |lthily even dinos-- accounts          |ALGERIA       | haggle. carefully final deposits detect slyly ...  |mint peru drab tan medium                    |Manufacturer#4  |Brand#44   |SMALL BURNISHED COPPER   |31        |JUMBO BAG      |998.06           |hless packages inte    |Supplier#000002573  |9fNFFKoAJ90HZrDn0iGbGHq5j4P7BOHER       |23-558-420-3222  |460.37       |excuses are express foxes. cour                     |9462           |194.21           |inst the carefully regular frets. blithely iron...  |F                |43216.22        |1993-11-09     |3-MEDIUM           |Clerk#000000250  |0                 |cial packages are carefull                          |AFRICA       |lar deposits. blithely final packages cajole. r...  |Customer#000078599  |,JCe4u v rgPuucLDCJoo1WFVIVAMtVQnsau bUw  |10-101-961-1014  |3636.74      |MACHINERY       |eposits. final requests run slyly slyly bold de...  |\n",
      "|80230      |4             |7            |2766945     |189850     |3            |4887       |27.00         |52375.95           |0.10          |0.08     |A               |F               |1992-07-08    |1992-04-18      |1992-07-09       |COLLECT COD        |TRUCK         |sits among the furiously              |GERMANY       |l platelets. regular accounts x-ray: unusual, r...  |mint khaki puff sienna steel                 |Manufacturer#2  |Brand#23   |SMALL BURNISHED STEEL    |47        |JUMBO CASE     |1939.85          |serve. ironic,         |Supplier#000004887  |,TpTOfc1nx,PLUztfC5mpK74sd1GcZfU8QM     |17-114-984-7958  |7247.36      | ideas. final, pending packages wake. quickly f...  |7930           |303.27           |althily regular packages. quickly bold accounts...  |F                |233911.01       |1992-03-11     |4-NOT SPECIFIED    |Clerk#000000420  |0                 |tes. slyly regular pinto beans are al               |EUROPE       |ly final courts cajole furiously final excuse       |Customer#000080230  |iodg46xvEXv EjPCIE9kw,4EN                 |17-253-476-3169  |8807.16      |HOUSEHOLD       |d have to wake according to the final foxes. ac...  |\n",
      "|81821      |1             |2            |824896      |62093      |1            |4600       |16.00         |16881.44           |0.02          |0.02     |R               |F               |1993-01-28    |1993-03-07      |1993-02-15       |NONE               |REG AIR       |lithely carefully ironic accounts. e  |BRAZIL        |y alongside of the pending deposits. carefully ...  |moccasin brown gainsboro khaki navajo        |Manufacturer#1  |Brand#13   |PROMO BRUSHED COPPER     |37        |LG PACK        |1055.09          |ithely pen             |Supplier#000004600  |iGVGwr0UYHqbt5Mg                        |22-756-980-3416  |8513.63      |kages haggle furiously above the carefully bold     |2477           |777.31           |ideas are carefully blithely ironic accounts. f...  |F                |25827.89        |1993-01-13     |2-HIGH             |Clerk#000000173  |0                 |nic packages wake bold ideas. unusual somas sle...  |AMERICA      |hs use ironic, even requests. s                     |Customer#000081821  |OMDDws4FHfGJKroK96ju                      |12-615-171-4878  |5359.73      |BUILDING        |f the regular ideas. fluffily regular foxes caj...  |\n",
      "|88987      |6             |17           |1257380     |72313      |1            |9835       |36.00         |46271.16           |0.09          |0.00     |N               |O               |1998-10-22    |1998-09-17      |1998-11-17       |COLLECT COD        |FOB           |aves? furiously reg                   |PERU          |platelets. blithely pending dependencies use fl...  |spring gainsboro chartreuse lime aquamarine  |Manufacturer#1  |Brand#13   |LARGE POLISHED TIN       |1         |LG DRUM        |1285.31          |tructions              |Supplier#000009835  |MbMZ9KZ4B2GFPAtkTJESMS3mLs1CiMU1F5emUk  |30-205-790-9745  |6520.20      |ges. furiously bold instructions cajo               |9227           |99.12            |iously final packages. regular accounts sleep quic  |O                |211329.82       |1998-07-21     |5-LOW              |Clerk#000000389  |0                 |d furiously ironic requests-- quickly ir            |AMERICA      |hs use ironic, even requests. s                     |Customer#000088987  |WioflWDdQU                                |27-889-123-7877  |8102.06      |BUILDING        |fluffily silent requests: regular, pending plat...  |\n",
      "|91975      |1             |13           |4115072     |32334      |4            |4838       |12.00         |15195.96           |0.02          |0.06     |R               |F               |1993-10-02    |1993-09-06      |1993-10-23       |TAKE BACK RETURN   |AIR           |venly among the slyly regular pi      |JORDAN        |ic deposits are blithely about the carefully re...  |azure grey lavender saddle navy              |Manufacturer#4  |Brand#44   |SMALL POLISHED NICKEL    |23        |WRAP PKG       |1266.33          |e busy ide             |Supplier#000004838  |24L4lahMEi4Xe9nDFYMU                    |33-868-522-8983  |-889.12      |uriously close requests cajole carefully furiou...  |9717           |149.77           |ray. unusual, even grouches across the fluffily...  |F                |15785.56        |1993-07-31     |5-LOW              |Clerk#000000044  |0                 |ffix along the requests? slyly final                |MIDDLE EAST  |uickly special accounts cajole carefully blithe...  |Customer#000091975  |wxwfZBjWAqrjlUtces9W4 fiHHF4QFpt          |23-696-431-7055  |2109.87      |AUTOMOBILE      | packages-- carefully pending instructions inte...  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lineitem_order_part_supplier_customer_nation_region_spine_df_training = fs.generate_training_set(\n",
    "    lineitem_order_part_supplier_customer_nation_region_spine_df,\n",
    "    features = [lineitem_fv_v1, nation_fv_v1, part_fv_v1, supplier_fv_v1, part_supplier_fv_v1, orders_fv_v1, region_fv_v1, customer_fv_v1]\n",
    ")\n",
    "lineitem_order_part_supplier_customer_nation_region_spine_df_training.sort(keycols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f68dca-eb4d-4a2a-92e3-9496074793fb",
   "metadata": {
    "name": "show_multiple_entities_multiple_source_tables_md",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": [
    "We can see all the key-columns and all the features, prefixed by table synonym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "02d8abca-90f8-4f13-a4af-57cb65409862",
   "metadata": {
    "name": "show_multiple_entities_multiple_source_tables",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 1552
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CUSTKEY',\n",
       " 'LINENUMBER',\n",
       " 'NATIONKEY',\n",
       " 'ORDERKEY',\n",
       " 'PARTKEY',\n",
       " 'REGIONKEY',\n",
       " 'SUPPKEY',\n",
       " 'L_QUANTITY',\n",
       " 'L_EXTENDEDPRICE',\n",
       " 'L_DISCOUNT',\n",
       " 'L_TAX',\n",
       " 'L_RETURNFLAG',\n",
       " 'L_LINESTATUS',\n",
       " 'L_SHIPDATE',\n",
       " 'L_COMMITDATE',\n",
       " 'L_RECEIPTDATE',\n",
       " 'L_SHIPINSTRUCT',\n",
       " 'L_SHIPMODE',\n",
       " 'L_COMMENT',\n",
       " 'N_NAME',\n",
       " 'N_COMMENT',\n",
       " 'P_NAME',\n",
       " 'P_MFGR',\n",
       " 'P_BRAND',\n",
       " 'P_TYPE',\n",
       " 'P_SIZE',\n",
       " 'P_CONTAINER',\n",
       " 'P_RETAILPRICE',\n",
       " 'P_COMMENT',\n",
       " 'S_NAME',\n",
       " 'S_ADDRESS',\n",
       " 'S_PHONE',\n",
       " 'S_ACCTBAL',\n",
       " 'S_COMMENT',\n",
       " 'PS_AVAILQTY',\n",
       " 'PS_SUPPLYCOST',\n",
       " 'PS_COMMENT',\n",
       " 'O_ORDERSTATUS',\n",
       " 'O_TOTALPRICE',\n",
       " 'O_ORDERDATE',\n",
       " 'O_ORDERPRIORITY',\n",
       " 'O_CLERK',\n",
       " 'O_SHIPPRIORITY',\n",
       " 'O_COMMENT',\n",
       " 'R_NAME',\n",
       " 'R_COMMENT',\n",
       " 'C_NAME',\n",
       " 'C_ADDRESS',\n",
       " 'C_PHONE',\n",
       " 'C_ACCTBAL',\n",
       " 'C_MKTSEGMENT',\n",
       " 'C_COMMENT']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineitem_order_part_supplier_customer_nation_region_spine_df_training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387e63b-a908-46bf-91fd-109272b46d16",
   "metadata": {
    "name": "lowerlevel_features_from_higher_level_entity_md",
    "resultHeight": 274,
    "collapsed": false
   },
   "source": "####  Multiple Entities - Working with lower-level FeatureViews from a higher Entity level\n\nSometimes we may be building an ML model and working at an Entity level in the hierarchy, where we have Entities below it at a lower-level.  Recall, an Entity is essentially the Primary Key for a Business Entity.  That Primary Key, maybe used as the Secondary Key in other tables, or as part of a Compound Primary-Key in a lower-level.  It is highly likely that we will have multiple rows of data in the lower-level FeatureViews for each distinct Entity key-value from the higher level we are working at.\n\nLet's illustrate this with an example from our data.  We have an Orders table (FeatureView), and a Lineitem table (FeatureView).  We want to build a model to predict something related to Orders, so we need a record of training data per Order.  Within the Lineitem table we have mutliple records per Order, as it is part of the Compound Primary Key, where Linenumber provides uniqueness, within the ORDERKEY.  Lineitem includes additional raw features that we may want to include within our Training data.\n\nLet's check this out within the data, and look at a few different ways we might handle this."
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ed875280-1171-4ba9-b30d-c45d3b8b2868",
   "metadata": {
    "name": "show_orders_lineitem_entities_md",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 150
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "|\"NAME\"    |\"JOIN_KEYS\"              |\"DESC\"           |\"OWNER\"   |\n",
      "-------------------------------------------------------------------\n",
      "|LINEITEM  |[\"ORDERKEY,LINENUMBER\"]  |Lineitem entity  |SYSADMIN  |\n",
      "|ORDERS    |[\"ORDERKEY\"]             |Order entity     |SYSADMIN  |\n",
      "-------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.list_entities().filter(F.in_([F.col('NAME')], ['ORDERS','LINEITEM'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171c641-1e0b-49cb-9569-b6daa707cb06",
   "metadata": {
    "name": "order_lineitem_compound_key_md",
    "resultHeight": 83,
    "collapsed": false
   },
   "source": [
    "We can see the compound-key includes ORDERKEY in the Lineitem entity above.\n",
    "\n",
    "Lets get a sample of ORDERKEY's that have multiple rows in LINEITEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c6f56e0-3707-4c86-86e7-5d4025944ca6",
   "metadata": {
    "name": "get_multi_linenumber_orders",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 345
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "|\"ORDERKEY\"  |\"LINEITEM_COUNT\"  |\n",
      "---------------------------------\n",
      "|328135      |7                 |\n",
      "|827271      |6                 |\n",
      "|1959973     |6                 |\n",
      "|2890464     |6                 |\n",
      "|3659910     |3                 |\n",
      "|3745763     |5                 |\n",
      "|4000801     |2                 |\n",
      "|4136992     |6                 |\n",
      "|4210402     |5                 |\n",
      "|4556836     |7                 |\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": "multi_linenumber_orders_tbl = [sess_db, org,'MULTI_LINENUMBER_ORDERS']\n\nlineitem_sdf.select('ORDERKEY','LINENUMBER') \\\n    .distinct() \\\n    .group_by(\"ORDERKEY\") \\\n    .agg((F.col(\"LINENUMBER\"), \"count\")) \\\n    .with_column_renamed( F.col(\"COUNT(LINENUMBER)\"),'LINEITEM_COUNT') \\\n    .filter(F.col(\"LINEITEM_COUNT\") >1) \\\n    .sample(n= num_spine_rows) \\\n    .write.saveAsTable(multi_linenumber_orders_tbl, mode = 'overwrite', table_type = 'temp')\n\nmulti_linenumber_orders_df = session.table(multi_linenumber_orders_tbl)\nmulti_linenumber_orders_df.sort(F.col('LINEITEM_COUNT').desc() ).show()"
  },
  {
   "cell_type": "markdown",
   "id": "7fdc221a-8e01-4ef9-bf83-89f1f149cc73",
   "metadata": {
    "name": "create_order_spine_for_multi_linenumber_orders_md",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": "We create an Order Spine containing those ORDERKEYs with multiple Lineitem's."
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "914fd177-8197-4967-9b9b-3d2f53a56ded",
   "metadata": {
    "name": "create_order_spine_for_multi_linenumber_orders",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 1398
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "|\"ORDERKEY\"  |\"LINENUMBER\"  |\n",
      "-----------------------------\n",
      "|328135      |1             |\n",
      "|328135      |2             |\n",
      "|328135      |3             |\n",
      "|328135      |4             |\n",
      "|328135      |5             |\n",
      "|328135      |6             |\n",
      "|328135      |7             |\n",
      "|827271      |1             |\n",
      "|827271      |2             |\n",
      "|827271      |3             |\n",
      "|827271      |4             |\n",
      "|827271      |5             |\n",
      "|827271      |6             |\n",
      "|1959973     |1             |\n",
      "|1959973     |2             |\n",
      "|1959973     |3             |\n",
      "|1959973     |4             |\n",
      "|1959973     |5             |\n",
      "|1959973     |6             |\n",
      "|2890464     |1             |\n",
      "|2890464     |2             |\n",
      "|2890464     |3             |\n",
      "|2890464     |4             |\n",
      "|2890464     |5             |\n",
      "|2890464     |6             |\n",
      "|3659910     |1             |\n",
      "|3659910     |2             |\n",
      "|3659910     |3             |\n",
      "|3745763     |1             |\n",
      "|3745763     |2             |\n",
      "|3745763     |3             |\n",
      "|3745763     |4             |\n",
      "|3745763     |5             |\n",
      "|4000801     |1             |\n",
      "|4000801     |2             |\n",
      "|4136992     |1             |\n",
      "|4136992     |2             |\n",
      "|4136992     |3             |\n",
      "|4136992     |4             |\n",
      "|4136992     |5             |\n",
      "|4136992     |6             |\n",
      "|4210402     |1             |\n",
      "|4210402     |2             |\n",
      "|4210402     |3             |\n",
      "|4210402     |4             |\n",
      "|4210402     |5             |\n",
      "|4556836     |1             |\n",
      "|4556836     |2             |\n",
      "|4556836     |3             |\n",
      "|4556836     |4             |\n",
      "|4556836     |5             |\n",
      "|4556836     |6             |\n",
      "|4556836     |7             |\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_spine_tbl =  [sess_db, org,'ORDERS_SPINE']\n",
    "\n",
    "lineitem_sdf.select('ORDERKEY','LINENUMBER') \\\n",
    "    .join(multi_linenumber_orders_df, on = 'ORDERKEY') \\\n",
    "    .write.saveAsTable(order_spine_tbl, mode = 'overwrite', table_type = 'temp')\n",
    "\n",
    "order_spine_df = session.table(order_spine_tbl)\n",
    "\n",
    "order_spine_df.sort('ORDERKEY','LINENUMBER').drop('LINEITEM_COUNT').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95db871-1dfe-41fe-82e8-c7bb81926eb6",
   "metadata": {
    "name": "generate_trainingdata_for_multi_linenumber_orders_md",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": [
    "We can use the Spine to generate a training dataframe from the orders and lineitem FeatureViews, and check out the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1a2913cb-e036-4891-a9b4-7273df884777",
   "metadata": {
    "name": "generate_trainingdata_for_multi_linenumber_orders",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 575
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDERKEY\"  |\"LINENUMBER\"  |\"LINEITEM_COUNT\"  |\"O_ORDERSTATUS\"  |\"O_TOTALPRICE\"  |\"O_ORDERDATE\"  |\"O_ORDERPRIORITY\"  |\"O_CLERK\"        |\"O_SHIPPRIORITY\"  |\"O_COMMENT\"                                         |\"L_QUANTITY\"  |\"L_EXTENDEDPRICE\"  |\"L_DISCOUNT\"  |\"L_TAX\"  |\"L_RETURNFLAG\"  |\"L_LINESTATUS\"  |\"L_SHIPDATE\"  |\"L_COMMITDATE\"  |\"L_RECEIPTDATE\"  |\"L_SHIPINSTRUCT\"   |\"L_SHIPMODE\"  |\"L_COMMENT\"                                |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|328135      |1             |7                 |F                |165420.86       |1992-12-27     |1-URGENT           |Clerk#000000670  |0                 |he regularly regular acc                            |3.00          |6038.85            |0.05          |0.01     |R               |F               |1993-04-25    |1993-02-26      |1993-04-30       |TAKE BACK RETURN   |FOB           |e slyly ironic excuses. bold,              |\n",
      "|328135      |2             |7                 |F                |165420.86       |1992-12-27     |1-URGENT           |Clerk#000000670  |0                 |he regularly regular acc                            |1.00          |1625.61            |0.06          |0.00     |R               |F               |1993-01-19    |1993-02-09      |1993-02-15       |COLLECT COD        |FOB           |nag slyly platelet                         |\n",
      "|328135      |3             |7                 |F                |165420.86       |1992-12-27     |1-URGENT           |Clerk#000000670  |0                 |he regularly regular acc                            |12.00         |17765.88           |0.02          |0.07     |A               |F               |1992-12-29    |1993-02-28      |1993-01-08       |TAKE BACK RETURN   |TRUCK         |unts haggle. carefully even the            |\n",
      "|328135      |4             |7                 |F                |165420.86       |1992-12-27     |1-URGENT           |Clerk#000000670  |0                 |he regularly regular acc                            |33.00         |56914.44           |0.01          |0.04     |A               |F               |1993-01-22    |1993-02-17      |1993-02-06       |COLLECT COD        |MAIL          |heodolites sleep furio                     |\n",
      "|328135      |5             |7                 |F                |165420.86       |1992-12-27     |1-URGENT           |Clerk#000000670  |0                 |he regularly regular acc                            |16.00         |25193.60           |0.10          |0.05     |A               |F               |1993-01-28    |1993-03-03      |1993-02-02       |TAKE BACK RETURN   |AIR           |es. regular accounts nag closely pe        |\n",
      "|328135      |6             |7                 |F                |165420.86       |1992-12-27     |1-URGENT           |Clerk#000000670  |0                 |he regularly regular acc                            |15.00         |14475.75           |0.05          |0.01     |A               |F               |1993-03-19    |1993-03-13      |1993-03-20       |NONE               |REG AIR       |ding deposits                              |\n",
      "|328135      |7             |7                 |F                |165420.86       |1992-12-27     |1-URGENT           |Clerk#000000670  |0                 |he regularly regular acc                            |22.00         |41931.56           |0.01          |0.04     |R               |F               |1993-01-05    |1993-02-04      |1993-01-28       |NONE               |MAIL          |dly alongside of the excus                 |\n",
      "|827271      |1             |6                 |O                |280481.18       |1998-07-12     |5-LOW              |Clerk#000000563  |0                 |ffily regular deposits. requests are quickly fi...  |42.00         |84250.32           |0.07          |0.07     |N               |O               |1998-10-18    |1998-08-14      |1998-10-21       |NONE               |AIR           |cial account                               |\n",
      "|827271      |2             |6                 |O                |280481.18       |1998-07-12     |5-LOW              |Clerk#000000563  |0                 |ffily regular deposits. requests are quickly fi...  |27.00         |36295.83           |0.06          |0.00     |N               |O               |1998-09-25    |1998-08-11      |1998-10-23       |NONE               |TRUCK         |y bold requests. bold deposits run blit    |\n",
      "|827271      |3             |6                 |O                |280481.18       |1998-07-12     |5-LOW              |Clerk#000000563  |0                 |ffily regular deposits. requests are quickly fi...  |42.00         |66009.72           |0.05          |0.08     |N               |O               |1998-10-25    |1998-09-21      |1998-11-08       |NONE               |MAIL          |ly regular ideas.                          |\n",
      "|827271      |4             |6                 |O                |280481.18       |1998-07-12     |5-LOW              |Clerk#000000563  |0                 |ffily regular deposits. requests are quickly fi...  |19.00         |35414.67           |0.03          |0.07     |N               |O               |1998-07-16    |1998-10-04      |1998-08-15       |NONE               |SHIP          | regular tithes mold pinto beans. blithe   |\n",
      "|827271      |5             |6                 |O                |280481.18       |1998-07-12     |5-LOW              |Clerk#000000563  |0                 |ffily regular deposits. requests are quickly fi...  |15.00         |19670.10           |0.04          |0.02     |N               |O               |1998-10-16    |1998-10-08      |1998-11-12       |DELIVER IN PERSON  |TRUCK         |ironic deposits dete                       |\n",
      "|827271      |6             |6                 |O                |280481.18       |1998-07-12     |5-LOW              |Clerk#000000563  |0                 |ffily regular deposits. requests are quickly fi...  |36.00         |40146.84           |0.08          |0.05     |N               |O               |1998-09-15    |1998-08-19      |1998-09-23       |TAKE BACK RETURN   |TRUCK         |ts. foxes haggle s                         |\n",
      "|1959973     |1             |6                 |F                |181071.28       |1995-01-24     |2-HIGH             |Clerk#000000625  |0                 |g accounts haggle s                                 |4.00          |5841.44            |0.00          |0.08     |A               |F               |1995-02-05    |1995-03-29      |1995-02-26       |DELIVER IN PERSON  |AIR           |lar platelets. dependencies                |\n",
      "|1959973     |2             |6                 |F                |181071.28       |1995-01-24     |2-HIGH             |Clerk#000000625  |0                 |g accounts haggle s                                 |40.00         |47527.20           |0.05          |0.03     |A               |F               |1995-03-05    |1995-02-26      |1995-04-01       |NONE               |AIR           |s. special escapades wake.                 |\n",
      "|1959973     |3             |6                 |F                |181071.28       |1995-01-24     |2-HIGH             |Clerk#000000625  |0                 |g accounts haggle s                                 |28.00         |42768.88           |0.03          |0.08     |A               |F               |1995-02-16    |1995-03-08      |1995-02-21       |TAKE BACK RETURN   |REG AIR       |cial ideas alongsid                        |\n",
      "|1959973     |4             |6                 |F                |181071.28       |1995-01-24     |2-HIGH             |Clerk#000000625  |0                 |g accounts haggle s                                 |22.00         |43141.12           |0.01          |0.08     |A               |F               |1995-04-27    |1995-02-24      |1995-05-24       |NONE               |REG AIR       |re carefull                                |\n",
      "|1959973     |5             |6                 |F                |181071.28       |1995-01-24     |2-HIGH             |Clerk#000000625  |0                 |g accounts haggle s                                 |5.00          |8813.70            |0.00          |0.01     |R               |F               |1995-02-03    |1995-03-13      |1995-02-24       |NONE               |MAIL          |azzle quickly. regular theodolites tr      |\n",
      "|1959973     |6             |6                 |F                |181071.28       |1995-01-24     |2-HIGH             |Clerk#000000625  |0                 |g accounts haggle s                                 |16.00         |29516.32           |0.10          |0.07     |R               |F               |1995-01-30    |1995-03-04      |1995-02-27       |COLLECT COD        |MAIL          |about the blithely regu                    |\n",
      "|2890464     |1             |6                 |O                |239975.48       |1997-01-06     |4-NOT SPECIFIED    |Clerk#000000635  |0                 |ts cajole. ironic deposi                            |8.00          |10947.52           |0.08          |0.07     |N               |O               |1997-01-31    |1997-03-15      |1997-02-10       |NONE               |RAIL          |. dependencies against the silent asympt   |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_training = fs.generate_training_set(\n",
    "    order_spine_df,\n",
    "    features = [orders_fv_v1, lineitem_fv_v1]\n",
    ").sort('ORDERKEY', 'LINENUMBER').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a7465-d2af-4a1f-a31f-ebc617a417f8",
   "metadata": {
    "name": "show_trainingdata_for_multi_linenumber_orders_md",
    "resultHeight": 160,
    "collapsed": false
   },
   "source": "We can see that we have multiple rows of training data per ORDERKEY that includes the order (`O_`) features repeated (denormalised) for each distinct Linenumber and row of lineitem (`L_`) features.  This might be OK for our needs as-is, or we may need to perform some post-processing on this dataset to prepare the data further for our needs.  For example, maybe we want to pivot the lineitem features to columns, or more likely create derived aggregated features from them.\n\nWe can of course create additional FeatureViews within our Feature Store, that perform and maintain this processing for us.  Lets try creating some derived features at the Order entity level from the lineitem features.  We can easily create FeatureViews derived from other FeatureViews in our Feature Store to build data pipelines.\n\nFirstly we create some new derived Lineitem level features ; Discount, Tax Due, and Return / Non-Return indicators."
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e9b21898-482a-4a2b-99cb-2b1fb150cbec",
   "metadata": {
    "name": "derived_features_trainingdata_for_multi_linenumber_orders",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 659
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDERKEY\"  |\"LINENUMBER\"  |\"L_DISCOUNT\"  |\"L_TAX_DUE\"  |\"L_RETURN_IND\"  |\"L_NONRETURN_IND\"  |\"L_QUANTITY\"  |\"L_EXTENDEDPRICE\"  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|2400001     |1             |400.8900      |267.2600     |0               |1                  |10.00         |13363.00           |\n",
      "|2400001     |2             |805.0056      |603.7542     |1               |0                  |14.00         |20125.14           |\n",
      "|2400001     |3             |235.3014      |1647.1098    |0               |1                  |18.00         |23530.14           |\n",
      "|2400001     |4             |301.9770      |268.4240     |0               |1                  |2.00          |3355.30            |\n",
      "|2400001     |5             |1384.5923     |593.3967     |0               |1                  |13.00         |19779.89           |\n",
      "|2400002     |1             |2358.4428     |262.0492     |1               |0                  |14.00         |26204.92           |\n",
      "|2400002     |2             |0.0000        |58.9000      |1               |0                  |1.00          |1472.50            |\n",
      "|2400002     |3             |411.3711      |548.4948     |1               |0                  |7.00          |13712.37           |\n",
      "|2400002     |4             |1431.2880     |1113.2240    |0               |1                  |8.00          |15903.20           |\n",
      "|2400002     |5             |0.0000        |1259.4528    |1               |0                  |43.00         |62972.64           |\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDERKEY\"  |\"LA_ORDER_TOTAL_DISCOUNT_COST\"  |\"LA_TOTAL_TAX_DUE\"  |\"LA_ORDER_AVG_DISCOUNT_COST\"  |\"LA_AVG_TAX_DUE\"  |\"LA_SUM_RETURN\"  |\"LA_SUM_NONRETURN\"  |\"LA_RETURN_RATIO\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|4800003     |15268.1324                      |6582.5528           |3053.6264800000               |1316.5105600000   |4                |1                   |0.250000           |\n",
      "|4800066     |0.0000                          |2737.9664           |0E-10                         |2737.9664000000   |1                |0                   |0.000000           |\n",
      "|4800071     |3374.4593                       |7303.9123           |562.4098833333                |1217.3187166667   |1                |5                   |5.000000           |\n",
      "|4800160     |6763.2139                       |7413.9072           |1127.2023166667               |1235.6512000000   |6                |0                   |0.000000           |\n",
      "|4800161     |5569.2425                       |6412.8683           |1113.8485000000               |1282.5736600000   |5                |0                   |0.000000           |\n",
      "|4800194     |3180.0072                       |4169.6680           |1060.0024000000               |1389.8893333333   |3                |0                   |0.000000           |\n",
      "|4800098     |22.1016                         |33.1524             |22.1016000000                 |33.1524000000     |1                |0                   |0.000000           |\n",
      "|4800135     |4823.3327                       |122.6250            |2411.6663500000               |61.3125000000     |2                |0                   |0.000000           |\n",
      "|4800230     |7134.1196                       |6028.0569           |1426.8239200000               |1205.6113800000   |1                |4                   |4.000000           |\n",
      "|4800261     |6155.2652                       |5097.2939           |1538.8163000000               |1274.3234750000   |4                |0                   |0.000000           |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": "lineitem_derived_features = lineitem_fv_v1.feature_df.with_columns(['L_DISCOUNT', 'L_TAX_DUE', 'L_RETURN_IND', 'L_NONRETURN_IND'],\n                                                                   [F.col('L_EXTENDEDPRICE') * F.col('L_DISCOUNT'),  \n                                                                    F.col('L_EXTENDEDPRICE') * F.col('L_TAX'),\n                                                                    (F.when(F.col('L_RETURNFLAG') == 'R', F.lit(1)).otherwise(F.lit(0))),\n                                                                    (F.when(F.col('L_RETURNFLAG') == 'R', F.lit(0)).otherwise(F.lit(1)))\n                                                                    ]) \\\n    .select([ 'ORDERKEY', 'LINENUMBER', 'L_DISCOUNT', 'L_TAX_DUE', 'L_RETURN_IND', 'L_NONRETURN_IND', 'L_QUANTITY', 'L_EXTENDEDPRICE'])\n\nlineitem_derived_features.show()"
  },
  {
   "cell_type": "markdown",
   "id": "5805537e-3be6-430c-8931-6fea4a3fcd31",
   "metadata": {
    "name": "featureview_derived_features_trainingdata_for_multi_linenumber_orders_md",
    "collapsed": false
   },
   "source": "And we create a new FeatureView for these derived lineitem features."
  },
  {
   "cell_type": "code",
   "id": "c4fe9f9b-71c6-45fb-815d-bc3c844f93bb",
   "metadata": {
    "language": "python",
    "name": "featureview_derived_features_trainingdata_for_multi_linenumber_orders",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# LINEITEM DERIVED FEATUREVIEW\n\n# Create Lineitem Derived FeatureView in Feature Store\nlineitem_derived_fv = FeatureView(\n    name = f\"FV_LINEITEM_DERIVED\",\n    entities = [order_entity, lineitem_entity],\n    feature_df = lineitem_derived_features,\n    desc = f\"Order/Lineitem level, Derived features\"\n)\n# Register the Order Lineitem aggregated FeatureView in the schema\nlineitem_derived_fv_v1 = fs.register_feature_view(\n    feature_view = lineitem_derived_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ab200d6-03d1-41da-bf32-072e2f6b3920",
   "metadata": {
    "name": "order_level_featureview_derived_features_trainingdata_for_multi_linenumber_orders_md",
    "collapsed": false
   },
   "source": "We will now aggregate these Lineitem level features up to the Order level and use it to create a new FeatureView derived from the Lineitem FeatureView.  See how `lineitem_derived_fv_v1.feature_df`, is used as the source dataframe for the Order level aggregated features."
  },
  {
   "cell_type": "code",
   "id": "7f6125b9-6b78-4826-badf-ceda4486d8fb",
   "metadata": {
    "language": "python",
    "name": "order_level_derived_features_trainingdata_for_multi_linenumber_orders",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "order_lineitem_aggregated_features = lineitem_derived_fv_v1.feature_df.group_by('ORDERKEY') \\\n    .agg(F.sum(\"L_DISCOUNT\").alias(\"OLA_ORDER_TOTAL_DISCOUNT_COST\"),\n        F.sum(\"L_TAX_DUE\").alias(\"OLA_TOTAL_TAX_DUE\"),\n        F.avg(\"L_DISCOUNT\").alias(\"OLA_ORDER_AVG_DISCOUNT_COST\"),\n        F.avg(\"L_TAX_DUE\").alias(\"OLA_AVG_TAX_DUE\"),\n        F.count(\"*\").alias(\"OLA_LINEITEM_COUNT\"),         \n        F.sum(\"L_NONRETURN_IND\").alias(\"OLA_SUM_RETURN\"),\n        F.sum(\"L_RETURN_IND\").alias(\"OLA_SUM_NONRETURN\")\n        ) \\\n    .with_column(\"OLA_RETURN_RATIO\", F.div0(F.col(\"OLA_SUM_RETURN\"), F.col(\"OLA_SUM_NONRETURN\")))\n\norder_lineitem_aggregated_features.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b4e5a35-4164-4b52-8e36-b72ae59508e5",
   "metadata": {
    "name": "featureview_order_level_featureview_derived_features_trainingdata_for_multi_linenumber_orders_md",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": "We can use the Order Lineitem aggregated features dataframe to create our new Order level FeatureView."
  },
  {
   "cell_type": "code",
   "id": "607f50b7-21ff-489f-a6ff-d56227e59847",
   "metadata": {
    "language": "python",
    "name": "order_level_featureview_derived_features_trainingdata_for_multi_linenumber_orders",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# ORDER_AGGREGATED_LINEITEM FEATUREVIEW\n",
    "\n",
    "# Create Order Lineitem aggregated FeatureView in Feature Store\n",
    "order_lineitem_aggregated_fv = FeatureView(\n",
    "    name = f\"FV_ORDER_LINEITEM_AGG\",\n",
    "    entities = [order_entity],\n",
    "    feature_df = order_lineitem_aggregated_features,\n",
    "    desc = f\"Order level, Lineitem aggregated features\"\n",
    ")\n",
    "# Register the Order Lineitem aggregated FeatureView in the schema\n",
    "order_lineitem_aggregated_fv_v1 = fs.register_feature_view(\n",
    "    feature_view = order_lineitem_aggregated_fv,    # feature view created above, could also use external_fv\n",
    "    version = \"V1\",\n",
    "    block = True,               # whether function call blocks until initial data is available\n",
    "    overwrite = True,           # whether to replace existing feature view with same name/version\n",
    ")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7206d1ad-5e82-4da5-9288-96e8a710c082",
   "metadata": {
    "name": "cell47",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": [
    "We create new Orders Spine containing only the distinct ORDERKEYs, and then combine the Orders and Lineitem Aggregated features to create a training dataframe result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0f5a9c3c-44d2-43a6-8bc4-1e9be10a9c35",
   "metadata": {
    "name": "cell48",
    "language": "python",
    "codeCollapsed": false,
    "resultHeight": 351
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDERKEY\"  |\"O_ORDERSTATUS\"  |\"O_TOTALPRICE\"  |\"O_ORDERDATE\"  |\"O_ORDERPRIORITY\"  |\"O_CLERK\"        |\"O_SHIPPRIORITY\"  |\"O_COMMENT\"                                         |\"LA_ORDER_TOTAL_DISCOUNT_COST\"  |\"LA_TOTAL_TAX_DUE\"  |\"LA_ORDER_AVG_DISCOUNT_COST\"  |\"LA_AVG_TAX_DUE\"  |\"LA_SUM_RETURN\"  |\"LA_SUM_NONRETURN\"  |\"LA_RETURN_RATIO\"  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|328135      |F                |165420.86       |1992-12-27     |1-URGENT           |Clerk#000000670  |0                 |he regularly regular acc                            |4986.4042                       |6662.2776           |712.3434571429                |951.7539428571    |4                |3                   |0.750000           |\n",
      "|827271      |O                |280481.18       |1998-07-12     |5-LOW              |Clerk#000000563  |0                 |ffily regular deposits. requests are quickly fi...  |16436.7495                      |16058.0709          |2739.4582500000               |2676.3451500000   |6                |0                   |0.000000           |\n",
      "|1959973     |F                |181071.28       |1995-01-24     |2-HIGH             |Clerk#000000625  |0                 |g accounts haggle s                                 |7042.4696                       |10920.2106          |1173.7449333333               |1820.0351000000   |4                |2                   |0.500000           |\n",
      "|2890464     |O                |239975.48       |1997-01-06     |4-NOT SPECIFIED    |Clerk#000000635  |0                 |ts cajole. ironic deposi                            |18841.8988                      |12539.6380          |3140.3164666667               |2089.9396666667   |6                |0                   |0.000000           |\n",
      "|3659910     |F                |83944.45        |1994-09-01     |2-HIGH             |Clerk#000000645  |0                 |express excuses haggl                               |7429.6754                       |3317.7005           |2476.5584666667               |1105.9001666667   |1                |2                   |2.000000           |\n",
      "|3745763     |F                |137171.20       |1992-12-17     |1-URGENT           |Clerk#000000133  |0                 | believe. quickly ironic account                    |9778.5844                       |4795.8692           |1955.7168800000               |959.1738400000    |1                |4                   |4.000000           |\n",
      "|4000801     |F                |53894.71        |1992-03-26     |4-NOT SPECIFIED    |Clerk#000000455  |0                 |n ideas. slyly pending requests acro                |2911.6560                       |2066.3400           |1455.8280000000               |1033.1700000000   |1                |1                   |1.000000           |\n",
      "|4136992     |O                |240766.31       |1995-12-31     |3-MEDIUM           |Clerk#000000618  |0                 | regular deposits. stealthily                       |9250.0044                       |6361.3898           |1541.6674000000               |1060.2316333333   |6                |0                   |0.000000           |\n",
      "|4210402     |F                |177513.79       |1994-12-18     |5-LOW              |Clerk#000000381  |0                 |s wake slyly alongside o                            |9877.3167                       |8437.1148           |1975.4633400000               |1687.4229600000   |1                |4                   |4.000000           |\n",
      "|4556836     |O                |191162.89       |1996-08-30     |3-MEDIUM           |Clerk#000000009  |0                 |the furiously final pack                            |9942.9585                       |10404.8697          |1420.4226428571               |1486.4099571429   |7                |0                   |0.000000           |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_multiline_spine_tbl =  [sess_db, org,'ORDERS_MULTILINE_SPINE']\n",
    "\n",
    "order_spine_df.select('ORDERKEY').distinct().sample(n= num_spine_rows) \\\n",
    "    .write.saveAsTable(order_multiline_spine_tbl, mode = 'overwrite', table_type = 'temp')\n",
    "\n",
    "order_multiline_spine_df = session.table(order_multiline_spine_tbl)\n",
    "\n",
    "order_training = fs.generate_training_set(\n",
    "    order_multiline_spine_df,\n",
    "    features = [orders_fv_v1\n",
    "                , order_lineitem_aggregated_fv_v1\n",
    "                ]\n",
    ").sort('ORDERKEY').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec62b53-271d-4903-a8e7-72f0b7514364",
   "metadata": {
    "name": "cell49",
    "resultHeight": 42,
    "collapsed": false
   },
   "source": "We have produced a training dataframe containing a row per ORDERKEY value with the Order (`O_`) and Order Lineitem Aggregated (`OLA_`) features.\n\n### Summary \nSo far we have created :\n- base level Feature Views from source tables with 'raw' features (columns) as they appear in the source data.\n- Feature Views containing additional row-level derived and computed features like `L_DISCOUNT`, `L_TAX_DUE` and boolean indicators like `L_RETURN_IND`, `L_NONRETURN_IND`.\n- Feature Views using aggregated features, aggregating from an entity-key lower in the entity-hierarchy (e.g. `LINEITEM`) upto a higher level (e.g `ORDER`).\n\nWe have shown how we can join these Feature Views together to retrieve data for training, combining features from different Entities, and ensuring we get valid outputs.\n\nNext we will show additional feature preprocessing to prepare data into a more consumable model-ready state.  "
  },
  {
   "cell_type": "markdown",
   "id": "ec9baeb3-f215-4e92-be45-e59ec4d2b91c",
   "metadata": {
    "name": "DERIVED_FEATURES_PREPROCESSING",
    "collapsed": false
   },
   "source": "# Derived Features - PreProcessing\n\nA common question (or assumption) we here is _“Where should the feature pre-processing required for a model reside?”_  or _“Can I store pre-processed features in the Feature Store?”_\n\n__Feature pre-processing : a definition__\nIt is an important step, maybe the most important, in preparing raw data for machine-learning and is the process of treating and standardising features such that they can be directly used within an ML model and are better suited to the modelling problem.   For example,  many ML models make assumptions about the form of their input features, or may not perform well with untreated features.\n\n\n### Model-specific data processing (transformations) - FeatureStore or Model-Pipeline\n_How should we think about this, and where should model specific feature processing reside?_   \nIn general, model-specific processing transformations should be done as part of the model training pipeline, and not as a general transform applied to all data within the Feature Store using a FeatureView.   This is likely not so much an issue for encoders (e.g categorical), assuming the categories are very stable, or fixed, in the data, but can be an issue for scaling type processors that capture volatile state from the sample e.g. min/max scaler.  The scaler fit needs to be applied to each distinct sample of data at model training/testing/validation time to avoid data leakage. Hence it will differ for each new sample of data used in training, testing or cross-validation.   If this was done within feature-views, specific to each sample of data used for a model execution (training, testing, inference), you could end up with potentially 100's of feature-views, 1 per model-version.  Hence it is generally more appropriate for those state values to be captured by fitting to be saved within the model-pipeline, which can then be stored within the Snowflake model-registry, providing end-to-end lineage from ; raw source data -> model-generic feature-views -> model-specific preprocessing pipeline -> model.  Nevertheless, we will show an example of how these transformations can be performed within the FeatureStore should it be required."
  },
  {
   "cell_type": "markdown",
   "id": "7f15563c-97d9-4fc5-8759-4c3d68473128",
   "metadata": {
    "name": "CATEGORICAL_PREPROCESSING",
    "collapsed": false
   },
   "source": "\n### Encoding Categorical Data\nIt is common for categorical data to be encoded into numeric values for use within a model.  There are various common approaches for this:\n- Ordinal Encoding - Each distinct value is assigned an integer value used to represent the categorical value. Simplist feature encoding scheme.\n- Label Encoding - As for Ordinal Encoding, but no inherent ordering in values and their index.  Typically used for encoding target variables rather than features.  \n- One-Hot Encoding - Each distinct categorical value is represented as new feature of boolean or 0/1 numeric type. This technique can result in significant feature expansion where a large number of distinct values exist in the unencoded categorical feature and there are various techniques used to avoid this. e.g. only applying encoding to features with distinct values below a set level, or bucketing distinct values with low cardinality in a feature into a new single category.\n\nThere are many possible approaches that can be used. Here's a couple: \n- use Snowpark (or SQL) to generate dataframe code that represents the transformation directly, which is used as the input dataframe for a FeatureView.\n- use the preprocessing functions available in scikit-learn or the scalable equivalents in snowflake.ml.modelling. Fit a preprocessing model/pipeline with the data from FeatureViews.  Save the model into model-registry.  Reuse the model/pipeline function within a dataframe that is then used as the input to derive a FeatureView.\n\nWe will show the first approach showing how a One-Hot Encoding schema can be achieved using Snowpark Dataframe code alone.  For this, we capture the distinct values in categorical columns, derive new columns (via case expressions) using these values to derive the one-hot-encoded columns.  For simplicity we have assumed that string based columns are categorical.  We have also assigned a limit of 100 as the maximum number of distinct values that we will accept in a categorical column for one-hot encoding before an alternative treatment is necessary. This prevents the column (feature) explosion issue mentioned earlier.\n\nThe code below :\n- identifies string (categorical) feature in our base lineitem featureview dataframe.\n- gets the distinct value count for each string feature\n- filters out string features with a higher distinct value count than our threshold to derive the categorical feature list\n- gets the distinct values for each categorical feature\n- creates a select clause consisting of a new column for each categorical feature/distinct value combination plus an additional column to represent unknown values"
  },
  {
   "cell_type": "markdown",
   "id": "68681e55-8503-41f0-b699-afea156cb061",
   "metadata": {
    "name": "cell153",
    "collapsed": false
   },
   "source": "#### One Hot Encode"
  },
  {
   "cell_type": "code",
   "id": "e48dc515-0ebd-42f0-aad7-d969c21de9f2",
   "metadata": {
    "language": "python",
    "name": "cell61",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Use the schema to retrieve only the String columns we want to one-hot-encode\nli_fv_v1_schema = lineitem_fv_v1.feature_df.schema\nstring_fields = [field for field in li_fv_v1_schema.fields if isinstance(field.datatype, T.StringType)]\nprint(string_fields)\nprint(' ')\n\n# Get the distinct value count for each field as a Dict.\nfield_dv_counts = lineitem_fv_v1.feature_df.select([F.count_distinct(F.col(field.name)).alias(field.name)  for field in string_fields]) \\\n    .select(F.object_construct('*').alias('FIELD_DV_COUNTS')) \\\n    .collect()[0][0]\nfield_dv_counts_dict = json.loads(field_dv_counts)\nprint('Field Distinct Value Counts: ',field_dv_counts_dict)\nprint(' ')\n\n# Filter the Fields based on a threshold that determines whether we want to treat as Categorical, or leave untreated as String field\ncat_fields_threshold = 100\ncat_fields = [key for key, value in field_dv_counts_dict.items() if value <= cat_fields_threshold]\nprint('Categorical Fields: ', cat_fields)\nprint(' ')\n\n# Get a table of distinct values for each categorical column\ndv_fields = lineitem_fv_v1.feature_df.select([F.array_agg(F.replace(F.col(field),\" \", \"\") , is_distinct = True).alias(field)  for field in cat_fields])\ndv_fields.show()\nprint(' ')\n\n# and create a Dict of sorted categorical values for each Field from the table\ndv_fields_dict = json.loads(dv_fields.select(F.object_construct('*').alias('string_dv_dict')).collect()[0]['STRING_DV_DICT'])\ndv_fields_dict = {key: sorted(value) for key, value in dv_fields_dict.items()}\nprint(dv_fields_dict)\nprint(' ')\n\n# Create the select case expressions for the new columns \nselect_onehenc_expr = ['ORDERKEY','LINENUMBER']\n# For each Categorical Field\nfor field in dv_fields_dict:\n    # For each distinct value\n    for idx , v in enumerate(dv_fields_dict[field]):\n        # Create a case expression to test for value in Field\n        select_onehenc_expr.append(F.when(F.col(field) == v, F.lit(1)).otherwise(F.lit(0)).alias(f'{field}_{v}'))\n    # And create an additional Case Expression to handle and UNKNOWN values that appear in the table at a later date that we have not already handled    \n    select_onehenc_expr.append(F.when(F.col(field).in_(dv_fields_dict[field]), F.lit(0)).otherwise(F.lit(1)).alias(f'{field}_UNKNOWN'))    \nprint('Snowpark Case Expressions: ', select_onehenc_expr)\nprint(' ')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f914f6c3-b821-411d-a6e5-a9c128213f9e",
   "metadata": {
    "name": "cell70",
    "collapsed": false
   },
   "source": "Now that we have built the Select case-expression column list we can use it to process the raw categorical-features and derive our one-hot encoded feature columns."
  },
  {
   "cell_type": "code",
   "id": "ca36b95d-852b-4a95-b528-6841650ff656",
   "metadata": {
    "language": "python",
    "name": "cell66",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "lineitem_categorical_onehotenc_features = lineitem_fv_v1.feature_df.select(select_onehenc_expr)\nlineitem_categorical_onehotenc_features.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "210d25b0-29a4-4ca5-a80e-e9a6b4305018",
   "metadata": {
    "name": "cell71",
    "collapsed": false
   },
   "source": "We can now use the dataframe to create a new FeatureView with the One Hot Encoded features.  Our new dataframe is derived from the base `lineitem_fv_v1` FeatureView.  We could of equally chosen to derive it directly from the underlying source table dataframe (`lineitem_sdf`), but using the base-level FeatureView might give us additional useful lineage information."
  },
  {
   "cell_type": "code",
   "id": "c350b410-2064-4fb2-80c4-b183862d150b",
   "metadata": {
    "language": "python",
    "name": "cell64",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# LINEITEM CATEGORICAL ONE-HOT_ENCODED FEATURES FEATUREVIEW\n\n# Create Lineitem one-hot encoded FeatureView in Feature Store\nlineitem_onehotenc_fv = FeatureView(\n    name = f\"FV_LINEITEM_CAT_1HENC\",\n    entities = [order_entity, lineitem_entity],\n    feature_df = lineitem_categorical_onehotenc_features,\n    desc = f\"Order / Lineitem level - Lineitem categorical features treated with one-hot encoding scheme\"\n)\n# Register the Lineitem one-hot encoded FeatureView in the schema\nlineitem_onehotenc_fv_v1 = fs.register_feature_view(\n    feature_view = lineitem_onehotenc_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,                            # whether function call blocks until initial data is available\n    overwrite = True,                        # whether to replace existing feature view with same name/version\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d199fb1-b137-4f51-b462-17aecafe06b0",
   "metadata": {
    "language": "python",
    "name": "cell72",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "print(lineitem_onehotenc_fv_v1.feature_df.columns)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b32d3e93-2e1d-455b-af10-b5d28e3c1a55",
   "metadata": {
    "name": "cell78",
    "collapsed": false
   },
   "source": "We will use the Spine we created earlier which includes all the entity keys `lineitem_order_part_supplier_customer_nation_region_spine_df` and use it to generate a training dataset containing the raw source, derived and categorical features fpr our lineitem FeatureViews."
  },
  {
   "cell_type": "code",
   "id": "6f627721-4ebb-4158-bdc6-ce0e7f2f0196",
   "metadata": {
    "language": "python",
    "name": "cell76"
   },
   "outputs": [],
   "source": "lineitem_order_part_supplier_customer_nation_region_spine_df.sort(keycols).show(10)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f9d7826-21b4-4e20-848a-4629a8f71b67",
   "metadata": {
    "language": "python",
    "name": "cell143",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "lineitem_derived_and_categorical_df_training = fs.generate_training_set(\n    lineitem_order_part_supplier_customer_nation_region_spine_df,\n    features = [lineitem_fv_v1, lineitem_derived_fv_v1, lineitem_onehotenc_fv_v1]\n)\nlineitem_derived_and_categorical_df_training.sort(keycols).show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdcf79d2-f5d1-439d-9276-3da57c7104b5",
   "metadata": {
    "language": "python",
    "name": "cell77",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "lineitem_derived_and_categorical_df_training.columns",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9438e23b-53be-4d69-9a64-390b2da06951",
   "metadata": {
    "name": "ORDINAL_ENCODE",
    "collapsed": false
   },
   "source": "#### Ordinal Encode"
  },
  {
   "cell_type": "code",
   "id": "55f691c8-3474-43bb-822b-76b2a81604c0",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": "def ordinal_to_snowpark_expr(encoded_fit):\n    case_expressions = []\n    # Loop over each column we want to encode\n    for input_col, output_col in zip(encoded_fit.input_cols, encoded_fit.output_cols):\n        # Fetch categories and ensure conversion to list\n        categories_ndarray = encoded_fit.categories_.get(input_col, [])\n        categories = categories_ndarray.tolist() if isinstance(categories_ndarray, np.ndarray) else categories_ndarray\n        if not categories:\n            raise ValueError(f\"No categories found for column: {input_col}\")\n        \n        # Start a fresh case expression for this column\n        case_expr = None\n        # Build the ordinal when chain for this column\n        for idx, category in enumerate(categories):\n            if case_expr is None:\n                case_expr = F.when(F.col(input_col) == F.lit(category), F.lit(idx))\n            else:\n                case_expr = case_expr.when(F.col(input_col) == F.lit(category), F.lit(idx))\n\n        # Handle unknown categories (assign -1 if not found in the categories list)\n        if case_expr is not None:\n            case_expr = case_expr.otherwise(F.lit(-1))\n            case_expressions.append(case_expr.alias(output_col))\n        else:\n            raise ValueError(f\"Failed to construct case expression for column: {input_col}\")\n\n    return case_expressions\n\ndef ss_snowpark_expression_builder(ss_pdf, array_colnames):\n    ss_snowpark_exp = [] \n    for r in ss_pdf:\n        for a in array_colnames:    \n            ss = pickle.loads(codecs.decode(r['SS_MODEL_PKL'].encode(), \"base64\"))\n            res_row.append({'PARTITION_COL':r['PARTITION_COL'], 'SCALES' : ss.scale_.tolist(), 'MEAN' : ss.mean_.tolist(), 'VARIANCE' : ss.var_.tolist(), 'NROWS_IN_FIT_GROUP' : int(ss.n_samples_seen_), 'N_FEATURES_IN_ARRAY' : ss.n_features_in_, 'PKL_SS': r['SS_MODEL_PKL']})\n    return res_row     ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0a88929a-4d75-44c5-86de-37c3cb974ca2",
   "metadata": {
    "language": "python",
    "name": "cell149",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Lineitem FeatureView V1 : Categorical Coluns\nlineitem_fv_v1_sdf = (lineitem_fv_v1.feature_df\n    .select(F.col(\"ORDERKEY\"),\n            F.col(\"LINENUMBER\"),\n            F.col(\"L_RETURNFLAG\"),\n            F.col(\"L_LINESTATUS\"), \n            F.col(\"L_SHIPINSTRUCT\"), \n            F.col(\"L_SHIPMODE\")\n           )\n    )\n# Input and Output Columns\nli_input_cols = [\"L_RETURNFLAG\", \"L_LINESTATUS\", \"L_SHIPINSTRUCT\",\"L_SHIPMODE\"]\nli_output_cols = [f\"ORDINAL_{col}\" for col in li_input_cols]\n\n# Fit Ordinal Encoder Model\nli_encoder = OrdinalEncoder(\n    input_cols=li_input_cols,\n    output_cols=li_output_cols\n).fit(lineitem_fv_v1_sdf)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3bb4072e-0117-43e1-a4d2-1335b43b9e61",
   "metadata": {
    "language": "python",
    "name": "cell147",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Create Dataframe using Ordinal Encoded model, transformed to CASE expressions\nlineitem_ordinalEncodedCats_df = (lineitem_fv_v1.feature_df\n    .select(F.col(\"ORDERKEY\"),\n            F.col(\"LINENUMBER\"),\n            F.col(\"L_RETURNFLAG\"),\n            F.col(\"L_LINESTATUS\"), \n            F.col(\"L_SHIPINSTRUCT\"), \n            F.col(\"L_SHIPMODE\"))\n    .with_columns(li_encoder.output_cols, ordinal_to_snowpark_expr( li_encoder)))\n\n# Create Lineitem Ordinal Encoded FeatureView in Feature Store\nlineitem_ordinalEncodedCats_fv = FeatureView(\n    name = f\"FV_LINEITEM_ORDINALENCODEDCATS\",\n    entities = [order_entity, lineitem_entity],\n    feature_df = lineitem_ordinalEncodedCats_df,\n    desc = f\"Lineitem categorical columns original and ordinal encoded feature view\"\n)\n\n# Register the FeatureView in the schema\nlineitem_ordinalEncodedCats_fv_v1 = fs.register_feature_view(\n    feature_view = lineitem_ordinalEncodedCats_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)\n\nlineitem_ordinalEncodedCats_fv_v1.feature_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b615bc9-e8d4-4876-a19a-cbb7f0ee7787",
   "metadata": {
    "name": "NUMERIC_PREPROCESSING",
    "collapsed": false
   },
   "source": "### Numeric Scaling Example\n\nNumeric scaling is more typically applied as model-specific transformations within the Model pipeline.  Once fitted these can be stored within the Snowflake Model-Registry and used on new data when needed. Numeric Scaling techniques store global-state related to the data-set used to FIT them. e.g. min and max values for a feature.  \n\n> **Note:**\n> Snowflake Model-Registry supports transform only pipelines. You no longer need an ML model as the final step of the pipeline. If you have a common set of transformations you want to apply consistently, you can create a transformation only pipeline, fit and store it as a model in Model-Registry, and use it to transform whenever needed. \n\nThat said, it may also be useful to maintain and persist a version of these features computed over the global state, rather than training sample. e.g. for exploratory analysis, and to support 'quick/approximate' experimentation.  Below we show an approach to deriving and maintaining these within the Feature Store via a downstream _statistical summary_ FeatureView.  As the upstream FeatureView changes the downstream _statistical summary_ FeatureView global-state values will change to reflect those. We will show an example of this approach below.\n\nLike before for the Categorical Encoding example we will use the `LINEITEM` table to demonstrate. We first need to identify the candidate numeric columns that we want to apply the numeric scaling on."
  },
  {
   "cell_type": "code",
   "id": "2cbc644d-a07a-44aa-ba9e-9cfd5869ee88",
   "metadata": {
    "language": "python",
    "name": "cell62",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Use the schema to retrieve only the Numeric columns we want to scale\nli_fv_v1_schema = lineitem_fv_v1.feature_df.schema\nnumeric_types = (T.DecimalType, T.DoubleType, T.FloatType, T.IntegerType) # Numeric types we want to scale\nnumeric_fields = [field.name for field in li_fv_v1_schema.fields if isinstance(field.datatype,  numeric_types )]\nprint(numeric_fields)\nprint(' ')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81acaf40-27e3-45ba-83e4-25378b160aeb",
   "metadata": {
    "name": "cell81",
    "collapsed": false
   },
   "source": "Now we need to compute global summary level statistics for the `LINEITEM` FeatureView.  There are several ways we can do this, for example we can use the`describe()` helper function that returns the 5 number statistical summary for the columns.  We can add other summary statistics to this where needed for other scaling techniques. For example, the max-absolute value for use in max absolute scaling.\n\nWe will convert the output into a format that will allow us to use it dynamically to derive the numeric-scaled features for every row in our data.  We package the statistical values up into a Snowflake object keyed by feature-name, with an object containing each of the statistics.  "
  },
  {
   "cell_type": "code",
   "id": "85ec6005-5979-4a3b-ab67-ad9170f92454",
   "metadata": {
    "language": "python",
    "name": "cell65",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Compute the COUNT, MIN, MAX, MEAN & STDEV for each Numeric field\nlineitem_numericstatsummary_features = lineitem_fv_v1.feature_df.describe([field for field in numeric_fields ])\n# Add in the max-abs statistic for Max Absolute Scaling\nlineitem_numericstatsummary_features = lineitem_numericstatsummary_features.union(lineitem_fv_v1.feature_df.select([F.lit('maxabs')]+[F.max(F.abs(F.col(field))).alias(field) for field in numeric_fields ]))\nlineitem_numericstatsummary_features.show()\n\n# Convert the statistical summary from describe into an object of objects (dict of dicts)\nlineitem_numericstatsummary_features_dict = (  lineitem_numericstatsummary_features \n    .select([F.object_agg(F.upper(F.col(\"SUMMARY\")), field).alias(field)  for field in numeric_fields]) \n    .select(F.lit(f'{lineitem_fv_v1.name}${lineitem_fv_v1.version}').alias('FEATUREVIEW$VERSION'), F.object_construct('*').alias('NUMCOL_STAT_DICT')))\n\nlineitem_numericstatsummary_features_dict.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f82a99e-e33c-411d-9ada-25614bceca92",
   "metadata": {
    "name": "cell84",
    "collapsed": false
   },
   "source": "We could use this dataframe directly to join to our `LINEITEM` FeatureView data and derive the scaled features.  Another option is to create a new FeatureView using this Dataframe within our Feature Store. This will ensure the data is generally available and visible within the Feature Store and that the state values are always up to date as changes occur in the `LINEITEM` FeatureView. We have added an additional column to the data containing the FeatureView name and Version, to ensure we can identify what source FeatureView this relates to. That may be useful if we want to UNION all of these statistical summary FeatureViews together for exploratory analysis. We will also use this `FEATUREVIEW$VERSION` as the ENTITY key for the FeatureView."
  },
  {
   "cell_type": "markdown",
   "id": "97d80f58-da8c-4f43-9f53-9d47d5093e7f",
   "metadata": {
    "name": "cell73",
    "collapsed": false
   },
   "source": "##### METADATA : FEATUREVIEW ENTITY\nWe define the `FEATUREVIEW$VERSION` entity"
  },
  {
   "cell_type": "code",
   "id": "64653bde-6f1e-4c41-a446-2bb3069ac72b",
   "metadata": {
    "language": "python",
    "name": "cell67",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Metadata FEATUREVIEW Entity Definition \nfeatureview_version_entity = Entity(\n    name=\"_METADATA_FEATUREVIEW$VERSION\",\n    join_keys=[\"FEATUREVIEW$VERSION\"],\n    desc=\"Metadata Entity to represent versioned FeatureViews within the FeatureStore\"\n    )\nfs.register_entity(featureview_version_entity)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b24b9857-9e93-47f3-ac2e-767a2dde7b42",
   "metadata": {
    "name": "cell79",
    "collapsed": false
   },
   "source": "##### METADATA : LINEITEM NUMERIC STATISTICS\nWe use the statistical summary dataframe we created above as the source dataframe to create our FeatureView.  We will create this as a View based FeatureView to avoid the cost of recomputing it every time the upstream (`FV_LINEITEM$V1`) changes.  "
  },
  {
   "cell_type": "code",
   "id": "a5a96dc4-c23f-4ea9-9e15-b38764e07bb5",
   "metadata": {
    "language": "python",
    "name": "cell68"
   },
   "outputs": [],
   "source": "# Metadata  LINEITEM NUMERIC AGGREGATE STATISTICS FEATURES FEATUREVIEW\n# Create Lineitem numeric statistical summary FeatureView in Feature Store\nlineitem_numericstatsummary_fv = FeatureView(\n    name = f\"_METADATA_FV_LINEITEM_NUMERICSTATS\",\n    entities = [featureview_version_entity],\n    feature_df = lineitem_numericstatsummary_features_dict,\n    desc = f\"Metadata FeatureView : Order / Lineitem level - Lineitem Statistical Summary for Numeric columns\"\n)\n# Register the Lineitem numeric statistical summary FeatureView in the schema\nlineitem_numericstatsummary_fv_v1 = fs.register_feature_view(\n    feature_view = lineitem_numericstatsummary_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,                            # whether function call blocks until initial data is available\n    overwrite = True,                        # whether to replace existing feature view with same name/version\n)\n\nlineitem_numericstatsummary_fv_v1.feature_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "524c14d3-d922-47c7-b7ac-d9b31ddb7c9a",
   "metadata": {
    "name": "cell87",
    "collapsed": false
   },
   "source": "We could build the expressions (formula) to scale the numerics directly within our dataframe, but given we might use these expressions a lot, we will create some new Snowflake SQL functions that encapsulate this logic.  These are effectively inline inserted into the SQL whenever they are called, so very efficient. We create a SQL function for each of our scaling functions; standard scaling, min max scaling and max absolute scaling.  Note: we have registered these as IMMUTABLE and MEMOIZABLE that should also optimise their efficiency."
  },
  {
   "cell_type": "code",
   "id": "39e6ef6e-575a-4e77-8fff-219c20bc82c1",
   "metadata": {
    "language": "sql",
    "name": "cell82",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION {{sess_db}}.{{org}}.min_max_scale(feature FLOAT, X_min float, X_max float)\n  RETURNS FLOAT\n  IMMUTABLE MEMOIZABLE\n  AS\n  $$\n    div0((feature - X_min) , (X_max - X_min))\n  $$\n  ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cefb0c5a-f6c2-409d-ad4f-715ffbce1e72",
   "metadata": {
    "language": "sql",
    "name": "cell86",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION {{sess_db}}.{{org}}.standard_scale(feature FLOAT, X_mean float, X_stdev float)\n  RETURNS FLOAT\n  IMMUTABLE MEMOIZABLE  \n  AS\n  $$\n    div0((feature - X_mean) , X_stdev)\n  $$\n  ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4181be52-a9b7-4474-95b5-4804f22cd329",
   "metadata": {
    "language": "sql",
    "name": "cell85",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION {{sess_db}}.{{org}}.maxabs_scale(feature FLOAT, X_maxabs float)\n  RETURNS FLOAT\n  IMMUTABLE MEMOIZABLE \n  AS\n  $$\n    div0(feature , X_maxabs)\n  $$\n  ;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a897d26f-09f2-440f-aef4-e297b44e2435",
   "metadata": {
    "name": "cell89",
    "collapsed": false
   },
   "source": "We register these new SQL functions to our Snowpark session so we can use them easily in our dataframe code."
  },
  {
   "cell_type": "code",
   "id": "a39cac3b-ef82-4ba7-9041-dde58187cfa4",
   "metadata": {
    "language": "python",
    "name": "cell83",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "min_max_scale = F.builtin(f'{sess_db}.{org}.min_max_scale')\nstandard_scale = F.builtin(f'{sess_db}.{org}.standard_scale')\nmaxabs_scale = F.builtin(f'{sess_db}.{org}.maxabs_scale')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d76d2325-a3e1-4f49-8ada-86fe05b4e11d",
   "metadata": {
    "name": "cell90",
    "collapsed": false
   },
   "source": "We build up a list of expressions for each numeric columns for each of the Scaling functions, and  combine them into a single list.\n\nThis is then used to perform the scaling on the LINEITEM featureview source data.  We perform a cross (product) join from our statistical summary feature-view to ensure we have the required statistics passed into every row for computation.\n"
  },
  {
   "cell_type": "code",
   "id": "35b6906c-526f-4c3e-b488-d5294b465af3",
   "metadata": {
    "language": "python",
    "name": "cell88",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Build up scaling expressions\nmin_max_expressions = [min_max_scale(F.col(field),F.col('NUMCOL_STAT_DICT')[field]['MIN'],F.col('NUMCOL_STAT_DICT')[field]['MAX']).alias(f'{field}_MNMX_SCALED') for field in numeric_fields]\nstandard_scaled_expressions = [standard_scale(F.col(field),F.col('NUMCOL_STAT_DICT')[field]['MEAN'],F.col('NUMCOL_STAT_DICT')[field]['STDDEV']).alias(f'{field}_STD_SCALED') for field in numeric_fields]\nmax_abs_scaled_expressions = [maxabs_scale(F.col(field),F.col('NUMCOL_STAT_DICT')[field]['MAXABS']).alias(f'{field}_MAXABS_SCALED') for field in numeric_fields]\nscaled_numeric_expressions = min_max_expressions + standard_scaled_expressions + max_abs_scaled_expressions                     \n\n# Apply the scaling expressions to derive the scaled values.  We perform a cross (product) join from our statistical summary feature-view to ensure we have the required statistics passed into every row for computation.\nlineitem_numeric_scaled_features = lineitem_fv_v1.feature_df.join(lineitem_numericstatsummary_fv_v1.feature_df, how = \"cross\") \\\n              .select([F.col('ORDERKEY'), F.col('LINENUMBER')] + scaled_numeric_expressions)\nlineitem_numeric_scaled_features.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bcd261ba-d97a-4837-92b7-c976f7dc439d",
   "metadata": {
    "name": "cell91",
    "collapsed": false
   },
   "source": "We can use this dataframe to create a new FeatureView with the scaled features which will be maintained as the LINEITEM source data changes."
  },
  {
   "cell_type": "code",
   "id": "390f293a-bec9-41d1-9af9-91931b604b02",
   "metadata": {
    "language": "python",
    "name": "cell80",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# LINEITEM NUMERIC SCALED FEATURES FEATUREVIEW\n\n# Create Lineitem numeric scaled FeatureView in Feature Store\nlineitem_numeric_scaled_fv = FeatureView(\n    name = f\"FV_LINEITEM_NUM_SCALED\",\n    entities = [order_entity, lineitem_entity],\n    feature_df = lineitem_numeric_scaled_features,\n    desc = f\"Order / Lineitem level - Lineitem numeric features treated with standard, min/max and maxabs scaling\"\n)\n# Register the Lineitem numeric scaled FeatureView in the schema\nlineitem_numeric_scaled_fv_v1 = fs.register_feature_view(\n    feature_view = lineitem_numeric_scaled_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,                            # whether function call blocks until initial data is available\n    overwrite = True,                        # whether to replace existing feature view with same name/version\n)\n\nlineitem_numeric_scaled_fv_v1.feature_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "186bb4e9-c7b5-45e3-a409-f50b3e56b99f",
   "metadata": {
    "name": "cell92",
    "collapsed": false
   },
   "source": "Finally, lets combine the new Scaled FeatureView with the others we created earlier to generate a training dataset containing all the LINEITEM level features we have created including the raw untreated features."
  },
  {
   "cell_type": "code",
   "id": "43b75a2b-7b69-43c7-9f42-daa5595e7637",
   "metadata": {
    "language": "python",
    "name": "cell60",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "lineitem_order_part_supplier_customer_nation_region_spine_df.sort(keycols).show(10)\n\nlineitem_derived_and_categorical_and_numericscaled_df_training = fs.generate_training_set(\n    lineitem_order_part_supplier_customer_nation_region_spine_df,\n    features = [lineitem_fv_v1, lineitem_derived_fv_v1, lineitem_onehotenc_fv_v1, lineitem_numeric_scaled_fv_v1]\n)\nlineitem_derived_and_categorical_and_numericscaled_df_training.sort(keycols).show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ef5cf13-c6a4-4053-8356-c81182b56893",
   "metadata": {
    "name": "cell145",
    "collapsed": false
   },
   "source": "We can see all of the columns with the additional pre-processed columns appended to the end of our dataframe."
  },
  {
   "cell_type": "code",
   "id": "697ea318-3356-420f-a901-4d282193e763",
   "metadata": {
    "language": "python",
    "name": "cell63",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "lineitem_derived_and_categorical_and_numericscaled_df_training.columns",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62f4be91-7ecc-488a-bff2-c66c6457eb4e",
   "metadata": {
    "collapsed": false,
    "name": "cell43",
    "resultHeight": 211
   },
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "dcbce9b1-27df-460e-be55-976c6c105832",
   "metadata": {
    "name": "USING_SNOWFLAKEML_PREPROCESSING",
    "collapsed": false
   },
   "source": "## Feature Store & snowflake.ml.preprocessing\n\nBelow we show how you can use snowflake.ml.preprocessing functions to perform feature pre-processing with the Snowflake Feature Store.\n\nThese functions provide scalable 'drop-in' replacements for the equivalent functions in scikit-learn.  They achieve this scalability by generating SQL that performs the fitting and transformation functions, as opposed to retrieving the data into python (pandas).   In this regard they are not dissimilar to the approach we have applied earlier for our Categorical and Numeric preprocessing examples from first-principle."
  },
  {
   "cell_type": "code",
   "id": "57ef56cc-0f9b-4bfd-9d43-cba399a79b08",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "# Recreate a dataframe.  We will derive one from our Customer Feauture View\ncustomer_training_df = fs.generate_training_set(\n    customer_spine_df,\n    features = [customer_fv_v1]\n)\ncustomer_training_df.sort('CUSTKEY').show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2444eb42-25b7-4aed-8ee0-84d28145dabe",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "### Fit & Transform an Ordinal Encoder "
  },
  {
   "cell_type": "code",
   "id": "e4f5d391-d2aa-4165-b420-f8fc244e0c80",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder(\n    input_cols=[\"C_MKTSEGMENT\"],\n    output_cols=[\"ORDINAL_C_MKTSEGMENT\"]\n)\nencoded_fit = encoder.fit(customer_fv_v1.feature_df)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f2d336e-827a-4c18-b094-cf715d50cbfe",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "If we look at our Query History in Snowsight we should see that the `fit` has executed a couple of queries similar to this to captured the category state of the 'C_MKTSEGMENT' column.\n\n```\nCREATE  OR  REPLACE  TEMPORARY  TABLE  SNOWPARK_TEMP_TABLE_RFC3HM61NN(\"CUSTKEY\" BIGINT NOT NULL , \"C_NAME\" STRING(25), \"C_ADDRESS\" STRING(40), \"C_PHONE\" STRING(15), \"C_ACCTBAL\" NUMBER(12, 2), \"C_MKTSEGMENT\" STRING(10), \"C_COMMENT\" STRING(117), \"ORDINAL_C_MKTSEGMENT\" DOUBLE)    AS  SELECT  *  FROM ( SELECT \"CUSTKEY\", \"C_NAME\", \"C_ADDRESS\", \"C_PHONE\", \"C_ACCTBAL\", \"C_MKTSEGMENT\", \"C_COMMENT\", \"ORDINAL_C_MKTSEGMENT\" FROM ( SELECT  *  FROM (( SELECT \"CUSTKEY\" AS \"CUSTKEY\", \"C_NAME\" AS \"C_NAME\", \"C_ADDRESS\" AS \"C_ADDRESS\", \"C_PHONE\" AS \"C_PHONE\", \"C_ACCTBAL\" AS \"C_ACCTBAL\", \"C_MKTSEGMENT\" AS \"C_MKTSEGMENT\", \"C_COMMENT\" AS \"C_COMMENT\" FROM (\n                    SELECT\n                        l_0.*,\n                        r_0.* EXCLUDE (CUSTKEY)\n                    FROM (SELECT  *  FROM SIMON.TPCHSF1.CUSTOMER_SPINE) l_0\n                    LEFT JOIN (\n                        SELECT CUSTKEY, C_NAME, C_ADDRESS, C_PHONE, C_ACCTBAL, C_MKTSEGMENT, C_COMMENT\n                        FROM SIMON.TPCHSF1_FEATURE_STORE.FV_CUSTOMER$V1\n                    ) r_0\n                    ON l_0.CUSTKEY = r_0.CUSTKEY\n                )) AS SNOWPARK_LEFT LEFT OUTER JOIN ( SELECT \"_CATEGORY\" AS \"_CATEGORY\", \"ORDINAL_C_MKTSEGMENT\" AS \"ORDINAL_C_MKTSEGMENT\" FROM ( SELECT \"_CATEGORY\", \"_INDEX\" AS \"ORDINAL_C_MKTSEGMENT\" FROM (( SELECT  *  FROM SNOWPARK_TEMP_TABLE_D1FK378MCT WHERE \"_CATEGORY\" IS NOT NULL) UNION ( SELECT \"_COLUMN_NAME\", \"_CATEGORY\", 'NAN' :: FLOAT AS \"_INDEX\" FROM SNOWPARK_TEMP_TABLE_D1FK378MCT WHERE \"_CATEGORY\" IS NULL)) WHERE (\"_COLUMN_NAME\" = 'C_MKTSEGMENT'))) AS SNOWPARK_RIGHT ON EQUAL_NULL( CAST (\"C_MKTSEGMENT\" AS STRING), \"_CATEGORY\"))))\n```\nand\n```\n( SELECT 'C_MKTSEGMENT' AS \"_COLUMN_NAME\",  CAST (\"C_MKTSEGMENT\" AS STRING) AS \"_CATEGORY\",  CAST ((dense_rank() OVER (  ORDER BY \"C_MKTSEGMENT\" ASC NULLS FIRST ) - 1) AS FLOAT) AS \"_INDEX\" FROM ( SELECT  *  FROM ( SELECT \"C_MKTSEGMENT\" FROM ( SELECT \"C_MKTSEGMENT\" FROM (\n                    SELECT\n                        l_0.*,\n                        r_0.* EXCLUDE (CUSTKEY)\n                    FROM (SELECT  *  FROM SIMON.TPCHSF1.CUSTOMER_SPINE) l_0\n                    LEFT JOIN (\n                        SELECT CUSTKEY, C_NAME, C_ADDRESS, C_PHONE, C_ACCTBAL, C_MKTSEGMENT, C_COMMENT\n                        FROM SIMON.TPCHSF1_FEATURE_STORE.FV_CUSTOMER$V1\n                    ) r_0\n                    ON l_0.CUSTKEY = r_0.CUSTKEY\n                )) GROUP BY \"C_MKTSEGMENT\") WHERE \"C_MKTSEGMENT\" IS NOT NULL ORDER BY \"C_MKTSEGMENT\" ASC NULLS FIRST)) UNION ( SELECT 'C_MKTSEGMENT' AS \"_COLUMN_NAME\",  CAST (\"C_MKTSEGMENT\" AS STRING) AS \"_CATEGORY\", 'NAN' :: FLOAT AS \"_INDEX\" FROM ( SELECT \"C_MKTSEGMENT\" FROM ( SELECT \"C_MKTSEGMENT\" FROM (\n                    SELECT\n                        l_0.*,\n                        r_0.* EXCLUDE (CUSTKEY)\n                    FROM (SELECT  *  FROM SIMON.TPCHSF1.CUSTOMER_SPINE) l_0\n                    LEFT JOIN (\n                        SELECT CUSTKEY, C_NAME, C_ADDRESS, C_PHONE, C_ACCTBAL, C_MKTSEGMENT, C_COMMENT\n                        FROM SIMON.TPCHSF1_FEATURE_STORE.FV_CUSTOMER$V1\n                    ) r_0\n                    ON l_0.CUSTKEY = r_0.CUSTKEY\n                )) GROUP BY \"C_MKTSEGMENT\") WHERE \"C_MKTSEGMENT\" IS NULL)\n```                \n\nIf we then transform the input dataframe with the encoder, and look at the SQL generated in Query History"
  },
  {
   "cell_type": "code",
   "id": "4f0022f8-2db2-4dd1-ba95-860300d70a10",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "encoded_df = encoder.transform(customer_fv_v1.feature_df)\nencoded_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a60a83d8-d898-4463-8fce-1bfad2efa413",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "In Snowsight we can see that the SQL is referring to the temporary table created during the `fit`.\n```\nSELECT \"'C_MKTSEGMENT'\" AS \"COLUMN_NAME\", \"C_MKTSEGMENT\" AS \"UNKNOWN_VALUE\" \nFROM \n  ( SELECT 'C_MKTSEGMENT', \"C_MKTSEGMENT\" \n  FROM \n    ( SELECT \"ORDINAL_C_MKTSEGMENT\", \"C_MKTSEGMENT\" \n     FROM \n        ( SELECT \"ORDINAL_C_MKTSEGMENT\", \"C_MKTSEGMENT\" \n          FROM SNOWPARK_TEMP_TABLE_YXMJ72O95D) \n     GROUP BY \"ORDINAL_C_MKTSEGMENT\", \"C_MKTSEGMENT\") \n     WHERE \"ORDINAL_C_MKTSEGMENT\" IS NULL)\n```     \n\nThis is a problem if we want to use this as a Feature View as the temproary table will only persist for the duration of the session we are working in.  We need that captured data to be persisted so we can use it in a Feature View.\n\n`snowflake.ml.modelling.preprocessing` generates SQL that executes against Snowflake tables to ensure that the preprocessing functions can scale over very large datasets.  This is different to the way that the `sklearn.preprocessing` equivalent functions work, as they execute over Pandas data in-memory.\n\nWe can see the issue that the original code is hitting as we have a `CREATE  OR  REPLACE  TEMPORARY  TABLE` statement created to temporarily capture the information need for the `transform` step\n\nThe temporary table cannot be used within a View/Dynamic Table created via the FeatureView, as it only persists for the duration of the session it was executed in.\n\nThere are several ways that we can solve this.  We could extract an sklearn object from the fitted object ( `encoded_fit.to_sklearn()` ) , and then use that to create a Python UDF that performs the `transform` step.  \n\nOr we could create the required SQL using Snowpark, that will likely be a bit more efficient. There are a number of attributes of our original fitted object.  Three of these we can make use of to generate the required encoded column expressions that we are after."
  },
  {
   "cell_type": "markdown",
   "id": "c00ac399-31ea-4854-b160-45ed745955a2",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "f8c9d726-15b3-4252-80bb-a1972bdf5986",
   "metadata": {
    "language": "python",
    "name": "cell13",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "print(encoded_fit.input_cols)\nprint(encoded_fit.output_cols)\nprint(encoded_fit.categories_)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5566ec8c-ce2e-4398-a905-6a76db4f4bfe",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "We can create a function that returns a list of Case Expressions for each of the Ordinal Encoded columns that we have fitted."
  },
  {
   "cell_type": "code",
   "id": "c76a1f0d-e593-407c-8cf3-41b9932b2e54",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": "def ordinal_to_snowpark_expr(encoded_fit):\n    case_expressions = []\n    # Loop over each column we want to encode\n    for input_col, output_col in zip(encoded_fit.input_cols, encoded_fit.output_cols):\n        # Fetch categories and ensure conversion to list\n        categories_ndarray = encoded_fit.categories_.get(input_col, [])\n        categories = categories_ndarray.tolist() if isinstance(categories_ndarray, np.ndarray) else categories_ndarray\n        if not categories:\n            raise ValueError(f\"No categories found for column: {input_col}\")\n        \n        # Start a fresh case expression for this column\n        case_expr = None\n        # Build the ordinal when chain for this column\n        for idx, category in enumerate(categories):\n            if case_expr is None:\n                case_expr = F.when(F.col(input_col) == F.lit(category), F.lit(idx))\n            else:\n                case_expr = case_expr.when(F.col(input_col) == F.lit(category), F.lit(idx))\n\n        # Handle unknown categories (assign -1 if not found in the categories list)\n        if case_expr is not None:\n            case_expr = case_expr.otherwise(F.lit(-1))\n            case_expressions.append(case_expr.alias(output_col))\n        else:\n            raise ValueError(f\"Failed to construct case expression for column: {input_col}\")\n\n    return case_expressions",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "96257445-2842-42f1-9c2c-ad5c0f6e9a3a",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "We can then use this in a dataframe, for example with `with_columns` to add the ordinal encoded columns to our original dataframe."
  },
  {
   "cell_type": "code",
   "id": "7cab8af7-0604-46a8-8b42-365da2565a44",
   "metadata": {
    "language": "python",
    "name": "cell22",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "customer_ordinalEncodedCats_df = customer_fv_v1.feature_df.with_columns(encoded_fit.output_cols, ordinal_to_snowpark_expr( encoded_fit))['CUSTKEY','C_MKTSEGMENT', 'ORDINAL_C_MKTSEGMENT']\ncustomer_ordinalEncodedCats_df.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0531b95e-e6ca-43ef-ab25-2337cda518e1",
   "metadata": {
    "language": "python",
    "name": "cell94",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# CUSTOMER ENCODED FEATUREVIEW\n\n# Create Customer Encoded FeatureView in Feature Store\ncustomer_ordinalEncodedCats_fv = FeatureView(\n    name = f\"FV_CUSTOMER_ORDINALENCODEDCATS\",\n    entities = [customer_entity],\n    feature_df = customer_ordinalEncodedCats_df,\n    desc = f\"Customer categorical columns original and ordinal encoded feature view\"\n)\n\n# Register the Customer Encoded  FeatureView in the schema, and add the python featureview to our list\ncustomer_ordinalEncodedCats_fv_v1 = fs.register_feature_view(\n    feature_view = customer_ordinalEncodedCats_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b2bbaf2-cfa4-45ce-a158-2dd2b31102df",
   "metadata": {
    "language": "python",
    "name": "cell95"
   },
   "outputs": [],
   "source": "customer_ordinalEncodedCats_fv_v1.feature_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1cb3a005-d649-41f0-941b-d4c3f66aba87",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "Let's check that our function also works when we supply multiple categorical columns to be encoded.  We will use the lineitem training dataframe we created earlier."
  },
  {
   "cell_type": "code",
   "id": "df6168ac-16dd-4b2e-845e-7afb769efa22",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "lineitem_training = fs.generate_training_set(\n    lineitem_spine_df,\n    features = [lineitem_fv_v1]\n)\n\nlineitem_training.select(F.col(\"ORDERKEY\"),F.col(\"LINENUMBER\"),F.col(\"L_RETURNFLAG\"),F.col(\"L_LINESTATUS\"), F.col(\"L_SHIPINSTRUCT\"), F.col(\"L_SHIPMODE\")).sort('ORDERKEY', 'LINENUMBER').show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21c7a896-b993-4e9d-ad44-a1869639672f",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.preprocessing import OrdinalEncoder\nli_input_cols = [\"L_RETURNFLAG\", \"L_LINESTATUS\", \"L_SHIPINSTRUCT\",\"L_SHIPMODE\"]\nli_output_cols = [f\"ORDINAL_{col}\" for col in li_input_cols]\nli_encoder = OrdinalEncoder(\n    input_cols=li_input_cols,\n    output_cols=li_output_cols\n)\nli_encoded_fit = li_encoder.fit(lineitem_training)\nprint(li_encoded_fit.input_cols)\nprint(li_encoded_fit.output_cols)\nprint(li_encoded_fit.categories_)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c74afb4d-d7ec-45bd-bafc-2e386dc86993",
   "metadata": {
    "language": "python",
    "name": "cell23",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "lineitem_ordinalEncodedCats_df = ( lineitem_training\n    .select(F.col(\"ORDERKEY\"),F.col(\"LINENUMBER\"),F.col(\"L_RETURNFLAG\"),F.col(\"L_LINESTATUS\"), F.col(\"L_SHIPINSTRUCT\"), F.col(\"L_SHIPMODE\"))\n    .with_columns(li_encoder.output_cols, ordinal_to_snowpark_expr( li_encoder))\n                                 )\nlineitem_ordinalEncodedCats_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42949196-8344-4bf8-b57f-1f84bd0539e4",
   "metadata": {
    "language": "python",
    "name": "cell24",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# LINEITEM FEATUREVIEW\n\n# Create Lineitem FeatureView in Feature Store\nlineitem_ordinalEncodedCats_fv = FeatureView(\n    name = f\"FV_LINEITEM_ORDINALENCODEDCATS\",\n    entities = [order_entity, lineitem_entity],\n    feature_df = lineitem_ordinalEncodedCats_df,\n    desc = f\"Lineitem categorical columns original and ordinal encoded feature view\"\n)\n\n# Register the Lineitem FeatureView in the schema, and add the python featureview to our list\nlineitem_ordinalEncodedCats_fv_v1 = fs.register_feature_view(\n    feature_view = lineitem_ordinalEncodedCats_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd71cbe4-8103-4e19-8668-543c11c6d6f1",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "lineitem_ordinalEncodedCats_fv_v1.feature_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "add6b4f9-34fc-4729-bec1-69f47944cb4d",
   "metadata": {
    "name": "PARTITIONED_PREPROCESSING",
    "collapsed": false
   },
   "source": "## Preprocessing on partitioned sub-samples.\n\nRather than pre-processing over an entire dataset, we might need to partition the dataset in some way, and pre-process the data for each partition.  For example if we are training many models, one per partition, or we know that there is a high-degree of variance across the partitions. \n\nThere are a number of different ways of achieving this. Firstly, we can use scikit-learn or snowflake.ml to fit a preprocessing function on each sub-sample and capture the fitted model. For example we could use a partitioned table function to fit on each subset of data.\n\nOnce we have the fitted models we can make use of them via Snowpark or Snowflake.ml with model-registry. We can then use them with Snowpark/SQL to 'transform' data for each partition."
  },
  {
   "cell_type": "code",
   "id": "73568ba3-34c2-4013-9f8d-24f0fc79b3bf",
   "metadata": {
    "language": "python",
    "name": "Create_StandardScaler_Fit_Partitioned_UDTF",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.types import PandasDataFrame\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nimport cloudpickle\nimport codecs\nimport base64\nimport ast\n\n# Create a stage for our Python function code to be stored in.  \n# This is only needed if the Function is created as a permanent function.\nsession.sql('''create stage if not exists py_code;''').collect()\n\n# Create functions to encode and decode a model so that it can be saved/reused\n## Encode\ndef pickle_encode_model(model):\n    return codecs.encode(cloudpickle.dumps(model), \"base64\").decode()\n## Decode\ndef pickle_decode_model(model):\n    return cloudpickle.loads(codecs.decode(model.encode(), \"base64\"))\n\n# Create Python Class to be used in UDTF for fitting a model on sub-samples of data.\n# The data will be partitioned by Snowflake using a partitioned table function call.\nclass pp_std_scaler:\n    def end_partition(self, df: PandasDataFrame[str, list]) -> PandasDataFrame[str, str, str, str, int, int, str]:\n        # Directly convert the array column to a proper 2D numpy array\n        array_data = np.array(df['NUMERIC_COL_ARRAY'].tolist())\n\n        # Fit the scaler on the array data\n        scaler = StandardScaler()\n        scaler.fit(array_data)\n\n        # Encode the scaler model into a base64 pickle string\n        return pd.DataFrame(data={'PARTITION_COL':df['PARTITION_COL'].iloc[0], \n                                  'SCALES' : str(scaler.scale_.tolist()),\n                                  'MEAN' : str(scaler.mean_.tolist()),\n                                  'VARIANCE' : str(scaler.var_.tolist()),\n                                  'NROWS_IN_FIT_GROUP' : int(scaler.n_samples_seen_),\n                                  'N_FEATURES_IN_ARRAY' : scaler.n_features_in_,\n                                  'SS_MODEL_PKL': [pickle_encode_model(scaler)] })\n\n# Create a Snowflake Python UDTF from the class (pp_std_scaler)\npp_standard_scaler_udtf_input_cols = ['PARTITION_COL','NUMERIC_COL_ARRAY']\npp_standard_scaler_udtf_output_cols = ['PARTITION_COL', 'SCALES', 'MEAN', 'VARIANCE','N_ROWS_IN_FIT_GROUP', 'N_FEATURES_IN_ARRAY', 'SS_MODEL_PKL']\npp_standard_scaler_udtf = session.udtf.register(\n  pp_std_scaler,\n  is_permanent = True,\n  name = 'PP_STD_SCALER',\n  replace = True,  \n  stage_location = 'PY_CODE',  \n  packages = ['numpy==1.23.5', 'pandas', 'scikit-learn==1.5.2'],\n  input_names = pp_standard_scaler_udtf_input_cols, \n  output_schema = pp_standard_scaler_udtf_output_cols\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f467ed0-58a6-4932-9e8c-ee6b3302f702",
   "metadata": {
    "name": "demo_test_data_md",
    "collapsed": false
   },
   "source": "#### Use the Scaler Fitting Function to Fit on subsets of data\n\nWe will use our `LINEITEM` table (FeatureView) as to demonstrate with.  We will use the `L_RETURNFLAG` field to create a fitted model per return flag category. We can see there is a somewhat uneven distribution of rows per `L_RETURNFLAG` value."
  },
  {
   "cell_type": "code",
   "id": "859bb23c-8054-4481-90e6-330b8434833a",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "lineitem_fv_v1.feature_df.group_by(\"L_RETURNFLAG\").agg(F.count(\"*\").alias(\"COUNT\"))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9559637d-a2eb-489c-b42e-abcbdd483cd6",
   "metadata": {
    "name": "define_input_columns_md",
    "collapsed": false
   },
   "source": "Define the Entity-Key columns, column for sub-sample (partitionkey), and the numeric features we want to work with."
  },
  {
   "cell_type": "code",
   "id": "3b1b9ac0-bfa3-48c8-b501-2e3ccabf9c90",
   "metadata": {
    "language": "python",
    "name": "define_input_columns",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Entity Keys\nentitykeys = ['ORDERKEY','LINENUMBER']\n\n# Partition key - (standard-scaler model created for each partition)\npartitionkey = 'L_RETURNFLAG'\n\n# Feature columns - we can standard-scale any/all numeric features\n\n## Identify all numeric columns programmatically\nnumeric_types = [T.DecimalType, T.DoubleType, T.FloatType, T.IntegerType, T.LongType, T.ShortType]\nnumeric_columns = [field.name for field in lineitem_fv_v1.feature_df.schema.fields if type(field.datatype) in numeric_types]\n\n## Remove key columns from list\nnumeric_feature_columns = [numcol for numcol in numeric_columns if '_' in numcol] # only include Feature columns (e.g. those with L_ as the column name)\n\n## Create a comma delimited string form of Numeric columns ASIS\nnfc_string = ', '.join(numeric_feature_columns)  \n\n## Comma delimited string form of Numeric columns including cast as float and column rename\nnfc_float_string = ', '.join([f'{nfc}::FLOAT {nfc}_FLT' for nfc in numeric_feature_columns ]) \n\n## Comma delimited string form of Numeric columns renamed for float type\nnfc_float_colname_string = ', '.join([f'{nfc}_FLT' for nfc in numeric_feature_columns ])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb3abb68-a7b3-4d6c-b4a3-5dd9b52ec673",
   "metadata": {
    "name": "FIT_Use_UDTF_from_SQL_md",
    "collapsed": false
   },
   "source": "Here we show the SQL method of using the Python UDTF.  Rather than hardcoding the column names we are generating them programmatically and adding them to the SQL code using templating `{{ }}`"
  },
  {
   "cell_type": "code",
   "id": "efd269e1-22c6-4425-95c2-b8958e556c66",
   "metadata": {
    "language": "sql",
    "name": "FIT_Use_UDTF_from_SQL",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Using SQL with table function\n-- # Prepare source data for input\nwith t as (select {{ \", \".join(entitykeys) }} , -- Key-Columns\n                  {{partitionkey}},             -- Partition Column\n                  {{nfc_float_string}}          -- Feature-Columns\n             from {{lineitem_fv_v1.fully_qualified_name()}} \n             limit 1000 -- Limit the rows for demonstration purposes\n             )\n-- # Execute table-function over input data to FIT standard-scaler per L_RETURNFLAG category             \nselect \n  PARTITION_COL,                       -- The partitioning column - L_RETURNFLAG in our demonstration data\n  parse_json(SCALES)   SCALES_ARRAY,   -- Contains an array of the scales needed to scale each input-column\n  parse_json(MEAN)     MEAN_ARRAY,     -- Contains an array of the means for each input-column \n  parse_json(VARIANCE) VARIANCE_ARRAY, -- Contains an array of the Variance for each input-column \n  N_ROWS_IN_FIT_GROUP,                 -- Number of rows passed into each partition for the model fit.\n  N_FEATURES_IN_ARRAY,                 -- Number of features being fitted.\n  SS_MODEL_PKL                         -- Pickle encoded form of the StandardScaler model\nfrom t\n  , table(\n      PP_STD_SCALER({{partitionkey}}, \n                 -- Create an array of feature columns.  \n                 -- Using an array allows the function to operate over a variable number of feature-columns\n                    array_construct({{nfc_float_colname_string}}))  \n                 -- This is the column that the UDTF will use for partitioning the data.                      \n                    over ( partition by {{partitionkey}} ) \n          );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "162a7d2f-df60-46d6-b17f-94d8655e42e8",
   "metadata": {
    "name": "cell26",
    "collapsed": false
   },
   "source": "Here is the Snowpark dataframe equivalent of the SQL statement above. "
  },
  {
   "cell_type": "code",
   "id": "2ef5a6e7-033f-495a-b1c0-2ad706c9320e",
   "metadata": {
    "language": "python",
    "name": "FIT_Use_UDTF_from_Python_Snowpark",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# FIT - Using Snowpark with Table Function\n\n# Prepare source data for input\nin_df = (\n  lineitem_fv_v1.feature_df.select(*[[F.col(ek) for ek in entitykeys] + # Entity Key columns\n                                    [partitionkey] + # Partitioning Key column\n                                    [F.col(nfc) for nfc in  numeric_feature_columns] + # AsIs Numeric Columns\n                                    [ F.col(nfc).cast(T.FloatType()).as_(f'{nfc}_FLT') for nfc in  numeric_feature_columns] + # Numeric columns cast as Float\n                                    [F.array_construct( *[F.col(f'{nfc}_FLT') for nfc in  numeric_feature_columns]).as_('NUMERIC_COL_ARRAY')] # Array of Numeric Column Float values\n                                   ]) \n)\n# Reference to Table function to Snowpark \npp_standard_scaler_udtf = F.table_function(\"PP_STD_SCALER\")\n# Execute table-function over input data to FIT standard-scaler per L_RETURNFLAG category\nrf_fit_ss = (\n    in_df.selectExpr(f'''* exclude ({', '.join(entitykeys)})''').\n    join_table_function(pp_standard_scaler_udtf(in_df[partitionkey], in_df[\"NUMERIC_COL_ARRAY\"])\n                                                      .over(partition_by=partitionkey) ) \\\n    .select(*[\n          # [F.col(ek) for ek in entitykeys] +\n           [F.col('PARTITION_COL'), \n            F.parse_json(F.col('SCALES')).as_('SCALES_ARRAY'),\n            F.parse_json(F.col('MEAN')).as_('MEAN_ARRAY'),\n            F.parse_json(F.col('VARIANCE')).as_('VARIANCE_ARRAY'),\n            F.col('N_ROWS_IN_FIT_GROUP'),\n            F.col('N_FEATURES_IN_ARRAY'),         \n            F.col('SS_MODEL_PKL')] \n          ] )\n)\nrf_fit_tname = f'''{sess_db}.{fs_name}.LINEITEM_RETURNFLAG_STD_SCALER_FIT'''\nrf_fit_ss.write.mode(\"overwrite\").save_as_table(rf_fit_tname, table_type=\"\")\n\nrf_fit_sdf = session.table(rf_fit_tname)\nrf_fit_sdf.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ef361ab-0833-4e31-9275-477cfb22f906",
   "metadata": {
    "name": "cell27",
    "collapsed": false
   },
   "source": "We can use the table containing the fitted values above to transform the source-data (`lineitem_fv_v1`) via a join and some simple transformations."
  },
  {
   "cell_type": "code",
   "id": "96d847ef-43c0-4c8c-ae2b-4e1fe9bdcd2f",
   "metadata": {
    "language": "python",
    "name": "TRANSFORM_Snowpark",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Standard Scaler Expressions.\n# For each feature (array by index) we apply the scaling value (obtained from array by index)\nss_expr_dict = { f'{nfc}_SS' : (F.col('NUMERIC_COL_ARRAY')[i]/F.col('SCALES_ARRAY')[i]) for i, nfc in enumerate(numeric_feature_columns) }\n\n# TRANSFORM \n# We join the source data to the fitted-models results using the scales extracted from the model within the UDTF\nrf_transform_ss =  (\n     rf_fit_sdf\n    .join(in_df, (rf_fit_sdf['PARTITION_COL'] == in_df[partitionkey]) )\n    .with_columns(list(ss_expr_dict.keys()), list(ss_expr_dict.values())) \n    .select(*[\n              [F.col(ek) for ek in entitykeys] + # Entity Key columns\n              [F.col(partitionkey)] + # Partitioning Key column\n              [F.col(nfc) for nfc in  numeric_feature_columns] + # AsIs Numeric Feature Columns\n              [F.col(nfc).cast(T.FloatType()).as_(f'{nfc}_FLT') for nfc in  numeric_feature_columns] + # Numeric Feature Columns cast as Float\n              list(ss_expr_dict.keys()) # Standard Scaled Numeric Features\n              ])\n)\n\nrf_transform_ss.sort('ORDERKEY', 'LINENUMBER', 'L_RETURNFLAG').show(20)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "523800a7-f9bb-4544-9f17-fcf4b5e96d37",
   "metadata": {
    "name": "Transformed_Dataframe_Application_md",
    "collapsed": false
   },
   "source": "As we have seen in the prior examples, we can use the Dataframe definition above to create a FeatureView in our Feature Store with the preprocessed values.  If the range of data in the source table is changing frequently we may want to retrain the standard-scaler FIT on the new data regularly.  This could be done via a scheduled TASK for example.  We may also need to keep a history of these values through time so that we can apply the correct values at a given point in time, which we can do by adding a timestamp column to the maintained value table. \n\nWe can also combine the fit and transform steps into a single operation returning a dataframe. If this is used to create a `view` based FeatureView, it will always return the scaled results, as of the latest fitted state from the source FeatureView. "
  },
  {
   "cell_type": "code",
   "id": "c1d8ad6b-d29e-4697-94a2-ee56a8b84224",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "# FIT & TRANSFORM \n# We join the source data to the fitted-models results using the scales extracted from the model within the UDTF\nrf_fit_and_transform_ss =  (\n     rf_fit_ss\n    .join(in_df, (rf_fit_sdf['PARTITION_COL'] == in_df[partitionkey]) )\n    .with_columns(list(ss_expr_dict.keys()), list(ss_expr_dict.values())) \n    .select(*[\n              [F.col(ek) for ek in entitykeys] + # Entity Key columns\n              [F.col(partitionkey)] + # Partitioning Key column\n              [F.col(nfc) for nfc in  numeric_feature_columns] + # AsIs Numeric Feature Columns\n              [F.col(nfc).cast(T.FloatType()).as_(f'{nfc}_FLT') for nfc in  numeric_feature_columns] + # Numeric Feature Columns cast as Float\n              list(ss_expr_dict.keys()) # Standard Scaled Numeric Features\n              ])\n)\n\nrf_fit_and_transform_ss.sort('ORDERKEY', 'LINENUMBER', 'L_RETURNFLAG').show(20)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3040f1ed-de8d-4a3f-a18b-9a2ed2ac5728",
   "metadata": {
    "name": "Model_Registry__ManyModelInference",
    "collapsed": false
   },
   "source": "#### Model Registry - Many Model Inference\n\nWe can make use of the trained scaler models with Snowflake Model Registrys many-model inference capabilities.\n\n"
  },
  {
   "cell_type": "code",
   "id": "898ee4fe-2b72-43c7-9455-c88805db41d5",
   "metadata": {
    "language": "python",
    "name": "define_model_registry",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Name of the schema where we will persist our generated training datasets\nsession.sql(f''' Create schema if not exists {sess_db}.{mr_name}''').collect()\n\n# Log model\nreg = registry.Registry(session=session, \n                        database_name=sess_db, \n                        schema_name=mr_name)\n\nreg.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ab1b4b3-729d-4189-8886-3c93401f16b2",
   "metadata": {
    "name": "ManyModel_Partitioned_md",
    "collapsed": false
   },
   "source": "#### Define the Partitioned Model\nWe will now define a custom model. The partitoned custom model class inherits from `snowflake.ml.model.custom_model.CustomModel`, and inference methods are declared with the `@custom_model.partitioned_inference_api` decorator. Writing the model in this way allows it to run in parallel for each partition.\n\nTo make this flexible and generic to support varying numbers of numeric input columns we will pass the columns in as an `ARRAY`, rather than as individual columns."
  },
  {
   "cell_type": "code",
   "id": "e7d0e312-86d2-4f24-98a6-c81652999f23",
   "metadata": {
    "language": "python",
    "name": "input_output_cols"
   },
   "outputs": [],
   "source": "partitionkey = 'L_RETURNFLAG'\npp_standard_scaler_udtf_input_cols = ['PARTITION_COL','NUMERIC_COL_ARRAY']\npp_standard_scaler_udtf_output_cols = ['PARTITION_COL', 'SCALES', 'MEAN', 'VARIANCE','N_ROWS_IN_FIT_GROUP', 'N_FEATURES_IN_ARRAY', 'SS_MODEL_PKL']\nLReturnFlag_StandardScaler_input_cols =  ['PARTITION_COL','NUMERIC_COL_ARRAY']\nLReturnFlag_StandardScaler_output_cols = ['PARTITION_COL', 'SCALES', 'MEAN', 'VARIANCE','N_ROWS_IN_FIT_GROUP', 'N_FEATURES_IN_ARRAY', 'SS_MODEL_PKL']\nLReturnFlag_SS_NumFeatures = [ f'{nfc}_SS' for nfc in numeric_feature_columns ]",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f916dc5c-c876-4f1c-bdfc-9a02fa065588",
   "metadata": {
    "name": "MODEL_FROM_TABLE",
    "collapsed": false
   },
   "source": "We will first create a many-model Custom Model where the models are persisted and maintained in a table by partitioning key (e.g. L_RETURNFLAG).  The models are joined to each row at query time, to perform the transformation."
  },
  {
   "cell_type": "code",
   "id": "8c081635-4206-468d-84ec-1f7bcc4499f6",
   "metadata": {
    "language": "python",
    "name": "create_table_model",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Create custom model class.\n## Note the use of the  @custom_model.partitioned_inference_api to denote the model as partitioned\n\nclass LReturnFlag_StandardScaler_Table(custom_model.CustomModel):\n    def __init__(self, context: Optional[custom_model.ModelContext] = None) -> None:\n        super().__init__(context)\n        self.partition_id = None\n        self.model = None\n        self._cache = {}  # Add a cache to store models\n\n    @custom_model.partitioned_inference_api\n    def predict(self, input: pd.DataFrame) -> pd.DataFrame:\n        current_flag = input['L_RETURNFLAG'].iloc[0]\n        \n        # Check the cache first.  This should only need to run for the first row in each partition\n        if current_flag != self.partition_id:\n            if current_flag not in self._cache:\n                # Only decode and load model if not in cache\n                model_pickle = input['SS_MODEL_PICKLE'].iloc[0]\n                self._cache[current_flag] = cloudpickle.loads(\n                    codecs.decode(model_pickle.encode(), \"base64\")\n                )\n            self.model = self._cache[current_flag]\n            self.partition_id = current_flag\n\n        # Convert numeric arrays more efficiently using numpy\n        numeric_arrays = np.array([json.loads(x) for x in input['NUMERIC_COL_ARRAY']])\n        transformed_arrays = self.model.transform(numeric_arrays)\n\n        # Create result DataFrame more efficiently\n        result = pd.DataFrame({\n            'ORDERKEY_OUT': input['ORDERKEY'],\n            'LINENUMBER_OUT': input['LINENUMBER'],\n            'NUMERIC_COL_ARRAY': input['NUMERIC_COL_ARRAY'],\n            'NUMERIC_COL_ARRAY_SS': [json.dumps(row.tolist()) for row in transformed_arrays],\n        })\n\n        return result\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "650b6096-cb2f-45b4-9b3b-dc4e4833d880",
   "metadata": {
    "language": "python",
    "name": "cell146",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Create model signatures \n# - Note we are passing in the entity-key columns for the data so that we can join these transformed features to other featureviews if needed\ninput_signature= [\n  FeatureSpec(dtype=DataType.INT64 , name='ORDERKEY', nullable=True),\n  FeatureSpec(dtype=DataType.INT64 , name='LINENUMBER', nullable=True),    \n  FeatureSpec(dtype=DataType.STRING , name=partitionkey, nullable=True),\n  FeatureSpec(dtype=DataType.STRING , name='NUMERIC_COL_ARRAY', nullable=True),\n  FeatureSpec(dtype=DataType.STRING,  name='SS_MODEL_PICKLE', nullable=True),\n]\n# - Note we are passing out the entity-key columns for the data so that we can join these transformed features to other featureviews if needed.\n# - We give them a different name to avoid a result set error with ambiguous names.\noutput_signature = [\n  FeatureSpec(dtype=DataType.INT64 , name='ORDERKEY_OUT', nullable=True),\n  FeatureSpec(dtype=DataType.INT64 , name='LINENUMBER_OUT', nullable=True),     \n  FeatureSpec(dtype=DataType.STRING , name='NUMERIC_COL_ARRAY_OUT', nullable=True), \n  FeatureSpec(dtype=DataType.STRING , name='NUMERIC_COL_ARRAY_SS', nullable=True),     \n]\n\nsignature = ModelSignature(\n    inputs=input_signature,\n    outputs=output_signature,\n)\n\nLReturnFlag_StandardScaler_Table_instance = LReturnFlag_StandardScaler_Table()\nLReturnFlag_StandardScaler_Table_instance",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df4d573b-7475-41bb-9144-f4ace295e719",
   "metadata": {
    "name": "test_manymodel_class_locally_md",
    "collapsed": false
   },
   "source": "We can test the many-model class locally with a dataframe."
  },
  {
   "cell_type": "code",
   "id": "10dc270c-f09f-4d05-a387-cdc5fd9aa0c0",
   "metadata": {
    "language": "python",
    "name": "test_mm_class_locally",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Convert Snowpark Dataframe of source data to be standard-scaled to Pandas\nin10_pdf = in_df.limit(10).to_pandas()\n\n# Convert Snowpark Dataframe containing models to pandas\nrf_fit_pdf = rf_fit_sdf.select(F.col('PARTITION_COL'), F.col('SS_MODEL_PKL').as_('SS_MODEL_PICKLE') ).to_pandas()\n\n# Join to Pandas Dataframes together\npredict_input_pdf = pd.merge(rf_fit_pdf, in10_pdf, left_on='PARTITION_COL', right_on='L_RETURNFLAG')\n\n# Test predict method of class works on Pandas Dataframe, which is what the data gets coverted to to execute in database.\nrf_transform_scaled_pdf = LReturnFlag_StandardScaler_Table_instance.predict(predict_input_pdf)\n\nrf_transform_scaled_pdf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85de52a4-5655-4d20-a4db-5b6e441f6f8d",
   "metadata": {
    "name": "log_manymodel_model_registry_md",
    "collapsed": false
   },
   "source": "####  Log Model to Model Registry\nNext we will log the model to Snowflake Model Registry. We will first define the signature for our prediction method, then define the registry, and finally log the model."
  },
  {
   "cell_type": "code",
   "id": "eee291a4-2c65-4268-9950-2b7a80c3f409",
   "metadata": {
    "language": "python",
    "name": "delete_prior_table_model",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "try:\n    reg.delete_model(\"LReturnFlag_StandardScaler_Table\")\nexcept:\n    print('Model does not exist in model registry')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48d53073-9108-4029-a0ce-16672871add5",
   "metadata": {
    "language": "python",
    "name": "register_table_model",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "start = time.time() \n\noptions = {\n    \"function_type\": \"TABLE_FUNCTION\",\n    \"relax_version\": False\n}\n\nmv_table = reg.log_model(\n    LReturnFlag_StandardScaler_Table_instance,\n    model_name=\"LReturnFlag_StandardScaler_Table\",\n    version_name=\"V1\",\n    options=options,\n    conda_dependencies=['numpy==1.23.5', 'pandas', 'scikit-learn==1.5.2', \"cloudpickle==2.2.1\"], # cloudpickle version should be greater than 2.0.0 in notebook as well\n    signatures={\"predict\": signature}\n)\n\nend = time.time() \nprint(end - start)\n\nreg.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "53347549-1f90-4bb0-97ce-7f9ae09257ba",
   "metadata": {
    "name": "Transform_with_Table_manymodel_md",
    "collapsed": false
   },
   "source": "### Transform with Table many-model custom model.\nWe can now use the registered model to transform new input data.\n\nWe join the source data to the fitted-models results to attach the model to each row based on the partitioning key.\n"
  },
  {
   "cell_type": "code",
   "id": "809b4a98-232b-4ef9-af67-18af252f75e6",
   "metadata": {
    "language": "python",
    "name": "Table_mm_source_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\nrf_transform_input_df =  (\n     rf_fit_sdf\n    .join(in_df, (rf_fit_sdf['PARTITION_COL'] == in_df[partitionkey]) )\n    .select(F.col('ORDERKEY'),  F.col('LINENUMBER'), # Primary Key Columns\n            F.col('L_RETURNFLAG'), # Partitioning Key\n            #F.col('L_QUANTITY_FLT'), F.col('L_EXTENDEDPRICE_FLT'), F.col('L_DISCOUNT_FLT'), F.col('L_TAX_FLT'), # Source Columns\n            F.col('NUMERIC_COL_ARRAY').cast(T.StringType()).as_('NUMERIC_COL_ARRAY'),  # Source Columns as Numeric Array\n            F.col('SS_MODEL_PKL').as_('SS_MODEL_PICKLE'),\n           )\n)\n\nrf_transform_input_df.sort('ORDERKEY', 'LINENUMBER', 'L_RETURNFLAG').show(2)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "583de31e-1536-4111-8515-9701d93f5e4b",
   "metadata": {
    "name": "Apply_transform_via_mm_table_model_md",
    "collapsed": false
   },
   "source": "We can now use the `mv_table` model instance we created to transform our data."
  },
  {
   "cell_type": "code",
   "id": "e93c79e9-ffcd-491e-8cbb-87f6aac79f4b",
   "metadata": {
    "language": "python",
    "name": "Apply_transform_via_mm_table_model",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "rf_transform_input_df_10000 = rf_transform_input_df.limit(10000)\n\ntransformed_mm_result = mv_table.run(rf_transform_input_df_10000, \n                                     partition_column=\"L_RETURNFLAG\") \\\n    .select('ORDERKEY_OUT', 'LINENUMBER_OUT', 'NUMERIC_COL_ARRAY_OUT', 'NUMERIC_COL_ARRAY_SS')\n\ntransformed_mm_result.show(5,100)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed8e2d6c-4d4e-403a-8992-c4cda589e81f",
   "metadata": {
    "name": "MODEL_FROM_MEMORY_MD",
    "collapsed": false
   },
   "source": "### MEMORY MANY-MODEL CUSTOM MODEL\nWe can also create a many-model Custom Model where the models are persisted in the model-registry via the custom model-class at registration time. In this case the models are stored in a Dict keyed on the partitioning key (`L_RETURNFLAG`) values, which we use to retrieve the relevant model for transformation for each partition key value. "
  },
  {
   "cell_type": "code",
   "id": "7ac76980-86f5-4f72-ac28-c1e89a7ff0a3",
   "metadata": {
    "language": "python",
    "name": "delete_prior_memory_model"
   },
   "outputs": [],
   "source": "try:\n    reg.delete_model(\"LReturnFlag_StandardScaler_Memory\")\nexcept:\n    print('Model does not exist in model registry')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b85f064e-61f7-45ad-b65f-1c56f46f1aa1",
   "metadata": {
    "language": "python",
    "name": "create_memory_model",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Convert Snowpark Dataframe containing models to model-dictionary\n\n# Create dict of models keyed on partitioning key\nmodels = {\n    row['PARTITION_COL']: pickle_decode_model(row['SS_MODEL_PICKLE'])\n    for _, row in \n      rf_fit_sdf.select(F.col('PARTITION_COL'), F.col('SS_MODEL_PKL').as_('SS_MODEL_PICKLE') ).to_pandas().iterrows()\n}\n# create model context \n# Note: Which is used in the __init__ class definition below\nmem_mc = custom_model.ModelContext(\n  models=models\n)\n\n# Create model class\nclass LReturnFlag_StandardScaler_Memory(custom_model.CustomModel):\n    def __init__(self, context: Optional[custom_model.ModelContext] = mem_mc) -> None: \n        super().__init__(context)\n        self.partition_id = None\n        #self.model = None\n        self._cache = {}  # Add a cache to store models\n\n    @custom_model.partitioned_inference_api\n    def predict(self, input: pd.DataFrame) -> pd.DataFrame:\n        current_flag = input['L_RETURNFLAG'].iloc[0]\n        model = self.context.model_ref(current_flag)\n \n        # Convert numeric arrays more efficiently using numpy\n        transformed_arrays = model.transform(\n            np.array([json.loads(x) for x in input['NUMERIC_COL_ARRAY']])\n        )\n\n        # Create result DataFrame more efficiently\n        result = pd.DataFrame({\n            'ORDERKEY_OUT': input['ORDERKEY'],\n            'LINENUMBER_OUT': input['LINENUMBER'],\n            'NUMERIC_COL_ARRAY': input['NUMERIC_COL_ARRAY'],\n            'NUMERIC_COL_ARRAY_SS': [json.dumps(row.tolist()) for row in transformed_arrays],\n        })\n\n        return result\n\n# Create model signatures \n# - Note we are passing in the entity-key columns for the data so that we can join these transformed features to other featureviews if needed\ninput_signature= [\n  FeatureSpec(dtype=DataType.INT64 , name='ORDERKEY', nullable=True),\n  FeatureSpec(dtype=DataType.INT64 , name='LINENUMBER', nullable=True),    \n  FeatureSpec(dtype=DataType.STRING , name=partitionkey, nullable=True),\n  FeatureSpec(dtype=DataType.STRING , name='NUMERIC_COL_ARRAY', nullable=True),\n]\n# - Note we are passing out the entity-key columns for the data so that we can join these transformed features to other featureviews if needed.\n# - We give them a different name to avoid a result set error with ambiguous names.\noutput_signature = [\n  FeatureSpec(dtype=DataType.INT64 , name='ORDERKEY_OUT', nullable=True),\n  FeatureSpec(dtype=DataType.INT64 , name='LINENUMBER_OUT', nullable=True),     \n  FeatureSpec(dtype=DataType.STRING , name='NUMERIC_COL_ARRAY_OUT', nullable=True), \n  FeatureSpec(dtype=DataType.STRING , name='NUMERIC_COL_ARRAY_SS', nullable=True),     \n]\n\nsignature = ModelSignature(\n    inputs=input_signature,\n    outputs=output_signature,\n)\n\n# Create instance of class\nLReturnFlag_StandardScaler_Memory_instance = LReturnFlag_StandardScaler_Memory()\n\nstart = time.time() \n\n# Define Model Options\noptions = {\n    \"function_type\": \"TABLE_FUNCTION\",\n    \"relax_version\": False\n}\n\nmv_memory = reg.log_model(\n    LReturnFlag_StandardScaler_Memory_instance,\n    model_name=\"LReturnFlag_StandardScaler_Memory\",\n    version_name=\"V1\",\n    options=options,\n    conda_dependencies=['numpy==1.23.5', 'pandas', 'scikit-learn==1.5.2', \"cloudpickle==2.2.1\"], # cloudpickle version should be greater than 2.0.0 in notebook as well\n    signatures={\"predict\": signature}\n)\n\nend = time.time() \nprint(end - start)\n\nreg.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81f70b0a-3507-4f3d-aa88-a5f7ded7b0d2",
   "metadata": {
    "name": "local_memory_mm_test_md",
    "collapsed": false
   },
   "source": "We can test the class function locally on a pandas dataframe"
  },
  {
   "cell_type": "code",
   "id": "7bc93cde-0014-4421-bcdb-1c86ed0f035e",
   "metadata": {
    "language": "python",
    "name": "local_memory_mm_test",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Convert Snowpark Dataframe of source data to be standard-scaled to Pandas\nin_mem_10_pdf = in_df.limit(10).select(F.col('ORDERKEY'),F.col('LINENUMBER'),F.col('L_RETURNFLAG'),F.col('NUMERIC_COL_ARRAY'),).to_pandas()\nin_mem_10_pdf\n# Test predict method of class works on Pandas Dataframe, which is what the data gets coverted to to execute in database.\nLReturnFlag_StandardScaler_Memory_instance.predict(predict_input_pdf)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e5968102-9b1c-42ff-b534-7e34551e5a6f",
   "metadata": {
    "name": "Transform_with_Memory_manymodel_md",
    "collapsed": false
   },
   "source": "### Transform\nWe can now apply the in-memory model transformation.\n\nFirst we create some source data for transformation.  __Note:__ there is no need for the models to be joined/attached to the rows in this case as they are persisted within the model."
  },
  {
   "cell_type": "code",
   "id": "4b97f13c-f319-4d27-9113-ada304d9b626",
   "metadata": {
    "language": "python",
    "name": "Memory_mm_source_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# TRANSFORM \n\nrf_transform_mem_input_df =  (\n     in_df.select(F.col('ORDERKEY'),  F.col('LINENUMBER'), # Primary Key Columns\n                  F.col('L_RETURNFLAG'), # Partitioning Key\n                  F.col('NUMERIC_COL_ARRAY').cast(T.StringType()).as_('NUMERIC_COL_ARRAY'),  # Source Columns as Numeric Array\n           )\n)\n\nrf_transform_mem_input_df.sort('ORDERKEY', 'LINENUMBER', 'L_RETURNFLAG').show(3)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e8781fb-6875-4191-84ef-83ba436166f8",
   "metadata": {
    "name": "Apply_transform_via_mm_memory_model_md",
    "collapsed": false
   },
   "source": "We can now use the `mv_memory` model instance we created to transform our data."
  },
  {
   "cell_type": "code",
   "id": "9accab1d-98c7-4e6d-be41-1f9a7618cdc8",
   "metadata": {
    "language": "python",
    "name": "Apply_transform_via_mm_memory_model",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "rf_transform_mem_input_df_10000 = rf_transform_input_df_10000.limit(10000)\n\ntransformed_mm_mem_result = mv_memory.run(rf_transform_mem_input_df_10000, partition_column=\"L_RETURNFLAG\") \\\n    .select('ORDERKEY_OUT', 'LINENUMBER_OUT', 'NUMERIC_COL_ARRAY_OUT', 'NUMERIC_COL_ARRAY_SS')\n\ntransformed_mm_mem_result.show(5,100)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "abd91dc4-aac0-4b3c-9e77-4f9de315b3fd",
   "metadata": {
    "name": "ROLLING_WINDOWS",
    "collapsed": false
   },
   "source": "## ROLLING WINDOWS"
  },
  {
   "cell_type": "code",
   "id": "f6ae8a20-65d4-4175-b212-4a837a4f1b34",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "## Mock up some device data\n\nsession.sql(f'''\ncreate or replace table {sess_db}.{org}.device_events  as\nwith \ndevices as (\nSELECT seq4() device_id, \n  FROM TABLE(GENERATOR(ROWCOUNT => 10000)) v ) -- adjust the literal for the number of devices needed\n,\ntimepoints as (\nSELECT ROW_NUMBER() OVER (PARTITION BY null ORDER BY null) AS hrs, \n  FROM TABLE(GENERATOR(ROWCOUNT => (24*365*2))) v )  -- adjust the literal calculation for the number of hours needed\n  \n-- Join two CTES with product join to create a device reading for every hour.  \nSELECT device_id,\n  dateadd('hr',  hrs*-1 , date_trunc('hr',current_timestamp()) ) event_ts,\n  uniform(1, 1000, RANDOM(12)) device_reading\n  FROM devices, timepoints v \n  ORDER BY 1, 2 desc;\n  \n''').collect()  \n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7f47875-5b6d-43bf-b866-c614ca9918a0",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "device_events_sdf = session.table(f'''{sess_db}.{org}.DEVICE_EVENTS''')\ndevice_events_sdf.sort(F.col('DEVICE_ID'),F.col('EVENT_TS').desc()).limit(100).collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c53989f-f173-4d3f-afc0-9d0d8b044447",
   "metadata": {
    "language": "python",
    "name": "cell32",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#fs.delete_feature_view('FV_DEVICE_RAW','V1')\n#fs.delete_feature_view('FV_DEVICE_MVAVG','V1')\n#fs.delete_feature_view('FV_DEVICE_MVAVG_ARRAY','V1')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "87992907-320f-4728-9683-9d141a902cc0",
   "metadata": {
    "language": "python",
    "name": "cell25",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Device Entity Definition \ndevice_entity = Entity(\n    name=\"DEVICE\",\n    join_keys=[\"DEVICE_ID\"],\n    desc=\"Device entity\"\n    )\nfs.register_entity(device_entity)\n\n# RAW DEVICE FEATUREVIEW\n\n# Create Raw Device FeatureView in Feature Store\ndevice_raw_fv = FeatureView(\n    name = f\"FV_DEVICE_RAW\",\n    entities = [device_entity],\n    feature_df = device_events_sdf,\n    timestamp_col = 'EVENT_TS',\n    # refresh_freq = '1 hours',\n    desc = f\"Device raw features feature view\"\n)\n# Register the Customer FeatureView in the schema, and add the python featureview to our list\ndevice_raw_fv_v1 = fs.register_feature_view(\n    feature_view = device_raw_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)\n\ndevice_raw_fv_v1.feature_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "06e2f1e5-3efd-4254-86e9-87be3fa4ce21",
   "metadata": {
    "language": "python",
    "name": "cell31",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "device_mvavg_sdf = device_raw_fv_v1.feature_df.analytics.moving_agg(\n    aggs={\"DEVICE_READING\": [\"AVG\"]}, # Only using AVG here but you can add additional aggregation functions if needed\n    window_sizes=[4, 8 , 16], # Using 4 here as it makes the results easier to check, but you can substitute/add any number of time periods here\n    order_by=[\"EVENT_TS\"],\n    group_by=[\"DEVICE_ID\"],\n)\n\n# DEVICE MOVING_AVERAGE FEATURES FEATUREVIEW\n\n# Create Raw Device FeatureView in Feature Store\ndevice_mvavg_fv = FeatureView(\n    name = f\"FV_DEVICE_MVAVG\",\n    entities = [device_entity],\n    feature_df = device_mvavg_sdf,\n    timestamp_col = 'EVENT_TS',\n    refresh_freq = '1 hours',\n    desc = f\"Device moving average features feature view\"\n)\n# Register the Customer FeatureView in the schema, and add the python featureview to our list\ndevice_mvavg_fv_v1 = fs.register_feature_view(\n    feature_view = device_mvavg_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)\n\ndevice_mvavg_fv_v1.feature_df.sort(F.col('DEVICE_ID'),F.col('EVENT_TS').desc()).show(10)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "291330cc-3da4-445e-965f-d993bff8a9e6",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Window\n\n# Gather the four readings from four prior rows .\nwindow4 = Window.partition_by(\"DEVICE_ID\").order_by(\"EVENT_TS\").rows_between(-4, -1)\n# Gather then moving average readings into arrays based on window4\nmvavg_array_sdf = device_mvavg_fv_v1.feature_df.select(\n                    F.col(\"DEVICE_ID\"),\n                    F.col(\"EVENT_TS\"),\n                    F.array_agg(\"DEVICE_READING_AVG_4\", False).over(window4).alias(\"DEVICE_READING_AVG_4_ARRAY\")\n                   )\n\n# DEVICE MOVING_AVERAGE ARRAY FEATURES FEATUREVIEW\n\n# Create Raw Device FeatureView in Feature Store\ndevice_mvavg_array_fv = FeatureView(\n    name = f\"FV_DEVICE_MVAVG_ARRAY\",\n    entities = [device_entity],\n    feature_df = mvavg_array_sdf,\n    timestamp_col = 'EVENT_TS',\n    refresh_freq = '1 hours', # I would use a VIEW here to compute these on the fly when needed as it will eliminate a lot of data-duplication across the arrays.\n    desc = f\"Device moving average features as array feature view\"\n)\n# Register the Customer FeatureView in the schema, and add the python featureview to our list\ndevice_mvavg_array_fv_v1 = fs.register_feature_view(\n    feature_view = device_mvavg_array_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)\n\ndevice_mvavg_array_fv_v1.feature_df.sort(F.col('DEVICE_ID'),F.col('EVENT_TS').desc()).show(10)\n\n# UserWarning: Your pipeline won't be incrementally refreshed due to: \"This dynamic table contains a complex query. \n# Refresh mode has been set to FULL. If you wish to override this automatic choice, please re-create the dynamic table \n# and specify REFRESH_MODE=INCREMENTAL. For best results, we recommend reading \n# https://docs.snowflake.com/user-guide/dynamic-table-performance-guide \n# before setting the refresh mode to INCREMENTAL.\".",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d00f3eaa-8d42-476d-b11b-2c0154141825",
   "metadata": {
    "name": "SECOND_FEATURE_STORE",
    "collapsed": false
   },
   "source": "## SECOND FEATURE STORE\n\nIn this section we show how a we can reference and work with objects in one feature-store from another and how we can create and retrieve objects that span more than one Feature Store."
  },
  {
   "cell_type": "code",
   "id": "7803e291-aff8-486a-8a64-f606d02ea63b",
   "metadata": {
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": "# Source Data Database and Schema\nsrc_database = 'SNOWFLAKE_SAMPLE_DATA'\nsrc_schema = 'TPCH_SF1' # <-- Modify this if you want to test with one of the larger data scale-factors. e.g. TPCH_SF1, TPCH_SF10, TPCH_SF100, TPCH_SF1000\n\n# Database to use to create Schemas\nsess_db = 'SIMON' # The database within which we will create our Feature Store (schema), and data-source schema.\n\norg2 = f'{org}_2'                          # Name for working Schema and used to derive Feature Store Name\nfs_name_2 = f\"{org2}_FEATURE_STORE\"      # Feature Store Name.  This will create a Schema to contain our Feature Store database objects\nmr_name_2 = f\"{org2}_MODEL_REGISTRY\"     # Model-Registry Name.\nnum_spine_rows = 10                   # Maximum number of rows to use when sampling source data for Spine Entity Keys.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18ec2ca6-98ac-4679-834f-0e0b78dc5271",
   "metadata": {
    "language": "python",
    "name": "cell33",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Name of the schema where we will persist our generated training datasets\nsession.sql(f''' Create schema if not exists {sess_db}.{org}''').collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "030a6e84-59e4-4a26-8d57-e71596db6d2f",
   "metadata": {
    "name": "cell41",
    "collapsed": false
   },
   "source": "Create a second Feature Store for demonstration purposes."
  },
  {
   "cell_type": "code",
   "id": "7029f509-e33c-4f2f-be0f-5b3dd0eec0eb",
   "metadata": {
    "language": "python",
    "name": "cell40",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "fs_2 =  FeatureStore(\n        session=session,\n        database=sess_db,\n        name=fs_name_2,\n        default_warehouse=\"SIMON_XS\",\n        creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4a06cbf7-97e7-4593-bbb8-ee0543d06107",
   "metadata": {
    "name": "cell44",
    "collapsed": false
   },
   "source": "__Recreate Entities__"
  },
  {
   "cell_type": "code",
   "id": "13f44a59-60f4-4bef-8f2a-7b75e733f255",
   "metadata": {
    "language": "python",
    "name": "cell34",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# CUSTOMER ENTITY DEFINITION \n\n## Either using original source-code definition for our first Feature Store\n# customer_entity = Entity(\n#     name=\"CUSTOMER\",\n#     join_keys=[\"CUSTKEY\"],\n#     desc=\"Customer entity\"\n#     )\n\n## Or we can retrieve it from our first Feature Store\ncustomer_entity = fs.get_entity('CUSTOMER')\n\n## We can retireve it from the first feature-store and use it directly to register in the 2nd feature-store\nfs_2.register_entity(fs.get_entity('CUSTOMER'))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b71259e-3848-492d-bbe7-e8f48deb43eb",
   "metadata": {
    "language": "python",
    "name": "cell35",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "fs_2.list_entities()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e41dc1cf-a8a8-4add-aaf4-d75ffad51c23",
   "metadata": {
    "name": "cell151",
    "collapsed": false
   },
   "source": "__Recreate FeatureViews__\n\nThere are two options.  \n\n1) We can recreate the object as-is in the new feature store, using it's original definition.  In this case if the Feature View in the original Feature Store is a Dynamic Table, we will be duplicating compute effort and storage for the Dynamic Table.\n2) We can create the FeatureView and reference the underlying database object from the original Feature View, sharing the compute and storage from the first Feature Store\n"
  },
  {
   "cell_type": "code",
   "id": "52507e62-7a2e-438e-aec3-4210fcbbe011",
   "metadata": {
    "language": "python",
    "name": "cell110"
   },
   "outputs": [],
   "source": "# CUSTOMER FEATUREVIEW\n\n## Either using original source-code definition\n# Create Customer FeatureView in Feature Store\n# customer_fv = FeatureView(\n#     name = f\"FV_CUSTOMER\",\n#     entities = [customer_entity],\n#     feature_df = customer_nosk_sdf,\n#     desc = f\"Customer feature view\"\n# )\n\ncustomer_fv_v1 = fs.get_feature_view(\"FV_CUSTOMER\", 'V1')\n\n## Or reconstruct from FeatureView attributes\ncustomer_fv = FeatureView(\n    name = customer_fv_v1.name,\n    entities = customer_fv_v1.entities,\n    feature_df = customer_fv_v1.feature_df,\n    timestamp_col = customer_fv_v1.timestamp_col,\n    refresh_freq= customer_fv_v1.refresh_freq,\n    desc = customer_fv_v1.desc,\n    warehouse =  customer_fv_v1.warehouse,\n    initialize = customer_fv_v1.initialize,\n    refresh_mode = customer_fv_v1.refresh_mode,\n    cluster_by = customer_fv_v1.cluster_by\n).attach_feature_desc(customer_fv_v1.feature_descs)\ncustomer_fv\n\nfs2_customer_fv_v1 = fs_2.register_feature_view(\n    feature_view = customer_fv,    # feature view created above, could also use external_fv\n    version = customer_fv_v1.version,\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d5a8562-8103-46e3-bd53-39661dfd5e9f",
   "metadata": {
    "language": "python",
    "name": "cell103",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "customer_fv = FeatureView(\n    name = customer_fv_v1.name,\n    entities = customer_fv_v1.entities,\n    feature_df = session.table(customer_fv_v1.fully_qualified_name()),\n    timestamp_col = customer_fv_v1.timestamp_col,\n    refresh_freq= customer_fv_v1.refresh_freq,\n    desc = customer_fv_v1.desc,\n    warehouse =  customer_fv_v1.warehouse,\n    initialize = customer_fv_v1.initialize,\n    refresh_mode = customer_fv_v1.refresh_mode,\n    cluster_by = customer_fv_v1.cluster_by\n).attach_feature_desc(customer_fv_v1.feature_descs)\ncustomer_fv\n\nfs2_customer_fv_v1 = fs_2.register_feature_view(\n    feature_view = customer_fv,    # feature view created above, could also use external_fv\n    version = customer_fv_v1.version,\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8014ca8-91a1-4650-a5e8-573bb0dfcd21",
   "metadata": {
    "language": "python",
    "name": "cell36",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "fs_2.list_feature_views()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "efbcb0a2-839e-4938-8a18-305485d509d7",
   "metadata": {
    "language": "python",
    "name": "cell150"
   },
   "outputs": [],
   "source": "print(fs2_customer_fv_v1.feature_df.queries['queries'][0])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b632987-169a-4746-8e58-eb28a712c794",
   "metadata": {
    "language": "python",
    "name": "cell38",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "customer_spine_tbl = [sess_db,org,'CUSTOMER_SPINE']\n\ncustomer_sdf.select('CUSTKEY') \\\n    .distinct() \\\n    .sample(n=num_spine_rows) \\\n    .write.saveAsTable(customer_spine_tbl,mode = 'overwrite', table_type = 'temp')\n\ncustomer_spine_df = session.table(customer_spine_tbl)\n    \ncustomer_spine_df.sort('CUSTKEY').show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd736162-2ed4-4fe5-8521-5d0fdf056ad6",
   "metadata": {
    "language": "python",
    "name": "cell39"
   },
   "outputs": [],
   "source": "fs.list_feature_views()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "967af4f9-7a6d-409a-9833-8b59284e2b30",
   "metadata": {
    "language": "python",
    "name": "cell37",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "fs_2.generate_training_set(customer_spine_df, \n                           [fs.get_feature_view('FV_CUSTOMER', 'V1'),    # FeatureView in Feature Store 1\n                           fs_2.get_feature_view('FV_CUSTOMER', 'V1')]   # FeatureView in Feature Store 2\n                          ).show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5b1fd91-78a0-4895-bfac-09f3183a1b01",
   "metadata": {
    "language": "sql",
    "name": "cell45"
   },
   "outputs": [],
   "source": "show tags in database {{sess_db}};",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e030abf9-bf05-43d7-b1d3-9ff431f68c4a",
   "metadata": {
    "language": "python",
    "name": "cell46"
   },
   "outputs": [],
   "source": "fs_database = 'SIMON'\nsession.sql(f'''show tags in database {fs_database};''').collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "04aa9a03-e874-400d-9ad5-995f2b93c569",
   "metadata": {
    "language": "python",
    "name": "cell42",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def fs_get_database_entities(fs_database):\n    session.sql(f'''show tags in database {fs_database};''').collect()\n\n    result_df = session.sql(f'''\nselect \n       \"database_name\",\n       \"schema_name\",\n       split_part(\"name\",'_', 5) \"entity_name\",\n       \"allowed_values\" \"entity_keys\",\n       \"comment\"\n  from table(result_scan(last_query_id()))\n where startswith(\"name\", 'SNOWML_FEATURE_STORE_ENTITY_')\n                 ''')\n\n    return result_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd33bfd0-a6c7-4ab8-9915-b83da4d1d064",
   "metadata": {
    "language": "python",
    "name": "cell50",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "fs_get_database_entities('SIMON').to_pandas().to_markdown()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0248ccf-222e-4c65-bf37-241219c114ea",
   "metadata": {
    "name": "PARAMETER_TABLE",
    "collapsed": false
   },
   "source": "### Using Data Driven approach with parameter tables\nWe can use a parameter table to store small sets of values that we want to use within FeatureViews to avoid hard-coding literal values in Dataframes/SQL use as a source to FeatureViews.\n\nAs a trivial example lets say that we need to create a FeatureView for priority order categories but we may want the flexibility to change these categories.   We will use the O_ORDERPRIORITY column in ORDERS to illustrate."
  },
  {
   "cell_type": "code",
   "id": "ee90a871-99eb-45f5-b7a4-b5cdf7baa0f2",
   "metadata": {
    "language": "python",
    "name": "cell75"
   },
   "outputs": [],
   "source": "orders_nosk_sdf.select('O_ORDERPRIORITY').distinct().sort('O_ORDERPRIORITY').show(20)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf23461e-3101-4ade-b3de-ccda18abca17",
   "metadata": {
    "language": "python",
    "name": "cell52",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\ncreate or replace table {sess_db}.{fs_name}.fv_parameters\n (\n  fvname    varchar,\n  fvver     varchar,\n  parameter_name varchar,\n  parameter_value variant\n ) \n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f6d5b57-912f-4961-8347-482659b56fb2",
   "metadata": {
    "name": "cell51",
    "collapsed": false
   },
   "source": "Instead of hardcoding the categories within the SQL/Snowpark dataframe that is used as the source for the FeatureView, we create a parameter table containing those literal values.  We will join to this parameter table from our FeatureView dataframe definition to apply these rather than hard-coding them as literals. This table can be updated when needed, and the FeatureView will automatically refresh to apply the changes.  We don't need to recreate the FeatureView with a new definition."
  },
  {
   "cell_type": "code",
   "id": "5e3c9be1-7b29-4c7c-b347-25858483de33",
   "metadata": {
    "language": "python",
    "name": "cell54"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nInsert into  {sess_db}.{fs_name}.fv_parameters\nselect \n  fvname, fvver, parameter_name,  parse_json(parameter_value) parameter_value\nfrom (values\n  ('FV_ORDERS_EXPEDITED', 'V1', 'EXPEDITE_CATEGORIES', '[\"1-URGENT\", \"2-HIGH\"]')\n ) as oe (fvname, fvver, parameter_name, parameter_value)\n\"\"\").show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0338cf62-e959-4d3f-9f43-a9c03d16f5ef",
   "metadata": {
    "language": "python",
    "name": "cell69"
   },
   "outputs": [],
   "source": "fv_parameters.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63ba3de7-6879-40e9-be28-b3d0bff23438",
   "metadata": {
    "language": "python",
    "name": "cell57",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "f'{sess_db}.{fs_name}.fv_parameters'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4da753b5-7c14-4a64-9a49-d2c543380a2d",
   "metadata": {
    "language": "python",
    "name": "cell56",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ORDER\norders_table = 'ORDERS'\norders_sdf = session.table(f'{src_database}.{src_schema}.{orders_table}') \\\n                .with_columns(['ORDERKEY','CUSTKEY'],[F.col('O_ORDERKEY'),F.col('O_CUSTKEY')]) \\\n                .drop('O_ORDERKEY', 'O_CUSTKEY')\norders_nosk_sdf = orders_sdf.drop('CUSTKEY')\n\nfv_parameters = session.table(f'{sess_db}.{fs_name}.fv_parameters')\n\nfv_parameters_oe_flat =  (fv_parameters.filter( (F.col('FVNAME') == 'FV_ORDERS_EXPEDITED') &\n                                               (F.col('FVVER') == 'V1') &\n                                               (F.col('PARAMETER_NAME') == 'EXPEDITE_CATEGORIES')\n                                             )\n                                       .flatten(input = 'PARAMETER_VALUE')\n                                       .select(F.col('VALUE').as_('EXPEDITE_CATEGORY'))\n                         )\nfv_parameters_oe_flat.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e3f0dff2-44bc-499f-8af6-3f9755483088",
   "metadata": {
    "language": "python",
    "name": "cell98"
   },
   "outputs": [],
   "source": "session.sql(\"\"\"\nselect * \nfrom SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS \nwhere O_ORDERPRIORITY in ('1-URGENT', '2-HIGH')\n\"\"\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "913dfc1b-3999-4dcb-8652-eacf79488031",
   "metadata": {
    "language": "python",
    "name": "cell97"
   },
   "outputs": [],
   "source": "session.sql(\"\"\"\nwith parms as (\n  select PARAMETER_VALUE \n  from SIMON.TPCHSF1_FEATURE_STORE.fv_parameters\n  where FVNAME = 'FV_ORDERS_EXPEDITED'\n    and FVVER = 'V1'\n    and PARAMETER_NAME = 'EXPEDITE_CATEGORIES'\n)\nselect * exclude PARAMETER_VALUE\nfrom SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS, parms\nwhere array_contains(O_ORDERPRIORITY::variant, PARAMETER_VALUE)\n\"\"\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0049d6d1-431c-449e-99c2-91737751f977",
   "metadata": {
    "language": "sql",
    "name": "cell107"
   },
   "outputs": [],
   "source": "select min(O_Orderdate), max(O_Orderdate) from SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7d8f8e03-5e28-4497-ba5b-14388c835b1f",
   "metadata": {
    "language": "python",
    "name": "cell55",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "fv_parameters = session.table(f'{sess_db}.{fs_name}.fv_parameters')\n\nfv_parameters_oe_flat =  (session.table('SIMON.TPCHSF1_FEATURE_STORE.FV_PARAMETERS')\n                                 .filter( (F.col('FVNAME') == 'FV_ORDERS_EXPEDITED') &\n                                               (F.col('FVVER') == 'V1') &\n                                               (F.col('PARAMETER_NAME') == 'EXPEDITE_CATEGORIES')\n                                             )\n                                       .flatten(input = 'PARAMETER_VALUE')\n                                       .select(F.col('VALUE').as_('EXPEDITE_CATEGORY'))\n                         )\nfv_parameters_oe_flat.show()\n\n# ORDER\norders_table = 'ORDERS'\norders_sdf = (session.table(f'SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS') \n                .with_columns(['ORDERKEY'],[F.col('O_ORDERKEY')]) \n                .drop('O_ORDERKEY') \n             )\n\norders_expedited_sdf = (orders_sdf.join(fv_parameters_oe_flat, \n                                             (orders_sdf[\"O_ORDERPRIORITY\"] == fv_parameters_oe_flat[\"EXPEDITE_CATEGORY\"]))\n                                       .drop(\"EXPEDITE_CATEGORY\"))\n\norders_expedited_sdf.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "810c8b06-429d-47e4-83e9-edb8ea56273a",
   "metadata": {
    "language": "python",
    "name": "cell74",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "orders_expedited_sdf = (orders_sdf.join(fv_parameters_oe_flat, \n                                             (orders_sdf[\"O_ORDERPRIORITY\"] == fv_parameters_oe_flat[\"EXPEDITE_CATEGORY\"]))\n                                       .drop(\"EXPEDITE_CATEGORY\"))\n\norders_expedited_sdf.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b261e7bd-2474-4c68-b80a-0fdbde68c5fb",
   "metadata": {
    "language": "python",
    "name": "cell53",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ORDERS FEATUREVIEW\norders_features = orders_sdf.drop(\"ORDERKEY\", \"CUSTKEY\").columns\n\n# Create Order FeatureView in Feature Store\norders_expedited_fv = FeatureView(\n    name = f\"FV_ORDERS_EXPEDITED\",\n    entities = [order_entity],\n    feature_df = orders_expedited_sdf,\n    # refresh_freq = '1 minutes',\n    desc = f\"Orders Expedited feature view\"\n)\n# Register the Order FeatureView in the schema, and add the python featureview to our list\norders_expedited_fv_v1 = fs.register_feature_view(\n    feature_view = orders_expedited_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fad0602a-b62d-4825-8d04-42cf01c3bd13",
   "metadata": {
    "name": "cell131",
    "collapsed": false
   },
   "source": "### Data Retention Dates\n\nWe can also use the same principle to apply data-retention dates for the FeatureView which will allow us to adjust the data retention period without having to recreate the FeatureView."
  },
  {
   "cell_type": "code",
   "id": "f5187246-1905-4530-9fa0-3b159927a00f",
   "metadata": {
    "language": "python",
    "name": "cell108"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nInsert into  {sess_db}.{fs_name}.fv_parameters\nselect \n  fvname, fvver, parameter_name,  parse_json(parameter_value) parameter_value\nfrom (values\n  ('FV_ORDERS_EXPEDITED', 'V1', 'ORDER_RETENTION', '[1095]') -- Three years!\n ) as oe (fvname, fvver, parameter_name, parameter_value)\n\"\"\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea2e934b-0638-4bf6-89fe-0d7c1d75f513",
   "metadata": {
    "language": "python",
    "name": "cell109"
   },
   "outputs": [],
   "source": "session.sql(\"\"\"\nwith parms as (\n  select PARAMETER_VALUE \n  from SIMON.TPCHSF1_FEATURE_STORE.fv_parameters\n  where FVNAME = 'FV_ORDERS_EXPEDITED'\n    and FVVER = 'V1'\n    and PARAMETER_NAME = 'ORDER_RETENTION'\n),\nmax_od as (\nselect max(O_ORDERDATE) maxod\nfrom SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\n)\nselect min(O_ORDERDATE)\nfrom SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS, parms, max_od\nwhere dateadd('DAYS', PARAMETER_VALUE[0]::integer, O_ORDERDATE ) >= max_od.maxod\n\"\"\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4c2f752-ad59-4674-87a7-6e39bd6a0dc6",
   "metadata": {
    "language": "sql",
    "name": "cell93",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "drop view FV_ORDERS_EXPEDITED$V1;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "439be1c4-bc97-4c21-ac04-3d157e93d575",
   "metadata": {
    "name": "OBJECT_WRAP_FEATURES",
    "collapsed": false
   },
   "source": "### Object Wrap Features"
  },
  {
   "cell_type": "code",
   "id": "af69b654-b228-46e2-92e3-72684b0f3957",
   "metadata": {
    "language": "python",
    "name": "cell96"
   },
   "outputs": [],
   "source": "# Using SQL Expression\ndef object_wrap_features(fq_tbl, non_feature_cols):\n    \n   sdf = session.table(fq_tbl)\n   if isinstance(non_feature_cols, str): \n      try:  \n         non_feature_cols = json.loads(non_feature_cols)\n      except:\n         print('Feature Columns argument is not valid json :' , non_feature_cols) \n          \n   # feature_cols = [F.lit(x) for x in sdf.columns if x not in non_feature_cols]\n   \n   non_feature_cols_str =  ','.join(col for col in non_feature_cols)\n    \n   object_sdf = (sdf.with_column('F_OBJECT', \n                                  F.object_construct( F.sql_expr(f'''* exclude ({non_feature_cols_str})''')) )\n                    .select(non_feature_cols + ['F_OBJECT']) )\n   return object_sdf\n\n# Using list comprehension and pure Snowpark code \ndef object_wrap_features(fq_tbl, non_feature_cols):\n    import snowflake.snowpark.functions as F\n    sdf = session.table(fq_tbl).limit(10)\n    if isinstance(non_feature_cols, str):\n        try:\n            non_feature_cols = json.loads(non_feature_cols)\n        except:\n            print('Feature Columns argument is not valid json :', non_feature_cols)\n\n            # Get feature columns (columns not in non_feature_cols)\n    feature_cols = [col for col in sdf.columns if col not in non_feature_cols]\n\n    # Create object_construct with explicit column references\n    object_sdf = (sdf.with_column('F_OBJECT',\n                                  F.object_construct(*[item for name in feature_cols for item in (F.lit(name), F.col(name))]))\n                  .select(non_feature_cols + ['F_OBJECT'])\n                 )\n    return object_sdf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc8c1326-271c-470b-b6c4-95bfa6666c59",
   "metadata": {
    "language": "python",
    "name": "cell132"
   },
   "outputs": [],
   "source": "non_feature_cols = lineitem_fv_v1.list_columns().filter(F.col('CATEGORY') != 'FEATURE').select(F.col('NAME')).to_pandas()['NAME'].to_list()\nall_cols",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e3f12c4-96e3-4f84-8abd-a4dd0537df4d",
   "metadata": {
    "language": "python",
    "name": "cell118",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\n\nfcols = list(map(str, lineitem_fv_v1.feature_names))\nnon_fcols = list(set(all_cols) - set(fcols))\nnon_fcols",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e2ec312-03df-4bb9-bd50-c8db6b4008cf",
   "metadata": {
    "language": "python",
    "name": "cell134",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\nresult_df = object_wrap_features(lineitem_fv_v1.fully_qualified_name(),  json.dumps(non_fcols)) \nresult_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "39e85357-eabf-45a0-acc5-9644dfba5570",
   "metadata": {
    "language": "python",
    "name": "cell100",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "result_df = object_wrap_features('FEATURE_STORE_ONLINE_TEST_3.SOURCE_DATA.EVENT', '[\"EVENT_KEY\", \"ENTITY_KEY\", \"CTD_TS\", \"LST_UPD_TS\"]' )\nresult_df.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "88e896af-8d15-4ce0-823b-61028b7c45a4",
   "metadata": {
    "language": "python",
    "name": "cell101",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "result_df = object_wrap_features('FEATURE_STORE_ONLINE_TEST_3.SOURCE_DATA.EVENT', [\"EVENT_KEY\", \"ENTITY_KEY\", \"CTD_TS\", \"LST_UPD_TS\"] )\n\nresult_df.select('ENTITY_KEY', 'CTD_TS', 'LST_UPD_TS', 'F_OBJECT' ).show(10)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "360548ef-3e08-4960-80cf-7cfcf60bd16a",
   "metadata": {
    "language": "python",
    "name": "cell102",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "original_list = [\"FF_00000\", \"FF_00001\", \"FF_00002\", \"FF_0003\",\"FF_0004\",\"FF_0005\"]\nnew_list = [item for name in original_list for item in (F.lit(name), F.col(name))]\nprint(new_list)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "120d05e2-4842-4b71-8ba4-89bfd2074f41",
   "metadata": {
    "language": "python",
    "name": "cell104"
   },
   "outputs": [],
   "source": "result_df = object_wrap_features('SIMON.TPCHSF1.ORDERS', '[\"O_ORDERKEY\", \"O_ORDERDATE\"]' )\n\nresult_df.to_pandas().to_markdown()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1e7d9b1-bc60-48ba-b82d-f23158fae61f",
   "metadata": {
    "language": "python",
    "name": "cell127"
   },
   "outputs": [],
   "source": "result_df = object_wrap_features(orders_fv_v1.fully_qualified_name(), '[\"EVENT_KEY\", \"ENTITY_KEY\", \"CTD_TS\", \"LST_UPD_TS\"]' )\nresult_df.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d1aae93d-f6fe-400f-a099-fe97fa485f4f",
   "metadata": {
    "name": "FORMAT_SQL",
    "collapsed": false
   },
   "source": "### Format Dataframe generated SQL.\n\nThe SQL generated from Snowpark dataframe operations is not formatted to make it easy for a human-readability.  As Snowflake FeatureStore converts these DataFrames into database objects (Dynamic Tables or Views), passing the query through a formatting tool can make the objects easier to read/review directly.\n\nThe following function can be used to format the SQL nicely for readability, and if preferred can convert sub-queries to Common Table Expressions."
  },
  {
   "cell_type": "code",
   "id": "2921bcc0-9e4e-4355-8ea2-dd73ca25cd2b",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "!pip install sqlglot",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "58628815-476c-41e9-bc09-7e0cff779617",
   "metadata": {
    "language": "python",
    "name": "cell113"
   },
   "outputs": [],
   "source": "import sqlglot\nimport sqlglot.optimizer.optimizer\n\ndef formatSQL (df_in, subq_to_cte = False, print_sql = True):\n  \"\"\"\n  Prettify the given SQL from the input dataframe to nest/indent appropriately.\n  Optionally replace subqueries with CTEs.\n  query_in    : The raw SQL query to be prettified\n  subq_to_cte : When TRUE convert nested sub-queries to CTEs\n\n  return: a dataframe with the formatted SQL\n  \"\"\"\n  \n  query_in = df_in.queries['queries'][0]  \n    \n  expression = sqlglot.parse_one(query_in)    \n    \n  if subq_to_cte:\n      query_in = (sqlglot.optimizer.\n                     optimizer.eliminate_subqueries(expression).sql() )\n  pretty_sql = sqlglot.transpile(query_in, \n                                 read='snowflake', pretty=True)[0]  \n    \n  if print_sql:\n      print(query_in)\n      print(pretty_sql)      \n  return session.sql(pretty_sql)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "54104b49-a7cd-4b49-a302-1d925977b23c",
   "metadata": {
    "language": "python",
    "name": "cell133",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "print(orders_nosk_sdf.queries['queries'][0])\nprint(' ')\n\nformatSQL(orders_nosk_sdf)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cfa9e6ae-4eb7-4144-99eb-3029cb0d0c34",
   "metadata": {
    "name": "cell114"
   },
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "e474bcea-4480-4726-9d38-b3b08618cc82",
   "metadata": {
    "name": "DATA_METRIC_FUNCTIONS",
    "collapsed": false
   },
   "source": "### Monitoring Feature Quality with DMF's\n\nSnowflake has built in data quality monitoring functionality, that can also easily be applied to a Feature Store.  Below we demonstrate how these can be used to set up and record changes in our features statistics over time.\n\nFirstly we will create a copy of the Orders table from snowflake_sample_data, as that is a data-share, which does not support data-metric functions being applied to them directly. We also want to be able to add records to the table so we can demonstrate the DMF functions triggering and capturing statistical changes."
  },
  {
   "cell_type": "code",
   "id": "611704b6-d367-4d6f-bc68-7f28d0fc58c6",
   "metadata": {
    "language": "python",
    "name": "cell152"
   },
   "outputs": [],
   "source": "# session.sql(f''' Drop schema if exists {org}_FEATURE_STORE''').collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46da1d7d-ca92-4f04-9775-44753000323d",
   "metadata": {
    "language": "sql",
    "name": "cell111",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "create table {{sess_db}}.{{org}}.ORDERS if not exists as\nselect *\nfrom SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\norder by O_ORDERDATE\n;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "810df6cc-071b-482f-8251-1df0cce39474",
   "metadata": {
    "name": "cell135",
    "collapsed": false
   },
   "source": "First we setup a new dynamic table based Orders FeatureView that we will use for testing."
  },
  {
   "cell_type": "code",
   "id": "5f7561fa-bb65-4433-90ad-8feae2e31c08",
   "metadata": {
    "language": "python",
    "name": "cell136"
   },
   "outputs": [],
   "source": "# FEATURE STORE\nfs =  FeatureStore(\n        session=session,\n        database=sess_db,\n        name=fs_name,\n        default_warehouse=\"SIMON_XS\",\n        creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)\n\n# ORDER\norders_table = 'ORDERS'\norders_sdf = session.table(f'{sess_db}.{org}.{orders_table}') \\\n                .with_columns(['ORDERKEY','CUSTKEY'],[F.col('O_ORDERKEY'),F.col('O_CUSTKEY')]) \\\n                .drop('O_ORDERKEY', 'O_CUSTKEY')\norders_nosk_sdf = orders_sdf.drop('CUSTKEY')\n\n# ORDER ENTITY \norder_entity = Entity(\n    name=\"ORDERS\",\n    join_keys=[\"ORDERKEY\"],\n    desc=\"Order entity\"\n    )\nfs.register_entity(order_entity)\n\n# ORDERS FEATUREVIEW\norders_features = orders_sdf.drop(\"ORDERKEY\", \"CUSTKEY\").columns\n\n# Create Order FeatureView in Feature Store\norders_dmfs_fv = FeatureView(\n    name = f\"FV_ORDERS_DMFS\",\n    entities = [order_entity],\n    feature_df = orders_nosk_sdf,\n    refresh_freq = '1 minute',\n    desc = f\"Order feature view\"\n)\n# Register the Order FeatureView in the schema, and add the python featureview to our list\norders_dmfs_fv_v1 = fs.register_feature_view(\n    feature_view = orders_dmfs_fv,    # feature view created above, could also use external_fv\n    version = \"V1\",\n    block = True,               # whether function call blocks until initial data is available\n    overwrite = True,           # whether to replace existing feature view with same name/version\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b1bac89-291d-4101-90ef-192685ffed0c",
   "metadata": {
    "name": "cell112",
    "collapsed": false
   },
   "source": "We can use the data metric functions directly within SQL to return the current state.  For example:"
  },
  {
   "cell_type": "code",
   "id": "d5002f4b-317f-4cd3-a846-dffec98353ef",
   "metadata": {
    "language": "sql",
    "name": "cell106"
   },
   "outputs": [],
   "source": "SELECT SNOWFLAKE.CORE.UNIQUE_COUNT(\n  SELECT\n    O_ORDERSTATUS\n  FROM {{orders_dmfs_fv_v1.fully_qualified_name()}}\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "529f2b51-f7e0-4500-9f97-668eff5a5b26",
   "metadata": {
    "name": "cell156",
    "collapsed": false
   },
   "source": "We need to monitor features differently by type.  We create some simple functions that return the features by type."
  },
  {
   "cell_type": "code",
   "id": "8e3656b3-99d4-4ce7-bb5f-5fe90c6dd891",
   "metadata": {
    "language": "python",
    "name": "cell148",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.types import *\n\ndef get_numeric_columns(dataframe):\n    numeric_types = (IntegerType, DoubleType, DecimalType, FloatType, LongType)\n    return [field.name for field in dataframe.schema.fields \n            if isinstance(field.datatype, numeric_types)]\n\ndef get_string_columns(dataframe):\n    string_types = (StringType)\n    return [field.name for field in dataframe.schema.fields \n            if isinstance(field.datatype, string_types)]\n\ndef get_time_date_columns(dataframe):\n    time_date_types = (TimestampType, TimeType, DateType)\n    return [field.name for field in dataframe.schema.fields \n            if isinstance(field.datatype, time_date_types)]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31401545-5270-44d2-90ca-cd021ad4a4ca",
   "metadata": {
    "language": "python",
    "name": "cell157"
   },
   "outputs": [],
   "source": "# Usage\nnumeric_cols = get_numeric_columns(orders_dmfs_fv_v1.feature_df)\nprint('Numeric features :',numeric_cols)\nstring_columns = get_string_columns(orders_dmfs_fv_v1.feature_df)\nprint('Character features :',string_columns)\ntime_date_columns = get_time_date_columns(orders_dmfs_fv_v1.feature_df)\nprint('Temporal features :',time_date_columns)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f21ec05-d5ea-42cc-b1bf-46978e9571ab",
   "metadata": {
    "name": "cell158",
    "collapsed": false
   },
   "source": "Below is a function to define and create Data Metric Functions on the features of a given  Feature View."
  },
  {
   "cell_type": "code",
   "id": "480363d2-b1c5-4a95-b506-6dc14409dd3f",
   "metadata": {
    "language": "python",
    "name": "cell138",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def fv_create_dmfs(fv):\n    if fv.refresh_freq == None:\n        print(f'FeatureView {fv.fully_qualified_name} is a View, not a Dynamic Table.')\n        print(f'DMFs can only be defined on Dynamic Tables.')    \n\n    fv_df = fv.feature_df\n    fv_schema = fv_df.schema\n    fv_cols = fv_df.columns\n    dmf_cols = fv.feature_names\n    anytype_dmf = ['DUPLICATE_COUNT','NULL_COUNT', 'NULL_PERCENT', 'UNIQUE_COUNT' ]\n    string_dmf = ['BLANK_COUNT','BLANK_PERCENT' ]    \n    numeric_dmf = ['AVG', 'MAX', 'MIN', 'STDDEV']\n    fv_df_features = fv_df.select(fv.feature_names)\n\n    try:    \n        session.sql(f'''ALTER TABLE {fv.fully_qualified_name()} SET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES';''').collect()\n    except:\n        print(f'Schedule TRIGGER_ON_CHANGES on  featureview {fv.fully_qualified_name()} failed')\n      \n    for nc in get_numeric_columns(fv_df_features):\n        for dmf_fun in numeric_dmf:\n            try:    \n\n                session.sql(f''' ALTER TABLE {fv.fully_qualified_name()} ADD DATA METRIC FUNCTION SNOWFLAKE.CORE.{dmf_fun} ON ({nc});''').collect()\n            except:\n                print(f'Function {dmf_fun} failed column {nc} on featureview on {fv.fully_qualified_name()}')\n                \n    for sc in get_string_columns(fv_df_features):\n        for dmf_fun in string_dmf:\n            try:    \n                session.sql(f''' ALTER TABLE {fv.fully_qualified_name()} ADD DATA METRIC FUNCTION SNOWFLAKE.CORE.{dmf_fun} ON ({sc});''').collect()\n            except:\n                print(f'Function {dmf_fun} failed on column {sc} on featureview {fv.fully_qualified_name()}')\n\n    for ac in dmf_cols:\n        for dmf_fun in anytype_dmf:\n            try:                    \n                session.sql(f''' ALTER TABLE {fv.fully_qualified_name()} ADD DATA METRIC FUNCTION SNOWFLAKE.CORE.{dmf_fun} ON ({ac});''').collect()\n            except:\n                print(f'Function {dmf_fun} failed on column {ac} on featureview {fv.fully_qualified_name()}')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f01206fc-a97b-40ba-8d5f-3d9c923f50a6",
   "metadata": {
    "name": "cell159",
    "collapsed": false
   },
   "source": "Create the DMF's on our dynamic table Feature View."
  },
  {
   "cell_type": "code",
   "id": "f3afc560-ba44-4cfd-8072-61fd6d4fe9a3",
   "metadata": {
    "language": "python",
    "name": "cell155",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "fv_create_dmfs(orders_dmfs_fv_v1)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14c9665f-af06-47e0-ad12-40e0de43bc4d",
   "metadata": {
    "name": "cell154",
    "collapsed": false
   },
   "source": "We will simulate modifying some data in our ORDERS source table which will then reflect changes in the dynamic table Feature View."
  },
  {
   "cell_type": "code",
   "id": "4ec9ef45-0cd0-4a18-bfeb-d4837e43bda4",
   "metadata": {
    "language": "sql",
    "name": "cell129",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "select min(O_ORDERDATE), max(O_ORDERDATE)\nfrom {{sess_db}}.{{org}}.ORDERS",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a5cbd0b-aac2-4a6a-9226-ebcbaa5be302",
   "metadata": {
    "name": "cell116",
    "collapsed": false
   },
   "source": "First delete some records from the table, and monitor the changes in the Dynamic Tables and DMFs with the code below.  Then insert some records and do the same. You can cycle through this a few times and you should see some changes recorded in the values from the DMF's. e.g. the averages for the numeric columns will change."
  },
  {
   "cell_type": "code",
   "id": "fbcc514f-e895-45f8-b0a5-9ab8bd84dd12",
   "metadata": {
    "language": "sql",
    "name": "cell128",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "delete from {{sess_db}}.{{org}}.ORDERS where O_ORDERDATE >= '1998-01-01'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dec91013-87c2-4004-9939-2e0f5c7cffd5",
   "metadata": {
    "language": "sql",
    "name": "cell115",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "insert into {{sess_db}}.{{org}}.ORDERS \nselect *\nfrom SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\nwhere O_ORDERDATE >= '1998-01-01'\norder by O_ORDERDATE\n;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c8a8c8e0-38b0-42c0-9faa-4826addbf544",
   "metadata": {
    "name": "cell119",
    "collapsed": false
   },
   "source": "#### Monitoring the dynamic table and data-metric functions\n\nThe code below provides examples of monitoring the state of the Dynamic Table and the data metric functions."
  },
  {
   "cell_type": "code",
   "id": "1b2b1062-1819-4ed5-a4a6-4af50f6ca0b3",
   "metadata": {
    "language": "python",
    "name": "cell160"
   },
   "outputs": [],
   "source": "fs.get_refresh_history(orders_dmfs_fv_v1)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d1d23c0-77be-495f-a32f-808ff27df5a6",
   "metadata": {
    "language": "python",
    "name": "cell161",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"SELECT\n  database_name,\n  schema_name feature_store,\n  name feature_view,  \n  scheduling_state,\n  target_lag_type,\n  target_lag_sec,\nFROM\n  TABLE ( {fs._config.database}.INFORMATION_SCHEMA.DYNAMIC_TABLES() )\nWHERE True\n  and database_name = '{fs._config.database}'\n  and schema_name = '{fs._config.schema}'\n  -- add addtional filters to see a subset of the Feature Views\n  and name = 'FV_ORDERS_DMFS$V1'\nORDER BY\n  name\n\"\"\").show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0664dab6-88f7-47b4-9dac-532006b9e271",
   "metadata": {
    "language": "python",
    "name": "cell162",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"SELECT\n  name feature_view,\n  state,\n  state_code,\n  state_message,\n  query_id,\n  data_timestamp,\n  refresh_start_time,\n  refresh_end_time,\n  statistics,\n  refresh_action \nFROM\n  TABLE (\n    {fs._config.database}.INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY (\n      NAME_PREFIX => '{fs._config.database}.{fs._config.schema}.FV_ORDERS_DMFS$V1' --, ERROR_ONLY => TRUE\n    )\n  )\nORDER BY\n  name,\n  data_timestamp desc\n\"\"\").show(10)  ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98396cb1-8f58-4378-9ec3-eb4dc79e91f1",
   "metadata": {
    "name": "cell117",
    "collapsed": false
   },
   "source": "Retrieve data metric function results for the dynamic table FeatureView."
  },
  {
   "cell_type": "code",
   "id": "67e34464-67f5-4958-a03b-6a6214228f1e",
   "metadata": {
    "language": "python",
    "name": "cell125",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f'''\nselect \n  table_schema feature_store,\n  table_name feature_view,\n  argument_names[0]::varchar feature_name,\n  metric_name,\n  measurement_time,\n  value measurement_value\nfrom SNOWFLAKE.LOCAL.DATA_QUALITY_MONITORING_RESULTS \nwhere \n  table_database = '{orders_dmfs_fv_v1.database}' and \n  table_schema   = '{orders_dmfs_fv_v1.schema}' and \n  table_name     = '{orders_dmfs_fv_v1.name}${orders_dmfs_fv_v1.version}' \norder by table_name, feature_name, metric_name, change_commit_time desc  \n''').to_pandas().to_markdown()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d96852dc-aec9-4a77-94af-b5db23167ca1",
   "metadata": {
    "name": "cell121",
    "collapsed": false
   },
   "source": "To get the data metric functions configured on a given Feature View"
  },
  {
   "cell_type": "code",
   "id": "6bae1047-4122-4541-8023-266028a561d8",
   "metadata": {
    "language": "python",
    "name": "cell120",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"SELECT \nref_entity_schema_name feature_store,\nref_entity_name feature_view,\nparse_json(ref_arguments)[0]:\"name\"::varchar feature_name,\nmetric_name, \nmetric_signature, \nmetric_data_type, \n  FROM TABLE(\n    INFORMATION_SCHEMA.DATA_METRIC_FUNCTION_REFERENCES(\n      REF_ENTITY_NAME => '{orders_dmfs_fv_v1.database}.{orders_dmfs_fv_v1.schema}.{orders_dmfs_fv_v1.name}${orders_dmfs_fv_v1.version}',\n      REF_ENTITY_DOMAIN => 'table'\n    )\n  )\norder by 1,2,3,4  \n\"\"\").to_pandas().to_markdown()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f16c9d0d-2629-4ed5-9a3a-210a16c93057",
   "metadata": {
    "language": "sql",
    "name": "cell123",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ALTER TABLE {{orders_fv_v1.fully_qualified_name()}} SET DATA_METRIC_SCHEDULE = '5 MINUTE';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0c71c27a-9f89-4761-bfd6-71c60147996c",
   "metadata": {
    "name": "cell144",
    "collapsed": false
   },
   "source": "## SUMMARY\n\nWe have seen how we can create a number of Entities in the Feature Store representing the different business-entity levels that we need to store and maintain features at.  \n\nWe can easily retrieve features from different levels of the entity-hierarchy to enrich our training and inference datasets, by adding additional entity key-columns to our Spine dataframes and adding the additional FeatureViews that we want Features from to our data retrieval function (`generate_training_set`, `generate_dataset`, `retrieve_feature_values`). \n\nWe can also retrieve and combine features from lower entity-hierarchy levels, and perform additional processing as needed to derive features at the required level.\n\nWe have seen how we can use common categorical and numeric preprocessing techniques to derive new features, and how we can if needed created new FeatureViews to contain and maintain these.\n\nWe have demonstrated techniques for working across multiple feature stores.\n\nWe have seen how we can monitor our feature views refresh state and the data-quality of them."
  }
 ]
}